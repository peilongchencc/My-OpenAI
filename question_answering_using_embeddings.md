# question_answering_using_embeddings

æ–‡æ¡£ä¸­å®Œæ•´ä»£ç è§ `question_answering_using_embeddings.py` æ–‡ä»¶ã€‚<br>

- [question\_answering\_using\_embeddings](#question_answering_using_embeddings)
  - [Why search is better than fine-tuning(ä¸ºä»€ä¹ˆæœç´¢ä¼˜äºå¾®è°ƒ):](#why-search-is-better-than-fine-tuningä¸ºä»€ä¹ˆæœç´¢ä¼˜äºå¾®è°ƒ)
  - [Search(æœç´¢):](#searchæœç´¢)
  - [Full procedure(å®Œæ•´æµç¨‹):](#full-procedureå®Œæ•´æµç¨‹)
  - [Costs(æˆæœ¬):](#costsæˆæœ¬)
  - [Preamble(å‰è¨€):](#preambleå‰è¨€)
    - [Motivating example: GPT cannot answer questions about current events](#motivating-example-gpt-cannot-answer-questions-about-current-events)
  - [1. Prepare search data(å‡†å¤‡æœç´¢æ•°æ®):](#1-prepare-search-dataå‡†å¤‡æœç´¢æ•°æ®)
  - [3. Ask(è¯¢é—®)](#3-askè¯¢é—®)
  - [Example questions](#example-questions)
  - [Troubleshooting wrong answers](#troubleshooting-wrong-answers)
  - [More examples(æ›´å¤š)](#more-examplesæ›´å¤š)
  - [çŸ¥è¯†æ‹“å±•](#çŸ¥è¯†æ‹“å±•)
    - [åœ¨NLPé¢†åŸŸï¼Œä»€ä¹ˆå› ç´ å†³å®šäº†æ¨¡å‹çš„è¾“å…¥é•¿åº¦ï¼Ÿ](#åœ¨nlpé¢†åŸŸä»€ä¹ˆå› ç´ å†³å®šäº†æ¨¡å‹çš„è¾“å…¥é•¿åº¦)
    - [ä¸ºä»€ä¹ˆchatgptå¯ä»¥ç”¨å‡ åƒç”šè‡³å‡ ä¸‡é•¿åº¦çš„è¾“å…¥:](#ä¸ºä»€ä¹ˆchatgptå¯ä»¥ç”¨å‡ åƒç”šè‡³å‡ ä¸‡é•¿åº¦çš„è¾“å…¥)
    - [HyDEæŠ€æœ¯æ˜¯ä»€ä¹ˆï¼Ÿ](#hydeæŠ€æœ¯æ˜¯ä»€ä¹ˆ)

GPT excels(æ“…é•¿) at answering questions, but only on topics it remembers from its training data.<br>

GPTæ“…é•¿å›ç­”é—®é¢˜ï¼Œä½†ä»…é™äºå®ƒä»è®­ç»ƒæ•°æ®ä¸­è®°ä½çš„è¯é¢˜ã€‚<br>

What should you do if you want GPT to answer questions about unfamiliar topics? E.g.<br>

å¦‚æœä½ å¸Œæœ›GPTå›ç­”å…³äºä¸ç†Ÿæ‚‰è¯é¢˜çš„é—®é¢˜è¯¥æ€ä¹ˆåŠï¼Ÿä¾‹å¦‚ï¼š<br>

- Recent events after Sep 2021(2021å¹´9æœˆä¹‹åçš„æœ€æ–°äº‹ä»¶ï¼ŒæŒ‡GPTæš‚ä¸å«æœ‰çš„ä¿¡æ¯)

- Your non-public documents(ä½ çš„éå…¬å¼€æ–‡æ¡£)

- Information from past conversations(è¿‡å»å¯¹è¯ä¸­çš„ä¿¡æ¯)

- etc.(ç­‰ç­‰)

This notebook demonstrates(æ¼”ç¤º) a two-step Search-Ask method for enabling GPT to answer questions using a library of reference text.<br>

è¿™ä¸ªnotebookæ¼”ç¤ºäº†ä¸€ç§ä¸¤æ­¥éª¤çš„æœç´¢-è¯¢é—®æ–¹æ³•ï¼Œä½¿GPTèƒ½å¤Ÿä½¿ç”¨ä¸€åº“å‚è€ƒæ–‡æœ¬æ¥å›ç­”é—®é¢˜ã€‚<br>

1. Search: search your library of text for relevant(ç›¸å…³çš„) text sections(æœç´¢ï¼šæœç´¢ä½ çš„æ–‡æœ¬åº“ï¼Œå¯»æ‰¾ç›¸å…³çš„æ–‡æœ¬éƒ¨åˆ†)

2. Ask: insert the retrieved(æ£€ç´¢åˆ°çš„) text sections into a message to GPT and ask it the question(è¯¢é—®ï¼šå°†æ£€ç´¢åˆ°çš„æ–‡æœ¬éƒ¨åˆ†æ’å…¥åˆ°æ¶ˆæ¯ä¸­ï¼Œç„¶åå‘GPTæé—®)

## Why search is better than fine-tuning(ä¸ºä»€ä¹ˆæœç´¢ä¼˜äºå¾®è°ƒ):

GPT can learn knowledge in two ways(GPTå¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼å­¦ä¹ çŸ¥è¯†):<br>

- Via model weights (i.e., fine-tune the model on a training set)(é€šè¿‡æ¨¡å‹æƒé‡ï¼ˆå³ï¼Œåœ¨è®­ç»ƒé›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼‰)

- Via model inputs (i.e., insert the knowledge into an input message)(é€šè¿‡æ¨¡å‹è¾“å…¥ï¼ˆå³ï¼Œå°†çŸ¥è¯†æ’å…¥åˆ°è¾“å…¥æ¶ˆæ¯ä¸­ï¼‰)

Although fine-tuning can feel like the more natural optionâ€”training on data is how GPT learned all of its other knowledge, after allâ€”we generally do not recommend it as a way to teach the model knowledge. Fine-tuning is better suited to teaching specialized tasks or styles, and is less reliable(å¯é çš„) for factual recall.<br>

è™½ç„¶å¾®è°ƒæ„Ÿè§‰ä¸Šå¯èƒ½æ˜¯æ›´è‡ªç„¶çš„é€‰æ‹©â€”â€”**æ¯•ç«Ÿ**ï¼Œè®­ç»ƒæ•°æ®æ˜¯GPTå­¦ä¹ å…¶æ‰€æœ‰å…¶ä»–çŸ¥è¯†çš„æ–¹å¼â€”â€”æˆ‘ä»¬é€šå¸¸ä¸æ¨èå®ƒä½œä¸ºæ•™æˆæ¨¡å‹çŸ¥è¯†çš„æ–¹å¼ã€‚ **å¾®è°ƒæ›´é€‚åˆæ•™æˆä¸“é—¨çš„ä»»åŠ¡æˆ–é£æ ¼ï¼Œå¹¶ä¸”å¯¹äºäº‹å®å¬å›æ¥è¯´ä¸å¤ªå¯é ã€‚** ğŸš¨ğŸš¨ğŸš¨<br>

As an analogy(ç±»æ¯”ï¼›æ¯”å–»), model weights are like long-term memory. When you fine-tune a model, it's like studying for an exam a week away. When the exam arrives, the model may forget details, or misremember facts it never read.<br>

ä½œä¸ºä¸€ä¸ªç±»æ¯”ï¼Œ**æ¨¡å‹æƒé‡å°±åƒæ˜¯é•¿æœŸè®°å¿†**ã€‚å½“ä½ å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶ï¼Œå°±åƒæ˜¯ä¸ºä¸€å‘¨åçš„è€ƒè¯•å­¦ä¹ ã€‚å½“è€ƒè¯•åˆ°æ¥æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šå¿˜è®°ç»†èŠ‚ï¼Œæˆ–é”™è®°å®ƒä»æœªé˜…è¯»è¿‡çš„äº‹å®ã€‚<br>

In contrast(å¯¹æ¯”ï¼›å¯¹ç…§), message inputs are like short-term memory. When you insert knowledge into a message, it's like taking an exam with open notes. With notes in hand, the model is more likely to arrive at correct answers.<br>

ç›¸æ¯”ä¹‹ä¸‹ï¼Œ**æ¶ˆæ¯è¾“å…¥å°±åƒæ˜¯çŸ­æœŸè®°å¿†**ã€‚å½“ä½ å°†çŸ¥è¯†æ’å…¥åˆ°ä¸€æ¡æ¶ˆæ¯ä¸­æ—¶ï¼Œå°±åƒæ˜¯å¸¦ç€å¼€æ”¾çš„ç¬”è®°å‚åŠ è€ƒè¯•(**æŒ‡å¼€å·è€ƒè¯•**)ã€‚æ‰‹æŒç¬”è®°ï¼Œæ¨¡å‹æ›´æœ‰å¯èƒ½å¾—å‡ºæ­£ç¡®çš„ç­”æ¡ˆã€‚<br>

One downside(è´Ÿé¢ï¼›ä¸åˆ©æ–¹é¢ï¼›ç¼ºç‚¹) of text search relative(ç›¸å¯¹çš„ï¼›ä¹ŸæŒ‡ç›¸å…³çš„) to fine-tuning is that each model is limited by a maximum amount of text it can read at once:<br>

ä¸å¾®è°ƒç›¸æ¯”ï¼Œæ–‡æœ¬æœç´¢çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯æ¯ä¸ªæ¨¡å‹ä¸€æ¬¡èƒ½è¯»å–çš„æ–‡æœ¬é‡æœ‰é™ï¼š<br>

| Model        | Maximum text length       |
|--------------|---------------------------|
| gpt-3.5-turbo| 4,096 tokens (~5 pages)   |
| gpt-4        | 8,192 tokens (~10 pages)  |
| gpt-4-32k    | 32,768 tokens (~40 pages) |

(New model is available with longer contexts, `gpt-4-1106-preview` have 128K context window)<br>

æ–°å‹å·ç°å·²æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡ï¼Œ`gpt-4-1106-preview` æ‹¥æœ‰ 128K ä¸Šä¸‹æ–‡çª—å£ã€‚<br>

Continuing the analogy, you can think of the model like a student who can only look at a few pages of notes at a time, despite potentially having shelves of textbooks to draw upon.<br>

å»¶ç»­è¿™ä¸ªæ¯”å–»ï¼Œä½ å¯ä»¥å°†æ¨¡å‹æƒ³è±¡ä¸ºä¸€ä¸ªå­¦ç”Ÿï¼Œå°½ç®¡å¯èƒ½æœ‰æˆæ¶çš„æ•™ç§‘ä¹¦å¯ä»¥å‚è€ƒï¼Œä½†ä¸€æ¬¡åªèƒ½æŸ¥çœ‹å‡ é¡µç¬”è®°ã€‚<br>

Therefore, to build a system capable of drawing upon large quantities of text to answer questions, we recommend using a Search-Ask approach.<br>

å› æ­¤ï¼Œä¸ºäº†æ„å»ºä¸€ä¸ªèƒ½å¤Ÿåˆ©ç”¨å¤§é‡æ–‡æœ¬æ¥å›ç­”é—®é¢˜çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨æœç´¢-è¯¢é—®ï¼ˆSearch-Askï¼‰æ–¹æ³•ã€‚<br>

## Search(æœç´¢):

Text can be searched in many ways. E.g.,(æ–‡æœ¬å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œæœç´¢ï¼Œä¾‹å¦‚ï¼š)<br>

- Lexical-based search(åŸºäºè¯æ±‡çš„æœç´¢ï¼ŒLexicalè¡¨ç¤ºè¯æ±‡ï¼›è¯æ³•)

- Graph-based search(åŸºäºå›¾çš„æœç´¢)

- Embedding-based search(åŸºäºè¯å‘é‡çš„æœç´¢)

This example notebook uses embedding-based search. Embeddings are simple to implement and work especially well with questions, as questions often don't lexically overlap(é‡å ) with their answers.<br>

æœ¬ç¤ºä¾‹notebookä½¿ç”¨åŸºäºè¯å‘é‡çš„æœç´¢æ–¹å¼ã€‚Embeddingsç®€å•æ˜“è¡Œï¼Œå°¤å…¶é€‚ç”¨äºé—®é¢˜æœç´¢ï¼Œå› ä¸ºé—®é¢˜çš„æ–‡å­—å¾€å¾€ä¸å…¶ç­”æ¡ˆä¸ç›´æ¥é‡å ã€‚<br>

Consider embeddings-only search as a starting point for your own system. <br>

å°†ä»…åŸºäºè¯å‘é‡çš„æœç´¢è§†ä¸ºä½ è‡ªå·±ç³»ç»Ÿçš„èµ·ç‚¹ã€‚<br>

ğŸ« ğŸ« ğŸ« æœ¬æ–‡æ¡£ä»…ä»‹ç»åŸºäºè¯å‘é‡çš„æœç´¢ï¼Œæ›´å¥½çš„æœç´¢ç³»ç»Ÿä¸å±•ç¤ºï¼Œä½†å¯ä»¥å‚è€ƒä»¥ä¸‹æ–¹æ¡ˆã€‚ğŸš€ğŸš€ğŸš€<br>

Better search systems might combine multiple search methods, along with features like popularity, recency, user history, redundancy with prior search results, click rate data, etc.<br> 

æ›´å¥½çš„æœç´¢ç³»ç»Ÿå¯èƒ½ä¼šç»“åˆå¤šç§æœç´¢æ–¹æ³•ï¼Œä»¥åŠè¯¸å¦‚æµè¡Œåº¦ã€æœ€æ–°æ€§ã€ç”¨æˆ·å†å²è®°å½•ã€ä¸ä¹‹å‰æœç´¢ç»“æœçš„é‡å¤åº¦ã€ç‚¹å‡»ç‡æ•°æ®ç­‰ç‰¹å¾ã€‚<br>

Q&A retrieval(æ£€ç´¢) performance(æ€§èƒ½) may also be improved with techniques like [HyDE](https://arxiv.org/abs/2212.10496), in which questions are first transformed into hypothetical(å‡è®¾çš„) answers before being embedded. Similarly, GPT can also potentially(æ½œåœ¨çš„) improve search results by automatically transforming questions into sets of keywords or search terms.<br>

é‡‡ç”¨HyDEç­‰æŠ€æœ¯ï¼Œé¦–å…ˆå°†é—®é¢˜è½¬æ¢æˆ**å‡è®¾ç­”æ¡ˆ**å†è¿›è¡Œå‘é‡åŒ–ï¼Œä¹Ÿå¯èƒ½æå‡é—®ç­”æ£€ç´¢çš„æ€§èƒ½ã€‚åŒæ ·ï¼ŒGPTä¹Ÿæœ‰å¯èƒ½é€šè¿‡è‡ªåŠ¨å°†é—®é¢˜è½¬æ¢æˆä¸€ç»„å…³é”®è¯æˆ–æœç´¢è¯ï¼Œæ¥æ”¹å–„æœç´¢ç»“æœã€‚<br>

> HyDEæŠ€æœ¯å¯ä»¥ä» "çŸ¥è¯†æ‹“å±•" è·å–è¯¦ç»†ä»‹ç»ã€‚


## Full procedure(å®Œæ•´æµç¨‹):

Specifically, this notebook demonstrates the following procedure(å…·ä½“æ¥è¯´ï¼Œæœ¬notebookæ¼”ç¤ºäº†ä»¥ä¸‹æµç¨‹):<br>

1. Prepare search data (once per document)(å‡†å¤‡æœç´¢æ•°æ®ï¼ˆæ¯ä¸ªæ–‡æ¡£ä¸€æ¬¡ï¼‰)

- Collect: We'll download a few hundred Wikipedia articles about the 2022 Olympics(æ”¶é›†ï¼šæˆ‘ä»¬å°†ä¸‹è½½å‡ ç™¾ç¯‡å…³äº2022å¹´å¥¥è¿ä¼šçš„ç»´åŸºç™¾ç§‘æ–‡ç« )
- Chunk: Documents are split into short, mostly self-contained sections to be embedded(åˆ†å—ï¼šå°†æ–‡æ¡£åˆ†å‰²æˆçŸ­å°ã€åŸºæœ¬è‡ªå«çš„éƒ¨åˆ†ä»¥ä¾¿å‘é‡åŒ–)
- Embed: Each section is embedded with the OpenAI API(å‘é‡åŒ–ï¼šæ¯ä¸ªéƒ¨åˆ†éƒ½ä½¿ç”¨OpenAI APIè¿›è¡Œå‘é‡åŒ–)
- Store: Embeddings are saved (for large datasets, use a vector database)(å­˜å‚¨ï¼šä¿å­˜è¯å‘é‡ç»“æœï¼ˆå¯¹äºå¤§å‹æ•°æ®é›†ï¼Œä½¿ç”¨å‘é‡æ•°æ®åº“ï¼‰)

2. Search (once per query)(æœç´¢ï¼ˆæ¯æ¬¡æŸ¥è¯¢ä¸€æ¬¡ï¼‰)

- Given a user question, generate an embedding for the query from the OpenAI API(æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œä½¿ç”¨OpenAI APIç”ŸæˆæŸ¥è¯¢çš„è¯å‘é‡)
- Using the embeddings, rank the text sections by relevance to the query(åˆ©ç”¨è¯å‘é‡ï¼Œæ ¹æ®ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§å¯¹æ–‡æœ¬éƒ¨åˆ†è¿›è¡Œæ’å)

3. Ask (once per query)(è¯¢é—®ï¼ˆæ¯æ¬¡æŸ¥è¯¢ä¸€æ¬¡ï¼‰)

- Insert the question and the most relevant sections into a message to GPT(å°†é—®é¢˜å’Œæœ€ç›¸å…³çš„éƒ¨åˆ†æ’å…¥åˆ°ç»™GPTçš„æ¶ˆæ¯ä¸­)
- Return GPT's answer(è¿”å›GPTçš„å›ç­”)


## Costs(æˆæœ¬):

Because GPT is more expensive than embeddings search, a system with a decent volume of queries will have its costs dominated(ä¸»å¯¼ï¼›å æ®) by step 3.<br>

> "`a decent volume of`"å¹¶ä¸æ˜¯ä¸€ä¸ªå›ºå®šçŸ­è¯­ï¼Œä½†å®ƒç»å¸¸è¢«ä¸€èµ·ä½¿ç”¨æ¥è¡¨è¾¾ "è¶³å¤Ÿå¤§çš„æ•°é‡"ã€‚

å› ä¸ºGPTçš„æˆæœ¬é«˜äºåµŒå…¥æœç´¢ï¼Œæ‰€ä»¥å¯¹äºæŸ¥è¯¢é‡è¾ƒå¤§çš„ç³»ç»Ÿæ¥è¯´ï¼Œå…¶æˆæœ¬å°†ä¸»è¦ç”±ç¬¬ä¸‰æ­¥æ‰€å æ®ã€‚<br>

- For `gpt-3.5-turbo` using ~1,000 tokens per query, it costs ~$0.002 per query, or ~500 queries per dollar (as of Apr 2023)(å¯¹äºä½¿ç”¨å¤§çº¦1,000ä¸ª tokens æ¯æ¬¡æŸ¥è¯¢çš„gpt-3.5-turboï¼Œæ¯æ¬¡æŸ¥è¯¢çš„æˆæœ¬çº¦ä¸º$0.002ï¼Œå³æ¯ç¾å…ƒçº¦500æ¬¡æŸ¥è¯¢ï¼ˆæˆªè‡³2023å¹´4æœˆï¼‰)

- For `gpt-4`, again assuming ~1,000 tokens per query, it costs ~$0.03 per query, or ~30 queries per dollar (as of Apr 2023)(å¯¹äºgpt-4ï¼ŒåŒæ ·å‡è®¾æ¯æ¬¡æŸ¥è¯¢å¤§çº¦1,000ä¸ª token ï¼Œæ¯æ¬¡æŸ¥è¯¢çš„æˆæœ¬çº¦ä¸º$0.03ï¼Œå³æ¯ç¾å…ƒçº¦30æ¬¡æŸ¥è¯¢ï¼ˆæˆªè‡³2023å¹´4æœˆï¼‰)

Of course, exact costs will depend on the system specifics and usage patterns.<br>

å½“ç„¶ï¼Œç¡®åˆ‡çš„æˆæœ¬å°†å–å†³äºç³»ç»Ÿçš„å…·ä½“æƒ…å†µå’Œä½¿ç”¨æ¨¡å¼ã€‚<br>


## Preamble(å‰è¨€):

We'll begin by(æˆ‘ä»¬å°†ä»ä»¥ä¸‹å‡ æ­¥å¼€å§‹):<br>

- Importing the necessary libraries(å¯¼å…¥å¿…è¦çš„åº“)
- Selecting models for embeddings search and question answering(é€‰æ‹©ç”¨äºå‘é‡æœç´¢å’Œé—®é¢˜å›ç­”çš„æ¨¡å‹)

```python
# imports
import ast  # for converting embeddings saved as strings back to arrays
from openai import OpenAI # for calling the OpenAI API
import pandas as pd  # for storing text and embeddings data
import tiktoken  # for counting tokens
import os # for getting API token from env variable OPENAI_API_KEY
from scipy import spatial  # for calculating vector similarities for search

# models
EMBEDDING_MODEL = "text-embedding-ada-002"
GPT_MODEL = "gpt-3.5-turbo"

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "<your OpenAI API key if not set as env var>"))
```

**Troubleshooting: Installing libraries(æ•…éšœæ’é™¤ï¼šå®‰è£…åº“)**<br>

If you need to install any of the libraries above, run `pip install {library_name}` in your terminal.<br>

å¦‚æœä½ éœ€è¦å®‰è£…ä¸Šè¿°ä»»ä½•åº“ï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œ `pip install {library_name}`ã€‚<br>

For example, to install the openai library, run:<br>

ä¾‹å¦‚ï¼Œè¦å®‰è£… openai åº“ï¼Œè¯·è¿è¡Œï¼š<br>

```bash
pip install openai
```

(You can also do this in a notebook cell with `!pip install openai` or `%pip install openai`.)<br>

(ä½ ä¹Ÿå¯ä»¥åœ¨notebookå•å…ƒæ ¼ä¸­ä½¿ç”¨ `!pip install openai` æˆ– `%pip install openai` æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚)<br>

After installing, restart the notebook kernel so the libraries can be loaded.<br>

å®‰è£…å®Œæˆåï¼Œé‡å¯notebookå†…æ ¸ï¼Œä»¥ä¾¿å¯ä»¥åŠ è½½åº“ã€‚<br>

å®‰è£…scipyæ—¶æç¤ºäº†ä¸‹åˆ—å†…å®¹ï¼Œä½†ç»ˆç©¶æ˜¾ç¤ºæˆåŠŸå®‰è£…ï¼Œåç»­å¦‚æœå‡ºé—®é¢˜å†ä»è¿™é‡Œä¸‹æ‰‹æ‰¾è§£å†³æ–¹æ¡ˆ:<br>

```txt
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
gradio 4.16.0 requires pydantic>=2.0, but you have pydantic 1.10.12 which is incompatible.
Successfully installed numpy-1.26.3 scipy-1.12.0
```

**Troubleshooting: Setting your API key(æ•…éšœæ’é™¤ï¼šè®¾ç½®ä½ çš„ API å¯†é’¥)**<br>

The OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, you can set this environment variable by following these [instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).<br>

OpenAI åº“å°†å°è¯•ä» OPENAI_API_KEY ç¯å¢ƒå˜é‡è¯»å–ä½ çš„ API å¯†é’¥ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰è®¾ç½®ï¼Œå¯ä»¥æŒ‰ç…§è¿™äº›è¯´æ˜æ¥è®¾ç½®æ­¤ç¯å¢ƒå˜é‡ã€‚<br>

### Motivating example: GPT cannot answer questions about current events

åŠ±å¿—çš„ç¤ºä¾‹ï¼šGPT æ— æ³•å›ç­”æœ‰å…³å½“å‰äº‹ä»¶çš„é—®é¢˜<br>

Because the training data for `gpt-3.5-turbo` and `gpt-4` mostly ends in September 2021, the models cannot answer questions about more recent events, such as the 2022 Winter Olympics.<br>

å› ä¸º gpt-3.5-turbo å’Œ gpt-4 çš„è®­ç»ƒæ•°æ®å¤§å¤šæˆªæ­¢åˆ° 2021 å¹´ 9 æœˆï¼Œè¿™äº›æ¨¡å‹æ— æ³•å›ç­”æœ‰å…³æ›´è¿‘æœŸäº‹ä»¶çš„é—®é¢˜ï¼Œä¾‹å¦‚ 2022 å¹´å†¬å­£å¥¥è¿ä¼šã€‚<br>

For example, let's try asking 'Which athletes won the gold medal in curling in 2022?':<br>

ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬å°è¯•é—®ä¸€ä¸‹â€œ2022å¹´å“ªäº›è¿åŠ¨å‘˜èµ¢å¾—äº†å†°å£¶é¡¹ç›®çš„é‡‘ç‰Œï¼Ÿâ€ï¼š<br>

```python
# an example question about the 2022 Olympics
query = 'Which athletes won the gold medal in curling at the 2022 Winter Olympics?'

response = client.chat.completions.create(
    messages=[
        {'role': 'system', 'content': 'You answer questions about the 2022 Winter Olympics.'},
        {'role': 'user', 'content': query},
    ],
    model=GPT_MODEL,
    temperature=0,
)

print(response.choices[0].message.content)
```

ç»ˆç«¯è¾“å‡º:<br>

```txt
As an AI language model, I don't have real-time data. However, I can provide you with general information. The gold medalists in curling at the 2022 Winter Olympics will be determined during the event. The winners will be the team that finishes in first place in the respective men's and women's curling competitions. To find out the specific gold medalists, you can check the official Olympic website or reliable news sources for the most up-to-date information.
```

æ„æ€æ˜¯: ä½œä¸ºä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œæˆ‘æ²¡æœ‰å®æ—¶æ•°æ®ã€‚ç„¶è€Œï¼Œæˆ‘å¯ä»¥æä¾›ä¸€äº›ä¸€èˆ¬æ€§çš„ä¿¡æ¯ã€‚2022å¹´å†¬å­£å¥¥è¿ä¼šä¸Šå†°å£¶é¡¹ç›®çš„é‡‘ç‰Œå¾—ä¸»å°†åœ¨æ¯”èµ›æœŸé—´å†³å®šã€‚è·èƒœè€…å°†æ˜¯åœ¨å„è‡ªçš„ç”·å­å’Œå¥³å­å†°å£¶æ¯”èµ›ä¸­æ’åç¬¬ä¸€çš„é˜Ÿä¼ã€‚è¦äº†è§£å…·ä½“çš„é‡‘ç‰Œå¾—ä¸»ï¼Œä½ å¯ä»¥æŸ¥çœ‹å®˜æ–¹å¥¥è¿ä¼šç½‘ç«™æˆ–å¯é çš„æ–°é—»æ¥æºï¼Œä»¥è·å–æœ€æ–°çš„ä¿¡æ¯ã€‚<br>

In this case, the model has no knowledge of 2022 and is unable to answer the question.<br>

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯¹2022å¹´çš„æƒ…å†µä¸€æ— æ‰€çŸ¥ï¼Œå› æ­¤æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚<br>

You can give GPT knowledge about a topic by inserting it into an input message<br>

ä½ å¯ä»¥é€šè¿‡å°†ç›¸å…³å†…å®¹æ’å…¥è¾“å…¥æ¶ˆæ¯ä¸­ï¼Œæ¥ä½¿GPTäº†è§£æŸä¸ªè¯é¢˜ã€‚<br>

To help give the model knowledge of curling at the 2022 Winter Olympics, we can copy and paste the top half of a relevant Wikipedia article into our message:<br>

ä¸ºäº†å¸®åŠ©æ¨¡å‹äº†è§£2022å¹´å†¬å­£å¥¥è¿ä¼šçš„å†°å£¶æ¯”èµ›ï¼Œæˆ‘ä»¬å¯ä»¥å¤åˆ¶ç²˜è´´ç›¸å…³ç»´åŸºç™¾ç§‘æ–‡ç« çš„ä¸ŠåŠéƒ¨åˆ†åˆ°æˆ‘ä»¬çš„æ¶ˆæ¯ä¸­ï¼š<br>

```txt
# text copied and pasted from: https://en.wikipedia.org/wiki/Curling_at_the_2022_Winter_Olympics
# I didn't bother to format or clean the text, but GPT will still understand it
# the entire article is too long for gpt-3.5-turbo, so I only included the top few sections

wikipedia_article_on_curling = """Curling at the 2022 Winter Olympics

Article
Talk
Read
Edit
View history
From Wikipedia, the free encyclopedia
Curling
at the XXIV Olympic Winter Games
Curling pictogram.svg
Curling pictogram
Venue	Beijing National Aquatics Centre
Dates	2â€“20 February 2022
No. of events	3 (1 men, 1 women, 1 mixed)
Competitors	114 from 14 nations
â† 20182026 â†’
Men's curling
at the XXIV Olympic Winter Games
Medalists
1st place, gold medalist(s)		 Sweden
2nd place, silver medalist(s)		 Great Britain
3rd place, bronze medalist(s)		 Canada
Women's curling
at the XXIV Olympic Winter Games
Medalists
1st place, gold medalist(s)		 Great Britain
2nd place, silver medalist(s)		 Japan
3rd place, bronze medalist(s)		 Sweden
Mixed doubles's curling
at the XXIV Olympic Winter Games
Medalists
1st place, gold medalist(s)		 Italy
2nd place, silver medalist(s)		 Norway
3rd place, bronze medalist(s)		 Sweden
Curling at the
2022 Winter Olympics
Curling pictogram.svg
Qualification
Statistics
Tournament
Men
Women
Mixed doubles
vte
The curling competitions of the 2022 Winter Olympics were held at the Beijing National Aquatics Centre, one of the Olympic Green venues. Curling competitions were scheduled for every day of the games, from February 2 to February 20.[1] This was the eighth time that curling was part of the Olympic program.

In each of the men's, women's, and mixed doubles competitions, 10 nations competed. The mixed doubles competition was expanded for its second appearance in the Olympics.[2] A total of 120 quota spots (60 per sex) were distributed to the sport of curling, an increase of four from the 2018 Winter Olympics.[3] A total of 3 events were contested, one for men, one for women, and one mixed.[4]

Qualification
Main article: Curling at the 2022 Winter Olympics â€“ Qualification
Qualification to the Men's and Women's curling tournaments at the Winter Olympics was determined through two methods (in addition to the host nation). Nations qualified teams by placing in the top six at the 2021 World Curling Championships. Teams could also qualify through Olympic qualification events which were held in 2021. Six nations qualified via World Championship qualification placement, while three nations qualified through qualification events. In men's and women's play, a host will be selected for the Olympic Qualification Event (OQE). They would be joined by the teams which competed at the 2021 World Championships but did not qualify for the Olympics, and two qualifiers from the Pre-Olympic Qualification Event (Pre-OQE). The Pre-OQE was open to all member associations.[5]

For the mixed doubles competition in 2022, the tournament field was expanded from eight competitor nations to ten.[2] The top seven ranked teams at the 2021 World Mixed Doubles Curling Championship qualified, along with two teams from the Olympic Qualification Event (OQE) â€“ Mixed Doubles. This OQE was open to a nominated host and the fifteen nations with the highest qualification points not already qualified to the Olympics. As the host nation, China qualified teams automatically, thus making a total of ten teams per event in the curling tournaments.[6]

Summary
Nations	Men	Women	Mixed doubles	Athletes
 Australia			Yes	2
 Canada	Yes	Yes	Yes	12
 China	Yes	Yes	Yes	12
 Czech Republic			Yes	2
 Denmark	Yes	Yes		10
 Great Britain	Yes	Yes	Yes	10
 Italy	Yes		Yes	6
 Japan		Yes		5
 Norway	Yes		Yes	6
 ROC	Yes	Yes		10
 South Korea		Yes		5
 Sweden	Yes	Yes	Yes	11
 Switzerland	Yes	Yes	Yes	12
 United States	Yes	Yes	Yes	11
Total: 14 NOCs	10	10	10	114
Competition schedule

The Beijing National Aquatics Centre served as the venue of the curling competitions.
Curling competitions started two days before the Opening Ceremony and finished on the last day of the games, meaning the sport was the only one to have had a competition every day of the games. The following was the competition schedule for the curling competitions:

RR	Round robin	SF	Semifinals	B	3rd place play-off	F	Final
Date
Event
Wed 2	Thu 3	Fri 4	Sat 5	Sun 6	Mon 7	Tue 8	Wed 9	Thu 10	Fri 11	Sat 12	Sun 13	Mon 14	Tue 15	Wed 16	Thu 17	Fri 18	Sat 19	Sun 20
Men's tournament								RR	RR	RR	RR	RR	RR	RR	RR	RR	SF	B	F	
Women's tournament									RR	RR	RR	RR	RR	RR	RR	RR	SF	B	F
Mixed doubles	RR	RR	RR	RR	RR	RR	SF	B	F												
Medal summary
Medal table
Rank	Nation	Gold	Silver	Bronze	Total
1	 Great Britain	1	1	0	2
2	 Sweden	1	0	2	3
3	 Italy	1	0	0	1
4	 Japan	0	1	0	1
 Norway	0	1	0	1
6	 Canada	0	0	1	1
Totals (6 entries)	3	3	3	9
Medalists
Event	Gold	Silver	Bronze
Men
details	 Sweden
Niklas Edin
Oskar Eriksson
Rasmus WranÃ¥
Christoffer Sundgren
Daniel Magnusson	 Great Britain
Bruce Mouat
Grant Hardie
Bobby Lammie
Hammy McMillan Jr.
Ross Whyte	 Canada
Brad Gushue
Mark Nichols
Brett Gallant
Geoff Walker
Marc Kennedy
Women
details	 Great Britain
Eve Muirhead
Vicky Wright
Jennifer Dodds
Hailey Duff
Mili Smith	 Japan
Satsuki Fujisawa
Chinami Yoshida
Yumi Suzuki
Yurika Yoshida
Kotomi Ishizaki	 Sweden
Anna Hasselborg
Sara McManus
Agnes Knochenhauer
Sofia Mabergs
Johanna Heldin
Mixed doubles
details	 Italy
Stefania Constantini
Amos Mosaner	 Norway
Kristin Skaslien
Magnus Nedregotten	 Sweden
Almida de Val
Oskar Eriksson
Teams
Men
 Canada	 China	 Denmark	 Great Britain	 Italy
Skip: Brad Gushue
Third: Mark Nichols
Second: Brett Gallant
Lead: Geoff Walker
Alternate: Marc Kennedy

Skip: Ma Xiuyue
Third: Zou Qiang
Second: Wang Zhiyu
Lead: Xu Jingtao
Alternate: Jiang Dongxu

Skip: Mikkel Krause
Third: Mads NÃ¸rgÃ¥rd
Second: Henrik Holtermann
Lead: Kasper Wiksten
Alternate: Tobias Thune

Skip: Bruce Mouat
Third: Grant Hardie
Second: Bobby Lammie
Lead: Hammy McMillan Jr.
Alternate: Ross Whyte

Skip: JoÃ«l Retornaz
Third: Amos Mosaner
Second: Sebastiano Arman
Lead: Simone Gonin
Alternate: Mattia Giovanella

 Norway	 ROC	 Sweden	 Switzerland	 United States
Skip: Steffen Walstad
Third: Torger NergÃ¥rd
Second: Markus HÃ¸iberg
Lead: Magnus VÃ¥gberg
Alternate: Magnus Nedregotten

Skip: Sergey Glukhov
Third: Evgeny Klimov
Second: Dmitry Mironov
Lead: Anton Kalalb
Alternate: Daniil Goriachev

Skip: Niklas Edin
Third: Oskar Eriksson
Second: Rasmus WranÃ¥
Lead: Christoffer Sundgren
Alternate: Daniel Magnusson

Fourth: BenoÃ®t Schwarz
Third: Sven Michel
Skip: Peter de Cruz
Lead: Valentin Tanner
Alternate: Pablo Lachat

Skip: John Shuster
Third: Chris Plys
Second: Matt Hamilton
Lead: John Landsteiner
Alternate: Colin Hufman

Women
 Canada	 China	 Denmark	 Great Britain	 Japan
Skip: Jennifer Jones
Third: Kaitlyn Lawes
Second: Jocelyn Peterman
Lead: Dawn McEwen
Alternate: Lisa Weagle

Skip: Han Yu
Third: Wang Rui
Second: Dong Ziqi
Lead: Zhang Lijun
Alternate: Jiang Xindi

Skip: Madeleine Dupont
Third: Mathilde Halse
Second: Denise Dupont
Lead: My Larsen
Alternate: Jasmin Lander

Skip: Eve Muirhead
Third: Vicky Wright
Second: Jennifer Dodds
Lead: Hailey Duff
Alternate: Mili Smith

Skip: Satsuki Fujisawa
Third: Chinami Yoshida
Second: Yumi Suzuki
Lead: Yurika Yoshida
Alternate: Kotomi Ishizaki

 ROC	 South Korea	 Sweden	 Switzerland	 United States
Skip: Alina Kovaleva
Third: Yulia Portunova
Second: Galina Arsenkina
Lead: Ekaterina Kuzmina
Alternate: Maria Komarova

Skip: Kim Eun-jung
Third: Kim Kyeong-ae
Second: Kim Cho-hi
Lead: Kim Seon-yeong
Alternate: Kim Yeong-mi

Skip: Anna Hasselborg
Third: Sara McManus
Second: Agnes Knochenhauer
Lead: Sofia Mabergs
Alternate: Johanna Heldin

Fourth: Alina PÃ¤tz
Skip: Silvana Tirinzoni
Second: Esther Neuenschwander
Lead: Melanie Barbezat
Alternate: Carole Howald

Skip: Tabitha Peterson
Third: Nina Roth
Second: Becca Hamilton
Lead: Tara Peterson
Alternate: Aileen Geving

Mixed doubles
 Australia	 Canada	 China	 Czech Republic	 Great Britain
Female: Tahli Gill
Male: Dean Hewitt

Female: Rachel Homan
Male: John Morris

Female: Fan Suyuan
Male: Ling Zhi

Female: Zuzana PaulovÃ¡
Male: TomÃ¡Å¡ Paul

Female: Jennifer Dodds
Male: Bruce Mouat

 Italy	 Norway	 Sweden	 Switzerland	 United States
Female: Stefania Constantini
Male: Amos Mosaner

Female: Kristin Skaslien
Male: Magnus Nedregotten

Female: Almida de Val
Male: Oskar Eriksson

Female: Jenny Perret
Male: Martin Rios

Female: Vicky Persinger
Male: Chris Plys
"""
```

```python
query = f"""Use the below article on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found, write "I don't know."

Article:
\"\"\"
{wikipedia_article_on_curling}
\"\"\"

Question: Which athletes won the gold medal in curling at the 2022 Winter Olympics?"""

response = client.chat.completions.create(
    messages=[
        {'role': 'system', 'content': 'You answer questions about the 2022 Winter Olympics.'},
        {'role': 'user', 'content': query},
    ],
    model=GPT_MODEL,
    temperature=0,
)

print(response.choices[0].message.content)
```

æ­¤æ—¶ç»ˆç«¯è¾“å‡º:<br>

```txt
In the men's curling event, the gold medal was won by Sweden. In the women's curling event, the gold medal was won by Great Britain. In the mixed doubles curling event, the gold medal was won by Italy.
```

æ„æ€æ˜¯:åœ¨ç”·å­å†°å£¶æ¯”èµ›ä¸­ï¼Œç‘å…¸èµ¢å¾—äº†é‡‘ç‰Œã€‚åœ¨å¥³å­å†°å£¶æ¯”èµ›ä¸­ï¼Œè‹±å›½èµ¢å¾—äº†é‡‘ç‰Œã€‚åœ¨æ··åˆåŒæ‰“å†°å£¶æ¯”èµ›ä¸­ï¼Œæ„å¤§åˆ©èµ¢å¾—äº†é‡‘ç‰Œã€‚<br>

Thanks to the Wikipedia article included in the input message, GPT answers correctly.<br>

å¤šäºäº†è¾“å…¥æ¶ˆæ¯ä¸­åŒ…å«çš„ç»´åŸºç™¾ç§‘æ–‡ç« ï¼ŒGPTæ­£ç¡®åœ°ç»™å‡ºäº†ç­”æ¡ˆã€‚<br>

In this particular case, GPT was intelligent enough to realize that the original question was underspecified("æœªæ˜ç¡®æŒ‡å®šçš„",è¯»æ³•ä¸º [ÊŒndÉ™rËˆspÉ›sÉªfaÉªd]), as there were three curling gold medal events, not just one.<br>

åœ¨è¿™ä¸ªç‰¹å®šçš„æƒ…å†µä¸‹ï¼ŒGPTè¶³å¤Ÿèªæ˜åœ°æ„è¯†åˆ°åŸå§‹é—®é¢˜æè¿°ä¸å¤Ÿå…·ä½“ï¼Œå› ä¸ºæœ‰ä¸‰ä¸ªå†°å£¶é‡‘ç‰Œèµ›äº‹ï¼Œè€Œä¸ä»…ä»…ä¸€ä¸ªã€‚<br>

Of course, this example partly relied on human intelligence. We knew the question was about curling, so we inserted a Wikipedia article on curling.<br>

å½“ç„¶ï¼Œè¿™ä¸ªä¾‹å­éƒ¨åˆ†åœ°ä¾èµ–äºäººç±»æ™ºèƒ½ã€‚æˆ‘ä»¬çŸ¥é“é—®é¢˜æ˜¯å…³äºå†°å£¶çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬æ’å…¥äº†ä¸€ç¯‡å…³äºå†°å£¶çš„ç»´åŸºç™¾ç§‘æ–‡ç« ã€‚<br>

The rest of this notebook shows how to automate(è‡ªåŠ¨åŒ–) this knowledge insertion with embeddings-based search.<br>

æœ¬notebookçš„å…¶ä½™éƒ¨åˆ†å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨åŸºäºè¯å‘é‡çš„æœç´¢è‡ªåŠ¨åŒ–è¿™ç§çŸ¥è¯†æ’å…¥ã€‚<br>

## 1. Prepare search data(å‡†å¤‡æœç´¢æ•°æ®):

To save you the time & expense, we've prepared a pre-embedded dataset of a few hundred Wikipedia articles about the 2022 Winter Olympics.<br>

ä¸ºäº†èŠ‚çœä½ çš„æ—¶é—´å’Œè´¹ç”¨ï¼Œæˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ªé¢„å‘é‡åŒ–çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†å‡ ç™¾ç¯‡å…³äº2022å¹´å†¬å­£å¥¥è¿ä¼šçš„ç»´åŸºç™¾ç§‘æ–‡ç« ã€‚<br>

To see how we constructed this dataset, or to modify it yourself, see [Embedding Wikipedia articles for search](https://cookbook.openai.com/examples/embedding_wikipedia_articles_for_search).<br>

è¦äº†è§£æˆ‘ä»¬å¦‚ä½•æ„å»ºè¿™ä¸ªæ•°æ®é›†ï¼Œæˆ–è€…è‡ªè¡Œä¿®æ”¹å®ƒï¼Œè¯·å‚é˜…åµŒå…¥ç»´åŸºç™¾ç§‘æ–‡ç« ä»¥ä¾›æœç´¢ã€‚<br>

```python
# download pre-chunked text and pre-computed embeddings
# this file is ~200 MB, so may take a minute depending on your connection speed
embeddings_path = "https://cdn.openai.com/API/examples/data/winter_olympics_2022.csv"

df = pd.read_csv(embeddings_path)
```

```python
# convert embeddings from CSV str type back to list type
df['embedding'] = df['embedding'].apply(ast.literal_eval)
# the dataframe has two columns: "text" and "embedding"
df
```

ç»ˆç«¯æ˜¾ç¤º:<br>

| #    | text                                          | embedding                                   |
|------|-----------------------------------------------|---------------------------------------------|
| 0    | Lviv bid for the 2022 Winter Olympics\n{...   | [-0.00502106780162955, 0.0026050032465718687, ... |
| 1    | Lviv bid for the 2022 Winter Olympics\n{...   | [0.0033927420154213905, -0.007447326090186834, ... |
| 2    | Lviv bid for the 2022 Winter Olympics\n{...   | [-0.00915789045393467, -0.008366798982024193, ... |
| 3    | Lviv bid for the 2022 Winter Olympics\n{...   | [0.003095189109446182, -0.0060643148680585073, ... |
| 4    | Lviv bid for the 2022 Winter Olympics\n{...   | [-0.002936174161732197, -0.006185177247971296, ... |
| ...  | ...                                           | ...                                           |
| 6054 | Anais Chevalier-Bouchet\n==Personal life==\n...| [-0.027750400826334953, 0.001746018067933619, ... |
| 6055 | Uliana Nigmatullina\n{\n==short description | Rus... | [-0.0217141676694915474, 0.016001321375370026, ... |
| 6056 | Uliana Nigmatullina\n==Biathlon results==\n... | [-0.029143543913960457, 0.014653431840574741, ... |
| 6057 | Uliana Nigmatullina\n==Biathlon results==\n... | [-0.024266039952637565, 0.011665306985378265, ... |
| 6058 | Uliana Nigmatullina\n==Biathlon results==\n... | [-0.021818075329365323, 0.005420385394245386, ... |

6059 rows Ã— 2 columns<br>


2. Search(æœç´¢)

Now we'll define a search function that:<br>

æˆ‘ä»¬æ¥å®šä¹‰ä¸€ä¸ªæœç´¢å‡½æ•°ï¼Œå…·ä½“è¦æ±‚å¦‚ä¸‹ï¼š<br>

- Takes a user query and a dataframe with text & embedding columns(æ¥æ”¶ç”¨æˆ·æŸ¥è¯¢å’Œä¸€ä¸ªåŒ…å«æ–‡æœ¬å’ŒåµŒå…¥åˆ—çš„æ•°æ®æ¡†)
- Embeds the user query with the OpenAI API(åˆ©ç”¨ OpenAI API å¯¹ç”¨æˆ·æŸ¥è¯¢è¿›è¡ŒåµŒå…¥)
- Uses distance between query embedding and text embeddings to rank the texts(ä½¿ç”¨æŸ¥è¯¢åµŒå…¥å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„è·ç¦»å¯¹æ–‡æœ¬è¿›è¡Œæ’åº)
- Returns two lists:(è¿”å›ä¸¤ä¸ªåˆ—è¡¨)
    - The top N texts, ranked by relevance(æŒ‰ç›¸å…³æ€§æ’åçš„å‰ N ä¸ªæ–‡æœ¬)
    - Their corresponding relevance scores(å®ƒä»¬å¯¹åº”çš„ç›¸å…³æ€§åˆ†æ•°)

```python
# search function
def strings_ranked_by_relatedness(
    query: str,
    df: pd.DataFrame,
    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),
    top_n: int = 100
) -> tuple[list[str], list[float]]:
    """Returns a list of strings and relatednesses, sorted from most related to least."""
    query_embedding_response = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=query,
    )
    query_embedding = query_embedding_response.data[0].embedding
    strings_and_relatednesses = [
        (row["text"], relatedness_fn(query_embedding, row["embedding"]))
        for i, row in df.iterrows()
    ]
    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)
    strings, relatednesses = zip(*strings_and_relatednesses)
    return strings[:top_n], relatednesses[:top_n]
```

`relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y)`æ˜¯ä¸€ä¸ªPythonè¡¨è¾¾å¼ï¼Œå®šä¹‰äº†ä¸€ä¸ªåä¸º`relatedness_fn`çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°ç”¨äºè®¡ç®—ä¸¤ä¸ªå‘é‡`x`å’Œ`y`ä¹‹é—´çš„ç›¸ä¼¼åº¦æˆ–ç›¸å…³æ€§ã€‚è¿™ä¸ªå‡½æ•°æ˜¯åœ¨ä½¿ç”¨`lambda`å‡½æ•°ï¼ˆä¸€ä¸ªåŒ¿åå‡½æ•°ï¼‰çš„åŸºç¡€ä¸Šå®šä¹‰çš„ï¼Œå¹¶ä¸”ä¾èµ–äº`scipy.spatial.distance.cosine`å‡½æ•°æ¥è®¡ç®—**ä½™å¼¦è·ç¦»**ã€‚<br>

## 3. Ask(è¯¢é—®)

With the search function above, we can now automatically retrieve relevant knowledge and insert it into messages to GPT.<br>

é€šè¿‡ä¸Šè¿°çš„æœç´¢åŠŸèƒ½ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥è‡ªåŠ¨æ£€ç´¢ç›¸å…³çŸ¥è¯†å¹¶å°†å…¶æ’å…¥åˆ°å‘å¾€GPTçš„æ¶ˆæ¯ä¸­ã€‚<br>

Below, we define a function ask that(ä¸‹é¢ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåä¸ºâ€œæŸ¥è¯¢â€çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°):<br>

- Takes a user query(æ¥æ”¶ç”¨æˆ·çš„æŸ¥è¯¢è¯·æ±‚)
- Searches for text relevant to the query(æœç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æœ¬)
- Stuffs(åœ¨è¿™é‡Œçš„æ„æ€æ˜¯â€œå¡«å……â€æˆ–â€œå¡å…¥â€) that text into a message for GPT(å°†è¯¥æ–‡æœ¬å¡«å……åˆ°å‘å¾€GPTçš„æ¶ˆæ¯ä¸­)
- Sends the message to GPT(å°†æ¶ˆæ¯å‘é€ç»™GPT)
- Returns GPT's answer(è¿”å›GPTçš„å›ç­”)

```python
def num_tokens(text: str, model: str = GPT_MODEL) -> int:
    """Return the number of tokens in a string."""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))


def query_message(
    query: str,
    df: pd.DataFrame,
    model: str,
    token_budget: int
) -> str:
    """Return a message for GPT, with relevant source texts pulled from a dataframe."""
    strings, relatednesses = strings_ranked_by_relatedness(query, df)
    introduction = 'Use the below articles on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found in the articles, write "I could not find an answer."'
    question = f"\n\nQuestion: {query}"
    message = introduction
    for string in strings:
        next_article = f'\n\nWikipedia article section:\n"""\n{string}\n"""'
        if (
            num_tokens(message + next_article + question, model=model)
            > token_budget
        ):
            break
        else:
            message += next_article
    return message + question


def ask(
    query: str,
    df: pd.DataFrame = df,
    model: str = GPT_MODEL,
    token_budget: int = 4096 - 500,
    print_message: bool = False,
) -> str:
    """Answers a query using GPT and a dataframe of relevant texts and embeddings."""
    message = query_message(query, df, model=model, token_budget=token_budget)
    if print_message:
        print(message)
    messages = [
        {"role": "system", "content": "You answer questions about the 2022 Winter Olympics."},
        {"role": "user", "content": message},
    ]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
    response_message = response.choices[0].message.content
    return response_message
```

## Example questions

Finally, let's ask our system our original(æœ€åˆçš„) question about gold medal(å¥–ç« ï¼›è·å¾—å¥–ç‰Œ) curlers:<br>

æœ€åï¼Œè®©æˆ‘ä»¬ç”¨æˆ‘ä»¬æœ€åˆçš„é—®é¢˜è¯¢é—®æˆ‘ä»¬çš„ç³»ç»Ÿå…³äºå†¬å­£å¥¥è¿ä¼šå†°å£¶é‡‘ç‰Œè·å¾—è€…çš„ä¿¡æ¯ï¼š<br>

```python
# 2022å¹´å†¬å­£å¥¥è¿ä¼šå†°å£¶æ¯”èµ›ä¸­å“ªäº›è¿åŠ¨å‘˜èµ¢å¾—äº†é‡‘ç‰Œï¼Ÿ
print(ask('Which athletes won the gold medal in curling at the 2022 Winter Olympics?'))
```

ç»ˆç«¯æ˜¾ç¤º:<br>

```txt
"In the men's curling tournament, the gold medal was won by the team from Sweden, consisting of Niklas Edin, Oskar Eriksson, Rasmus WranÃ¥, Christoffer Sundgren, and Daniel Magnusson. In the women's curling tournament, the gold medal was won by the team from Great Britain, consisting of Eve Muirhead, Vicky Wright, Jennifer Dodds, Hailey Duff, and Mili Smith."
```

æ„æ€æ˜¯:<br>

```txt
"åœ¨ç”·å­å†°å£¶æ¯”èµ›ä¸­ï¼Œé‡‘ç‰Œç”±ç‘å…¸é˜Ÿèµ¢å¾—ï¼Œé˜Ÿå‘˜åŒ…æ‹¬Niklas Edin, Oskar Eriksson, Rasmus WranÃ¥, Christoffer Sundgren, å’Œ Daniel Magnussonã€‚åœ¨å¥³å­å†°å£¶æ¯”èµ›ä¸­ï¼Œé‡‘ç‰Œç”±è‹±å›½é˜Ÿèµ¢å¾—ï¼Œé˜Ÿå‘˜åŒ…æ‹¬Eve Muirhead, Vicky Wright, Jennifer Dodds, Hailey Duff, å’Œ Mili Smithã€‚"
```

Despite `gpt-3.5-turbo` having no knowledge of the 2022 Winter Olympics, our search system was able to retrieve reference text for the model to read, allowing it to correctly list the gold medal winners in the Men's and Women's tournaments.<br>

å°½ç®¡gpt-3.5-turboå¯¹2022å¹´å†¬å­£å¥¥è¿ä¼šæ²¡æœ‰çŸ¥è¯†ï¼Œæˆ‘ä»¬çš„æœç´¢ç³»ç»Ÿè¿˜æ˜¯èƒ½å¤Ÿä¸ºæ¨¡å‹æ£€ç´¢å‚è€ƒæ–‡æœ¬ï¼Œä½¿å…¶èƒ½å¤Ÿæ­£ç¡®åˆ—å‡ºç”·å­å’Œå¥³å­æ¯”èµ›çš„é‡‘ç‰Œè·å¾—è€…ã€‚<br>

However, it still wasn't quite perfectâ€”the model failed to list the gold medal winners from the Mixed doubles event.<br>

ç„¶è€Œï¼Œå®ƒä»ç„¶ä¸å¤Ÿå®Œç¾â€”â€”æ¨¡å‹æœªèƒ½åˆ—å‡ºæ··åˆåŒæ‰“é¡¹ç›®çš„é‡‘ç‰Œè·å¾—è€…ã€‚<br>


## Troubleshooting wrong answers

To see whether a mistake is from a lack(ç¼ºå°‘) of relevant source text (i.e., failure of the search step) or a lack of reasoning reliability (i.e., failure of the ask step), you can look at the text GPT was given by setting `print_message=True`.<br>

è¦åˆ¤æ–­ä¸€ä¸ªé”™è¯¯æ˜¯å› ä¸ºç¼ºä¹ç›¸å…³çš„æºæ–‡æœ¬ï¼ˆå³ï¼Œæœç´¢æ­¥éª¤å¤±è´¥ï¼‰è¿˜æ˜¯å› ä¸ºç¼ºä¹æ¨ç†çš„å¯é æ€§ï¼ˆå³ï¼Œæé—®æ­¥éª¤å¤±è´¥ï¼‰ï¼Œä½ å¯ä»¥é€šè¿‡è®¾ç½® `print_message=True` æ¥æŸ¥çœ‹ GPT è¢«ç»™äºˆçš„æ–‡æœ¬ã€‚<br>

In this particular(ç‰¹å®šçš„) case, looking at the text below, it looks like the #1 article given to the model did contain medalists(è·å¥–è€…) for all three events, but the later results emphasized(å¼ºè°ƒ) the Men's and Women's tournaments(é”¦æ ‡èµ›ï¼›æ¯”èµ›), which may have distracted(ä½¿åˆ†å¿ƒ) the model from giving a more complete answer.<br>

åœ¨è¿™ä¸ªç‰¹å®šçš„æ¡ˆä¾‹ä¸­ï¼ŒæŸ¥çœ‹ä¸‹é¢çš„æ–‡æœ¬ï¼Œçœ‹èµ·æ¥åƒæ˜¯ç»™æ¨¡å‹çš„ç¬¬ä¸€ç¯‡æ–‡ç« ç¡®å®åŒ…å«äº†æ‰€æœ‰ä¸‰é¡¹èµ›äº‹çš„å¥–ç‰Œå¾—ä¸»ï¼Œä½†åç»­çš„ç»“æœå¼ºè°ƒäº†ç”·å­å’Œå¥³å­æ¯”èµ›ï¼Œè¿™å¯èƒ½è®©æ¨¡å‹åˆ†å¿ƒï¼Œæœªèƒ½ç»™å‡ºä¸€ä¸ªæ›´å®Œæ•´çš„ç­”æ¡ˆã€‚<br>

```python
# set print_message=True to see the source text GPT was working off of
# è®¾ç½® `print_message=True` æŸ¥çœ‹GPTè¢«ç»™äºˆçš„æ–‡æœ¬
print(ask('Which athletes won the gold medal in curling at the 2022 Winter Olympics?', print_message=True))
```

ç»ˆç«¯æ˜¾ç¤ºå¦‚ä¸‹:<br>

```txt
Use the below articles on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found in the articles, write "I could not find an answer."

Wikipedia article section:
"""
List of 2022 Winter Olympics medal winners

==Curling==

{{main|Curling at the 2022 Winter Olympics}}
{|{{MedalistTable|type=Event|columns=1|width=225|labelwidth=200}}
|-valign="top"
|Men<br/>{{DetailsLink|Curling at the 2022 Winter Olympics â€“ Men's tournament}}
|{{flagIOC|SWE|2022 Winter}}<br/>[[Niklas Edin]]<br/>[[Oskar Eriksson]]<br/>[[Rasmus WranÃ¥]]<br/>[[Christoffer Sundgren]]<br/>[[Daniel Magnusson (curler)|Daniel Magnusson]]
|{{flagIOC|GBR|2022 Winter}}<br/>[[Bruce Mouat]]<br/>[[Grant Hardie]]<br/>[[Bobby Lammie]]<br/>[[Hammy McMillan Jr.]]<br/>[[Ross Whyte]]
|{{flagIOC|CAN|2022 Winter}}<br/>[[Brad Gushue]]<br/>[[Mark Nichols (curler)|Mark Nichols]]<br/>[[Brett Gallant]]<br/>[[Geoff Walker (curler)|Geoff Walker]]<br/>[[Marc Kennedy]]
|-valign="top"
|Women<br/>{{DetailsLink|Curling at the 2022 Winter Olympics â€“ Women's tournament}}
|{{flagIOC|GBR|2022 Winter}}<br/>[[Eve Muirhead]]<br/>[[Vicky Wright]]<br/>[[Jennifer Dodds]]<br/>[[Hailey Duff]]<br/>[[Mili Smith]]
|{{flagIOC|JPN|2022 Winter}}<br/>[[Satsuki Fujisawa]]<br/>[[Chinami Yoshida]]<br/>[[Yumi Suzuki]]<br/>[[Yurika Yoshida]]<br/>[[Kotomi Ishizaki]]
|{{flagIOC|SWE|2022 Winter}}<br/>[[Anna Hasselborg]]<br/>[[Sara McManus]]<br/>[[Agnes Knochenhauer]]<br/>[[Sofia Mabergs]]<br/>[[Johanna Heldin]]
|-valign="top"
|Mixed doubles<br/>{{DetailsLink|Curling at the 2022 Winter Olympics â€“ Mixed doubles tournament}}
|{{flagIOC|ITA|2022 Winter}}<br/>[[Stefania Constantini]]<br/>[[Amos Mosaner]]
|{{flagIOC|NOR|2022 Winter}}<br/>[[Kristin Skaslien]]<br/>[[Magnus Nedregotten]]
|{{flagIOC|SWE|2022 Winter}}<br/>[[Almida de Val]]<br/>[[Oskar Eriksson]]
|}
"""

Wikipedia article section:
"""
Curling at the 2022 Winter Olympics

==Results summary==

===Women's tournament===

====Playoffs====

=====Gold medal game=====

''Sunday, 20 February, 9:05''
{{#lst:Curling at the 2022 Winter Olympics â€“ Women's tournament|GM}}
{{Player percentages
| team1 = {{flagIOC|JPN|2022 Winter}}
| [[Yurika Yoshida]] | 97%
| [[Yumi Suzuki]] | 82%
| [[Chinami Yoshida]] | 64%
| [[Satsuki Fujisawa]] | 69%
| teampct1 = 78%
| team2 = {{flagIOC|GBR|2022 Winter}}
| [[Hailey Duff]] | 90%
| [[Jennifer Dodds]] | 89%
| [[Vicky Wright]] | 89%
| [[Eve Muirhead]] | 88%
| teampct2 = 89%
}}
"""

Wikipedia article section:
"""
Curling at the 2022 Winter Olympics

==Medal summary==

===Medal table===

{{Medals table
 | caption        = 
 | host           = 
 | flag_template  = flagIOC
 | event          = 2022 Winter
 | team           = 
 | gold_CAN = 0 | silver_CAN = 0 | bronze_CAN = 1
 | gold_ITA = 1 | silver_ITA = 0 | bronze_ITA = 0
 | gold_NOR = 0 | silver_NOR = 1 | bronze_NOR = 0
 | gold_SWE = 1 | silver_SWE = 0 | bronze_SWE = 2
 | gold_GBR = 1 | silver_GBR = 1 | bronze_GBR = 0
 | gold_JPN = 0 | silver_JPN = 1 | bronze_JPN - 0
}}
"""

Wikipedia article section:
"""
Curling at the 2022 Winter Olympics

==Results summary==

===Men's tournament===

====Playoffs====
... ...
```

Knowing that this mistake was due to imperfect(ä¸å®Œç¾ï¼›æœ‰ç¼ºç‚¹çš„) reasoning in the ask step, rather than imperfect retrieval in the search step, let's focus on improving the ask step.<br>

äº†è§£åˆ°è¿™ä¸ªé”™è¯¯æ˜¯å› ä¸ºæé—®æ­¥éª¤ä¸­çš„è¯¢é—®ï¼Œè€Œä¸æ˜¯æœç´¢æ­¥éª¤ä¸­çš„æ£€ç´¢ä¸å®Œç¾ï¼Œæˆ‘ä»¬å°±ä¸“æ³¨äºæ”¹å–„æé—®æ­¥éª¤ã€‚<br>

The easiest way to improve results is to use a more capable model, such as `GPT-4`. Let's try it.<br>

æ”¹å–„ç»“æœçš„æœ€ç®€å•æ–¹å¼æ˜¯ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œæ¯”å¦‚ `GPT-4` ï¼Œè®©æˆ‘ä»¬è¯•è¯•çœ‹ã€‚<br>

```python
# use more powerful model
print(ask('Which athletes won the gold medal in curling at the 2022 Winter Olympics?', model="gpt-4"))
```

ç»ˆç«¯æ˜¾ç¤º:<br>

```txt
"The athletes who won the gold medal in curling at the 2022 Winter Olympics are:\n\nMen's tournament: Niklas Edin, Oskar Eriksson, Rasmus WranÃ¥, Christoffer Sundgren, and Daniel Magnusson from Sweden.\n\nWomen's tournament: Eve Muirhead, Vicky Wright, Jennifer Dodds, Hailey Duff, and Mili Smith from Great Britain.\n\nMixed doubles tournament: Stefania Constantini and Amos Mosaner from Italy."
```

æ„æ€æ˜¯:<br>

```txt
åœ¨2022å¹´å†¬å­£å¥¥è¿ä¼šä¸Šè·å¾—å†°å£¶é¡¹ç›®é‡‘ç‰Œçš„è¿åŠ¨å‘˜æœ‰ï¼š

ç”·å­æ¯”èµ›ï¼šæ¥è‡ªç‘å…¸çš„Niklas Edinã€Oskar Erikssonã€Rasmus WranÃ¥ã€Christoffer Sundgrenå’ŒDaniel Magnussonã€‚

å¥³å­æ¯”èµ›ï¼šæ¥è‡ªè‹±å›½çš„Eve Muirheadã€Vicky Wrightã€Jennifer Doddsã€Hailey Duffå’ŒMili Smithã€‚

æ··åˆåŒäººæ¯”èµ›ï¼šæ¥è‡ªæ„å¤§åˆ©çš„Stefania Constantiniå’ŒAmos Mosanerã€‚
```

GPT-4 succeeds perfectly, correctly identifying all 12 gold medal winners in curling.<br>

GPT-4 å®Œç¾æˆåŠŸï¼Œå‡†ç¡®è¯†åˆ«äº†å†°å£¶é¡¹ç›®æ‰€æœ‰12ä½é‡‘ç‰Œè·å¾—è€…ã€‚<br>


## More examples(æ›´å¤š)

Below are a few more examples of the system in action. Feel free to try your own questions, and see how it does. <br>

> "Feel free":é¼“åŠ±æŸäººåœ¨æ²¡æœ‰é™åˆ¶æˆ–é¡¾è™‘çš„æƒ…å†µä¸‹åšæŸäº‹ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›ç³»ç»Ÿè¿è¡Œä¸­çš„æ›´å¤šç¤ºä¾‹ã€‚æ¬¢è¿å°è¯•ä½ è‡ªå·±çš„é—®é¢˜ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚<br>

In general, search-based systems do best on questions that have a simple lookup, and worst on questions that require multiple partial sources to be combined and reasoned(åˆç†çš„ï¼›æ¨ç†) about.<br>

> "reasoned" ä¸º "reason" çš„è¿‡å»åˆ†è¯å’Œè¿‡å»å¼ï¼Œadj. åˆç†çš„;åˆä¹é€»è¾‘çš„;ç¼œå¯†çš„ v. æ¨ç†;ç†è§£;æ€è€ƒ;æ¨æ–­;æ¨è®º

é€šå¸¸æƒ…å†µä¸‹ï¼ŒåŸºäºæœç´¢çš„ç³»ç»Ÿåœ¨éœ€è¦ç®€å•æŸ¥æ‰¾çš„é—®é¢˜ä¸Šè¡¨ç°æœ€å¥½ï¼Œè€Œåœ¨éœ€è¦ç»“åˆå’Œæ¨ç†å¤šä¸ªéƒ¨åˆ†ä¿¡æ¯æºçš„é—®é¢˜ä¸Šè¡¨ç°æœ€å·®ã€‚<br>

```python
# counting(è®¡ç®—) question,2022å¹´å†¬å­£å¥¥æ—åŒ¹å…‹è¿åŠ¨ä¼šåˆ›é€ äº†å¤šå°‘è®°å½•ï¼Ÿ
print(ask('How many records were set at the 2022 Winter Olympics?'))
# 'I could not find an answer.'
```

```python
# comparison(æ¯”è¾ƒ) question, ç‰™ä¹°åŠ æˆ–å¤å·´åœ¨2022å¹´å†¬å­£å¥¥è¿ä¼šä¸Šçš„è¿åŠ¨å‘˜æ›´å¤šå—ï¼Ÿ
print(ask('Did Jamaica or Cuba have more athletes at the 2022 Winter Olympics?'))

# "Jamaica had more athletes at the 2022 Winter Olympics. According to the provided information, Jamaica had a total of 7 athletes (6 men and 1 woman) competing in 2 sports, while there is no information about Cuba's participation in the 2022 Winter Olympics."

# â€œåœ¨2022å¹´å†¬å­£å¥¥è¿ä¼šä¸Šï¼Œç‰™ä¹°åŠ çš„è¿åŠ¨å‘˜æ›´å¤šã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œç‰™ä¹°åŠ å…±æœ‰7åè¿åŠ¨å‘˜ï¼ˆ6ç”·1å¥³ï¼‰å‚åŠ äº†2é¡¹è¿åŠ¨ï¼Œè€Œå…³äºå¤å·´å‚åŠ 2022å¹´å†¬å­£å¥¥è¿ä¼šçš„ä¿¡æ¯åˆ™æ²¡æœ‰ã€‚â€
```

```python
# subjective question(ä¸»è§‚é—®é¢˜),å“ªé¡¹å¥¥æ—åŒ¹å…‹è¿åŠ¨æœ€æœ‰å¨±ä¹æ€§(æœ€æœ‰è¶£)ï¼Ÿ, entertaining adj. å¨±ä¹çš„ï¼›æœ‰è¶£çš„
print(ask('Which Olympic sport is the most entertaining?'))
# 'I could not find an answer.'
```


## çŸ¥è¯†æ‹“å±•

### åœ¨NLPé¢†åŸŸï¼Œä»€ä¹ˆå› ç´ å†³å®šäº†æ¨¡å‹çš„è¾“å…¥é•¿åº¦ï¼Ÿ

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œæ¨¡å‹çš„è¾“å…¥é•¿åº¦å—åˆ°å¤šç§å› ç´ çš„å½±å“ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼š<br>

1. **æ¨¡å‹æ¶æ„**ï¼šä¸åŒçš„NLPæ¨¡å‹æ¶æ„å¯¹è¾“å…¥é•¿åº¦æœ‰ä¸åŒçš„é™åˆ¶ã€‚ä¾‹å¦‚ï¼Œä¼ ç»Ÿçš„RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰åœ¨å¤„ç†é•¿åºåˆ—æ—¶ä¼šé‡åˆ°æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬å¤„ç†é•¿è¾“å…¥åºåˆ—çš„èƒ½åŠ›ã€‚è€ŒåŸºäºTransformerçš„æ¨¡å‹ï¼Œå¦‚BERTæˆ–GPTç³»åˆ—ï¼Œè™½ç„¶èƒ½å¤Ÿå¤„ç†ç›¸å¯¹æ›´é•¿çš„åºåˆ—ï¼Œä½†å®ƒä»¬ä»ç„¶æœ‰å›ºå®šçš„æœ€å¤§è¾“å…¥é•¿åº¦é™åˆ¶ï¼Œè¿™ä¸»è¦ç”±æ¨¡å‹çš„ä½ç½®ç¼–ç å’Œå†…å­˜é™åˆ¶å†³å®šã€‚

2. **ä½ç½®ç¼–ç **ï¼šTransformeræ¨¡å‹é€šè¿‡ä½ç½®ç¼–ç æ¥æ•è·åºåˆ—ä¸­å•è¯çš„é¡ºåºä¿¡æ¯ã€‚åœ¨è®¸å¤šæ¨¡å‹å®ç°ä¸­ï¼Œä½ç½®ç¼–ç çš„é•¿åº¦æ˜¯é¢„å…ˆå®šä¹‰çš„ï¼Œè¿™ç›´æ¥é™åˆ¶äº†æ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚

3. **å†…å­˜å’Œè®¡ç®—èµ„æº**ï¼šå¤„ç†é•¿åºåˆ—éœ€è¦å¤§é‡çš„å†…å­˜å’Œè®¡ç®—èµ„æºã€‚æ¨¡å‹è¾“å…¥é•¿åº¦çš„å¢åŠ ä¼šå¯¼è‡´è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚æˆå€å¢é•¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŸºäºTransformerçš„æ¨¡å‹ï¼Œå…¶è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚æ˜¯è¾“å…¥é•¿åº¦çš„å¹³æ–¹çº§åˆ«ã€‚å› æ­¤ï¼Œç¡¬ä»¶èµ„æºçš„é™åˆ¶ä¹Ÿæ˜¯å†³å®šæ¨¡å‹è¾“å…¥é•¿åº¦çš„ä¸€ä¸ªé‡è¦å› ç´ ã€‚

4. **ä»»åŠ¡éœ€æ±‚å’Œæ•°æ®ç‰¹æ€§**ï¼šä¸åŒçš„NLPä»»åŠ¡å’Œæ•°æ®é›†ç‰¹æ€§ä¹Ÿä¼šå½±å“æ¨¡å‹è¾“å…¥é•¿åº¦çš„é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œä¸€äº›ä»»åŠ¡å¯èƒ½éœ€è¦åˆ†æé•¿æ–‡æœ¬ï¼ˆå¦‚æ–‡æ¡£åˆ†ç±»æˆ–é—®ç­”ç³»ç»Ÿä¸­çš„æ–‡æœ¬ç†è§£ï¼‰ï¼Œè€Œå…¶ä»–ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰å¯èƒ½åªéœ€è¦è¾ƒçŸ­çš„æ–‡æœ¬ç‰‡æ®µã€‚æ­¤å¤–ï¼Œæ•°æ®çš„è¯­è¨€å’Œç»“æ„ä¹Ÿä¼šå½±å“åˆ°åˆé€‚çš„è¾“å…¥é•¿åº¦ã€‚

5. **æ•ˆç‡å’Œæ€§èƒ½å¹³è¡¡**ï¼šè™½ç„¶è¾ƒé•¿çš„è¾“å…¥å¯ä»¥æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°ç†è§£æ–‡æœ¬ï¼Œä½†è¿™ä¹Ÿå¯èƒ½å¯¼è‡´è®¡ç®—æ•ˆç‡é™ä½å’Œè®­ç»ƒéš¾åº¦å¢åŠ ã€‚å› æ­¤ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦åœ¨æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡ç‚¹ï¼Œè¿™é€šå¸¸é€šè¿‡å®éªŒæ¥ç¡®å®šæœ€ä¼˜çš„è¾“å…¥é•¿åº¦ã€‚

æ€»çš„æ¥è¯´ï¼Œæ¨¡å‹çš„è¾“å…¥é•¿åº¦æ˜¯ç”±æ¨¡å‹æ¶æ„ã€æŠ€æœ¯é™åˆ¶ã€ä»»åŠ¡éœ€æ±‚å’Œæ•ˆç‡è€ƒè™‘ç­‰å¤šä¸ªå› ç´ å…±åŒå†³å®šçš„ã€‚åœ¨è®¾è®¡æˆ–é€‰æ‹©NLPæ¨¡å‹æ—¶ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡çš„éœ€æ±‚å’Œå¯ç”¨èµ„æºæ¥åˆç†è®¾ç½®è¾“å…¥é•¿åº¦ã€‚<br>

### ä¸ºä»€ä¹ˆchatgptå¯ä»¥ç”¨å‡ åƒç”šè‡³å‡ ä¸‡é•¿åº¦çš„è¾“å…¥:

ChatGPTï¼ŒåŸºäºOpenAIçš„GPTï¼ˆGenerative Pre-trained Transformerï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å‡ åƒç”šè‡³å‡ ä¸‡é•¿åº¦çš„è¾“å…¥ï¼Œè¿™å¾—ç›Šäºå‡ ä¸ªå…³é”®æŠ€æœ¯å’Œè®¾è®¡é€‰æ‹©ï¼š<br>

1. **ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶**ï¼šè™½ç„¶æ ‡å‡†çš„Transformeræ¨¡å‹ä½¿ç”¨çš„å…¨è‡ªæ³¨æ„åŠ›æœºåˆ¶é™åˆ¶äº†å®ƒå¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›ï¼ˆç”±äºå†…å­˜å’Œè®¡ç®—æˆæœ¬éšåºåˆ—é•¿åº¦å¹³æ–¹å¢é•¿ï¼‰ï¼Œä½†é€šè¿‡é‡‡ç”¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ŒGPTèƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜ä½¿ç”¨ã€‚ç¨€ç–æ³¨æ„åŠ›é€šè¿‡ä»…åœ¨åºåˆ—çš„å­é›†ä¸Šæ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—æ¥å®ç°ï¼Œä»è€Œå…è®¸æ¨¡å‹å¤„ç†æ›´é•¿çš„æ–‡æœ¬åºåˆ—ã€‚

2. **åˆ†å—å¤„ç†**ï¼šä¸ºäº†å¤„ç†é•¿æ–‡æœ¬ï¼ŒChatGPTå¯ä»¥å°†è¾“å…¥æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°çš„å—æˆ–ç‰‡æ®µï¼Œè¿™äº›å—å¯ä»¥ç‹¬ç«‹åœ°è¢«æ¨¡å‹å¤„ç†ã€‚ç„¶åï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡æŸç§å½¢å¼çš„çŠ¶æ€ç®¡ç†æˆ–ä¸Šä¸‹æ–‡èåˆæŠ€æœ¯æ¥æ•´åˆè¿™äº›å—çš„è¾“å‡ºï¼Œä»¥ç”Ÿæˆè¿è´¯çš„å›å¤ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹é—´æ¥åœ°å¤„ç†è¶…å‡ºå…¶æœ€å¤§è¾“å…¥é•¿åº¦é™åˆ¶çš„æ–‡æœ¬ã€‚

3. **å†…å­˜å’Œä¸Šä¸‹æ–‡ç¼“å­˜æœºåˆ¶**ï¼šæŸäº›Transformeræ¨¡å‹çš„å˜ä½“ï¼Œå¦‚GPT-3åŠå…¶åç»­ç‰ˆæœ¬ï¼Œé‡‡ç”¨äº†æ”¹è¿›çš„å†…å­˜ç®¡ç†å’Œä¸Šä¸‹æ–‡ç¼“å­˜æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†è¿ç»­å¯¹è¯æˆ–é•¿æ–‡æœ¬æ—¶â€œè®°ä½â€å…ˆå‰çš„ä¸Šä¸‹æ–‡ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥åœ¨ä¸åŒæ—¶é—´ç‚¹è®¿é—®å’Œå‚è€ƒä¹‹å‰çš„è¾“å…¥ï¼Œå³ä½¿è¿™äº›è¾“å…¥åœ¨ä¸€ä¸ªæ“ä½œæ­¥éª¤ä¸­è¶…è¿‡äº†æ¨¡å‹çš„æ ‡å‡†è¾“å…¥é™åˆ¶ã€‚

4. **æ¨¡å‹å’Œæ¡†æ¶ä¼˜åŒ–**ï¼šé€šè¿‡è½¯ä»¶å’Œç¡¬ä»¶ä¼˜åŒ–ï¼Œå¦‚æ›´é«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•æ“ä½œã€æ›´å¥½çš„å†…å­˜ç®¡ç†ç­–ç•¥å’Œä½¿ç”¨ä¸“é—¨çš„ç¡¬ä»¶ï¼ˆå¦‚GPUæˆ–TPUï¼‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹å¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›ã€‚è¿™äº›ä¼˜åŒ–æœ‰åŠ©äºé™ä½å¤„ç†é•¿è¾“å…¥æ—¶çš„å»¶è¿Ÿå’Œèµ„æºéœ€æ±‚ã€‚

5. **åŠ¨æ€æ³¨æ„åŠ›çª—å£å’Œå±‚æ¬¡åŒ–æ³¨æ„åŠ›**ï¼šä¸€äº›ç ”ç©¶æå‡ºäº†åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›çª—å£å¤§å°çš„æ–¹æ³•ï¼Œæˆ–è€…é€šè¿‡å±‚æ¬¡åŒ–çš„æ³¨æ„åŠ›ç»“æ„æ¥æœ‰æ•ˆå¤„ç†é•¿è·ç¦»ä¾èµ–ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹å¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›ã€‚

è¿™äº›æŠ€æœ¯å’Œè®¾è®¡ç­–ç•¥çš„ç»„åˆä½¿å¾—ChatGPTèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿è¾“å…¥åºåˆ—ï¼Œå³ä½¿æ˜¯å‡ åƒåˆ°å‡ ä¸‡ä¸ªå­—ç¬¦çš„é•¿åº¦ï¼Œä»è€Œåœ¨å¤šè½®å¯¹è¯ã€é•¿æ–‡æœ¬ç†è§£å’Œç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤„ç†éå¸¸é•¿çš„è¾“å…¥ä»ç„¶ä¼šå—åˆ°å®é™…è®¡ç®—èµ„æºå’Œæ•ˆç‡çš„é™åˆ¶ã€‚<br>

### HyDEæŠ€æœ¯æ˜¯ä»€ä¹ˆï¼Ÿ

æ ‡å‡†åŒ–è§£é‡Š:<br>

HyDEï¼ˆHypothetical Document Embeddingsï¼‰æŠ€æœ¯æ˜¯ä¸€ç§ä¿¡æ¯æ£€ç´¢æ–¹æ³•ï¼Œç‰¹åˆ«ç”¨äºé›¶æ ·æœ¬ï¼ˆZero-Shotï¼‰å¯†é›†æ£€ç´¢åœºæ™¯ï¼Œä¸éœ€è¦ç›¸å…³æ€§æ ‡ç­¾ã€‚è¿™é¡¹æŠ€æœ¯é€šè¿‡ç”Ÿæˆâ€œå‡è®¾æ€§æ–‡æ¡£â€æ¥é¿å¼€ä¼ ç»Ÿå¯†é›†æ£€ç´¢ä¸­éœ€è¦çš„ç›¸å…³æ€§æ ‡ç­¾ã€‚åœ¨HyDEä¸­ï¼Œä½¿ç”¨æŒ‡ä»¤è·Ÿéšå‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚InstructGPTï¼‰æ ¹æ®æŸ¥è¯¢ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£ï¼Œç„¶åå°†è¿™ä¸ªæ–‡æ¡£ç¼–ç åˆ°ä¸€ä¸ªæ–‡æ¡£ä»…åµŒå…¥ç©ºé—´ä¸­ï¼Œè¯¥ç©ºé—´é€šè¿‡æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ å¾—åˆ°ï¼Œèƒ½å¤Ÿæ•è·æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚è¿™ç§æ–¹æ³•å…è®¸ç³»ç»Ÿåœ¨æ²¡æœ‰ä»»ä½•æŸ¥è¯¢é›†ã€æ–‡æ¡£é›†æˆ–ä»»ä½•ç›¸å…³æ€§è¯„åˆ¤çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ æŸ¥è¯¢å’Œæ–‡æ¡£çš„åµŒå…¥å‡½æ•°ï¼Œä»è€Œå®ç°é›¶æ ·æœ¬å¯†é›†æ£€ç´¢ã€‚<br>

HyDEçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå®ƒçš„èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ²¡æœ‰ç»†ç²’åº¦è°ƒæ•´æˆ–ç›¸å…³æ€§æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£æ¥æé«˜æ£€ç´¢çš„å‡†ç¡®ç‡å’Œç›¸å…³æ€§ã€‚è¿™ç§ç­–ç•¥è¢«è®¤ä¸ºæ¯”ç®€å•çš„é›¶æ ·æœ¬æ£€ç´¢æ›´æœ‰æ•ˆï¼Œå°½ç®¡å®ƒç”Ÿæˆçš„æ–‡æ¡£å¯èƒ½å¹¶ä¸äº‹å®æ­£ç¡®ï¼Œä½†èƒ½å¤Ÿæ•è·æŸ¥è¯¢æ‰€éœ€çš„ç›¸å…³æ€§ç»“æ„ã€‚è¿™ç§æ–¹æ³•æ—¢é¿å¼€äº†ç›´æ¥å­¦ä¹ æŸ¥è¯¢å’Œæ–‡æ¡£ç¼–ç å™¨çš„é—®é¢˜ï¼Œä¹Ÿå°†é—®é¢˜ä¿æŒåœ¨æ–‡æ¡£åˆ°æ–‡æ¡£çš„æ£€ç´¢é¢†åŸŸå†…ï¼Œè¿™æ˜¯ä¸€ç§å·²çŸ¥çš„æ— ç›‘ç£æŠ€æœ¯ã€‚<br>

ç®€è€Œè¨€ä¹‹ï¼ŒHyDEæŠ€æœ¯é€šè¿‡ç”Ÿæˆä¸æŸ¥è¯¢ç›¸å…³ä½†å¯èƒ½ä¸å®Œå…¨å‡†ç¡®çš„å‡è®¾æ€§æ–‡æ¡£ï¼Œç„¶ååˆ©ç”¨è¿™äº›æ–‡æ¡£çš„åµŒå…¥æ¥è¿›è¡Œä¿¡æ¯æ£€ç´¢ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†é›¶æ ·æœ¬å¯†é›†æ£€ç´¢çš„æŒ‘æˆ˜ã€‚<br>

**ğŸ³ğŸ³ğŸ³ä¸Šé¢çš„ç­”æ¡ˆå¤ªè¿‡äºå­¦æœ¯ï¼Œç¬”è€…ä»¥ä¸€ç§æ›´æ˜“äºç†è§£çš„æ–¹å¼è§£é‡Šä¸€ä¸‹:**<br>

å‡è®¾ä½ æƒ³çŸ¥é“å¥¥è¿ä¼šæœ‰å“ªäº›æ¯”èµ›é¡¹ç›®ï¼Œå¦‚æœç›´æ¥ä»æ‰€æœ‰æ–‡æ¡£ä¸­æŸ¥æ‰¾ï¼Œæ£€ç´¢çš„æˆæœ¬è¿‡é«˜ï¼Œä¸”ç”±äºé—®å¥ä¿¡æ¯é‡çš„é™åˆ¶ï¼Œæ£€ç´¢å‡ºçš„ç»“æœå¯èƒ½ä¸å¤Ÿå‡†ç¡®ã€‚<br>

æ­¤æ—¶ï¼Œå¦‚æœé‡‡ç”¨è®©æ¨¡å‹å…ˆç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§çš„ç­”æ¡ˆï¼Œç„¶åç”¨è¿™ä¸ªå‡è®¾æ€§çš„ç­”æ¡ˆå»å’Œæ‰€æœ‰çœŸå®æ–‡æ¡£åŒ¹é…ï¼Œè¿™æ ·æ‰¾å‡ºçš„æœ€ç›¸ä¼¼çš„æ–‡æ¡£å¯ä¿¡åº¦æ›´é«˜ï¼Œè¾¾åˆ°çš„ç­”æ¡ˆä¹Ÿæ›´å‡†ç¡®ã€‚<br>

ç®€è€Œè¨€ä¹‹ï¼ŒHyDEæŠ€æœ¯é€šè¿‡åˆ›é€ ä¸€ä¸ªä¸­é—´æ­¥éª¤â€”â€”ç”Ÿæˆä¸€ä¸ªå‡è®¾çš„ç­”æ¡ˆæ–‡æ¡£ï¼Œæ¥å¸®åŠ©æ‰¾åˆ°ä½ çœŸæ­£éœ€è¦çš„ä¿¡æ¯ï¼Œå³ä½¿åœ¨å¾ˆå¤æ‚çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å·¥ä½œå¾—å¾ˆå¥½ã€‚è¿™æ ·ï¼Œå³ä½¿ä½ æå‡ºçš„é—®é¢˜å¾ˆéš¾ç›´æ¥æ‰¾åˆ°ç­”æ¡ˆï¼ŒHyDEæŠ€æœ¯ä¹Ÿèƒ½å¸®ä½ æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯ã€‚<br>

