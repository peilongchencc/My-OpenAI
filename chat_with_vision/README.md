# OpenAI Vision

æœ¬ç« ä»‹ç»ä½¿ç”¨å¤§æ¨¡å‹ä¸å›¾ç‰‡è¿›è¡Œäº¤äº’ï¼Œä¾‹å¦‚ç”¨æˆ·ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ï¼Œå¹¶æé—® "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ"ï¼Œå¤§æ¨¡å‹ä¼šåˆ†æå›¾ç‰‡ç„¶åè¿”å›ç»“æœã€‚<br>
- [OpenAI Vision](#openai-vision)
  - [ä¼ è¾“æ–¹å¼:](#ä¼ è¾“æ–¹å¼)
  - [ä»£ç ç¤ºä¾‹:](#ä»£ç ç¤ºä¾‹)
    - [ä¼ é€’å›¾ç‰‡é“¾æ¥ç¤ºä¾‹:](#ä¼ é€’å›¾ç‰‡é“¾æ¥ç¤ºä¾‹)
    - [ä¼ é€’base64ç¼–ç çš„å›¾ç‰‡ç¤ºä¾‹:](#ä¼ é€’base64ç¼–ç çš„å›¾ç‰‡ç¤ºä¾‹)
    - [Notes:](#notes)
  - [æ¨¡å‹å±€é™æ€§:](#æ¨¡å‹å±€é™æ€§)
  - [å‚æ•°è§£é‡Š:](#å‚æ•°è§£é‡Š)
  - [FAQ (Frequently Asked Questions):](#faq-frequently-asked-questions)
    - [ä¸ºä»€ä¹ˆè¦å°†å›¾ç‰‡è½¬åŒ–ä¸ºbase64ç¼–ç æ ¼å¼ï¼Ÿ](#ä¸ºä»€ä¹ˆè¦å°†å›¾ç‰‡è½¬åŒ–ä¸ºbase64ç¼–ç æ ¼å¼)


## ä¼ è¾“æ–¹å¼:

Images are made available to the model in two main ways: by passing a link to the image or by passing the base64 encoded image directly in the request.<br>

å›¾ç‰‡å¯ä»¥é€šè¿‡ä¸¤ç§ä¸»è¦æ–¹å¼æä¾›ç»™æ¨¡å‹ï¼šé€šè¿‡ä¼ é€’å›¾ç‰‡é“¾æ¥æˆ–åœ¨è¯·æ±‚ä¸­ç›´æ¥ä¼ é€’base64ç¼–ç çš„å›¾ç‰‡ã€‚<br>


## ä»£ç ç¤ºä¾‹:

### ä¼ é€’å›¾ç‰‡é“¾æ¥ç¤ºä¾‹:

```python
"""
Description: openaiè¯»å–è¿œç¨‹é“¾æ¥å‹å›¾ç‰‡ä»£ç ç¤ºä¾‹ã€‚
Notes: 
"""
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv("env_config/.env.local")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

response = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Whatâ€™s in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            "detail": "high"
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0].message.content)
```

### ä¼ é€’base64ç¼–ç çš„å›¾ç‰‡ç¤ºä¾‹:

```python
"""
Description: openaiè¯»å–æœ¬åœ°å›¾ç‰‡ä»£ç ç¤ºä¾‹ã€‚
Notes: 
"""
import os
import base64
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv("env_config/.env.local")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# å°†å›¾ç‰‡ç¼–ç ä¸ºbase64
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

image_path = "FizdHLbjxY.jpg"
# è·å– base64 string
base64_image = encode_image(image_path)

response = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "è¯·å¸®æˆ‘æå–å‡ºå›¾ç‰‡ä¸­çš„å†…å®¹"},
        {
          "type": "image_url",  
          "image_url": {
            "url": f"data:image/jpeg;base64,{base64_image}",
            "detail": "high"
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0].message.content)
```

### Notes:

The model is best at answering general questions about what is present in the images.<br>

è¯¥æ¨¡å‹æœ€æ“…é•¿å›ç­”æœ‰å…³å›¾åƒä¸­å­˜åœ¨çš„äº‹ç‰©çš„ä¸€èˆ¬é—®é¢˜ã€‚<br>

While it does understand the relationship between objects in images, it is not yet optimized to answer detailed questions about the location of certain objects in an image.<br>

è™½ç„¶å®ƒç¡®å®ç†è§£å›¾åƒä¸­å¯¹è±¡ä¹‹é—´çš„å…³ç³»ï¼Œä½†å°šæœªé’ˆå¯¹å›ç­”æœ‰å…³å›¾åƒä¸­æŸäº›å¯¹è±¡ä½ç½®çš„è¯¦ç»†é—®é¢˜è¿›è¡Œä¼˜åŒ–ã€‚<br>

For example, you can ask it what color a car is or what some ideas for dinner might be based on what is in your fridge, but if you show it an image of a room and ask it where the chair is, it may not answer the question correctly.<br>

ä¾‹å¦‚ï¼Œä½ å¯ä»¥é—®å®ƒè½¦æ˜¯ä»€ä¹ˆé¢œè‰²çš„ï¼Œæˆ–è€…æ ¹æ®å†°ç®±é‡Œçš„ç‰©å“è¯¢é—®æ™šé¤çš„å»ºè®®ï¼Œä½†å¦‚æœä½ ç»™å®ƒçœ‹ä¸€å¼ æˆ¿é—´çš„å›¾åƒå¹¶é—®æ¤…å­åœ¨å“ªé‡Œï¼Œå®ƒå¯èƒ½æ— æ³•æ­£ç¡®å›ç­”è¿™ä¸ªé—®é¢˜ã€‚<br>

It is important to keep in mind the limitations of the model as you explore what use-cases visual understanding can be applied to. <br>

åœ¨æ¢ç´¢è§†è§‰ç†è§£å¯ä»¥åº”ç”¨äºå“ªäº›ç”¨ä¾‹æ—¶ï¼Œç‰¢è®°è¯¥æ¨¡å‹çš„å±€é™æ€§æ˜¯å¾ˆé‡è¦çš„ã€‚<br>


## æ¨¡å‹å±€é™æ€§:

è™½ç„¶å…·å¤‡è§†è§‰åŠŸèƒ½çš„ GPT-4 åŠŸèƒ½å¼ºå¤§ï¼Œå¯ä»¥ç”¨äºè®¸å¤šåœºæ™¯ï¼Œä½†ç†è§£å…¶å±€é™æ€§éå¸¸é‡è¦ã€‚ä»¥ä¸‹æ˜¯äº†è§£åˆ°çš„ä¸€äº›å±€é™æ€§ï¼š<br>

- åŒ»ç–—å½±åƒï¼šè¯¥æ¨¡å‹ä¸é€‚ç”¨äºè§£é‡Šä¸“ä¸šçš„åŒ»ç–—å½±åƒï¼Œå¦‚ CT æ‰«æï¼Œä¹Ÿä¸åº”è¯¥ç”¨äºåŒ»ç–—å»ºè®®ã€‚

- éè‹±æ–‡ï¼šè¯¥æ¨¡å‹åœ¨å¤„ç†å«æœ‰éæ‹‰ä¸å­—æ¯æ–‡æœ¬çš„å›¾åƒæ—¶ï¼Œå¯èƒ½è¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚æ—¥æ–‡æˆ–éŸ©æ–‡ã€‚

- å°æ–‡æœ¬ï¼šæ”¾å¤§å›¾åƒä¸­çš„æ–‡æœ¬ä»¥æé«˜å¯è¯»æ€§ï¼Œä½†é¿å…è£å‰ªæ‰é‡è¦ç»†èŠ‚ã€‚

- æ—‹è½¬ï¼šè¯¥æ¨¡å‹å¯èƒ½ä¼šè¯¯è§£æ—‹è½¬æˆ–å€’ç½®çš„æ–‡æœ¬æˆ–å›¾åƒã€‚

- è§†è§‰å…ƒç´ ï¼šè¯¥æ¨¡å‹å¯èƒ½éš¾ä»¥ç†è§£å›¾è¡¨æˆ–é¢œè‰²å’Œæ ·å¼ï¼ˆå¦‚å®çº¿ã€è™šçº¿æˆ–ç‚¹çº¿ï¼‰å˜åŒ–çš„æ–‡æœ¬ã€‚

- ç©ºé—´æ¨ç†ï¼šè¯¥æ¨¡å‹åœ¨éœ€è¦ç²¾ç¡®ç©ºé—´å®šä½çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚è¯†åˆ«å›½é™…è±¡æ£‹ä½ç½®ã€‚

- å‡†ç¡®æ€§ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹å¯èƒ½ç”Ÿæˆä¸å‡†ç¡®çš„æè¿°æˆ–å­—å¹•ã€‚

- å›¾åƒå½¢çŠ¶ï¼šè¯¥æ¨¡å‹åœ¨å¤„ç†å…¨æ™¯å’Œé±¼çœ¼å›¾åƒæ—¶è¡¨ç°ä¸ä½³ã€‚

- å…ƒæ•°æ®å’Œè°ƒæ•´å¤§å°ï¼šè¯¥æ¨¡å‹ä¸ä¼šå¤„ç†åŸå§‹æ–‡ä»¶åæˆ–å…ƒæ•°æ®ï¼Œå›¾åƒåœ¨åˆ†æå‰ä¼šè¢«è°ƒæ•´å¤§å°ï¼Œå½±å“å…¶åŸå§‹å°ºå¯¸ã€‚

- è®¡æ•°ï¼šå¯èƒ½å¯¹å›¾åƒä¸­çš„ç‰©ä½“æ•°é‡è¿›è¡Œå¤§è‡´ä¼°è®¡ã€‚

**ğŸš¨ç¬”è€…ä½¿ç”¨è¿‡ç¨‹ä¸­çš„ç»éªŒ:**<br>

åˆ©ç”¨gpt visionåšOCRçš„æ•ˆæœä¸ç†æƒ³ï¼Œæ¨æµ‹è®­ç»ƒæ–¹å¼ä¸º "å›¾A <--pair--> å›¾Açš„æè¿°æ€§æ–‡æœ¬"ï¼Œæ‰€ä»¥ç”¨æˆ·è¯¢é—®ç±»ä¼¼ "å›¾ä¸­æœ‰å“ªäº›å†…å®¹ï¼Ÿ" çš„é—®é¢˜æ—¶æ•ˆæœå¥½ï¼Œä½†è¯¢é—® "è¯·å¸®æˆ‘å°†å›¾ç‰‡ä¸­çš„æ–‡æœ¬æå–å‡ºæ¥" æ•ˆæœä¸ç†æƒ³ã€‚<br>


## å‚æ•°è§£é‡Š:

By controlling the `detail` parameter, which has three options, `low`, `high`, or `auto`, you have control over how the model processes the image and generates its textual understanding.<br>

é€šè¿‡æ§åˆ¶ç»†èŠ‚å‚æ•°ï¼Œè¯¥å‚æ•°æœ‰ä¸‰ä¸ªé€‰é¡¹ï¼š`low`ã€`high`æˆ–`auto`ï¼Œæ‚¨å¯ä»¥æ§åˆ¶æ¨¡å‹å¦‚ä½•å¤„ç†å›¾åƒå¹¶ç”Ÿæˆå…¶æ–‡æœ¬ç†è§£ã€‚<br>

By default, the model will use the `auto` setting which will look at the image input size and decide if it should use the `low` or `high` setting.<br>

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹å°†ä½¿ç”¨ `auto` è®¾ç½®ï¼Œè¯¥è®¾ç½®å°†æŸ¥çœ‹å›¾åƒè¾“å…¥å¤§å°å¹¶å†³å®šæ˜¯å¦åº”ä½¿ç”¨ `low` æˆ– `high` è®¾ç½®ã€‚<br>

ğŸ”¥`low` will enable the "low res" mode. / `low` å°†å¯ç”¨â€œä½åˆ†è¾¨ç‡â€æ¨¡å¼ã€‚<br>

The model will receive a low-res 512px x 512px version of the image, and represent the image with a budget of 85 tokens.<br>

æ¨¡å‹å°†æ¥æ”¶ä¸€ä¸ªä½åˆ†è¾¨ç‡çš„512px x 512pxç‰ˆæœ¬çš„å›¾åƒï¼Œå¹¶ä½¿ç”¨85ä¸ªæ ‡è®°æ¥è¡¨ç¤ºå›¾åƒã€‚<br>

This allows the API to return faster responses and consume fewer input tokens for use cases that do not require high detail.<br>

è¿™ä½¿APIèƒ½å¤Ÿåœ¨ä¸éœ€è¦é«˜ç»†èŠ‚çš„ç”¨ä¾‹ä¸­è¿”å›æ›´å¿«çš„å“åº”å¹¶æ¶ˆè€—æ›´å°‘çš„è¾“å…¥æ ‡è®°ã€‚<br>

ğŸ”¥`high` will enable "high res" mode, which first allows the model to first see the low res image (using 85 tokens) and then creates detailed crops using 170 tokens for each 512px x 512px tile.<br>

`high`å°†å¯ç”¨â€œé«˜åˆ†è¾¨ç‡â€æ¨¡å¼ï¼Œè¯¥æ¨¡å¼é¦–å…ˆå…è®¸æ¨¡å‹çœ‹åˆ°ä½åˆ†è¾¨ç‡å›¾åƒï¼ˆä½¿ç”¨85ä¸ªæ ‡è®°ï¼‰ï¼Œç„¶åä¸ºæ¯ä¸ª512px x 512pxçš„å›¾å—ä½¿ç”¨170ä¸ªæ ‡è®°åˆ›å»ºè¯¦ç»†çš„è£å‰ªã€‚<br>


## FAQ (Frequently Asked Questions): 

### ä¸ºä»€ä¹ˆè¦å°†å›¾ç‰‡è½¬åŒ–ä¸ºbase64ç¼–ç æ ¼å¼ï¼Ÿ

å°†æœ¬åœ°å›¾åƒè½¬åŒ–ä¸ºbase64ç¼–ç æ ¼å¼æ˜¯ä¸ºäº†åœ¨HTTPè¯·æ±‚ä¸­å‘é€å›¾åƒæ•°æ®ã€‚HTTPè¯·æ±‚é€šå¸¸æ˜¯ä»¥æ–‡æœ¬æ ¼å¼å‘é€çš„ï¼Œbase64ç¼–ç æ˜¯ä¸€ç§å¸¸è§çš„æ–¹æ³•ï¼Œå¯ä»¥å°†äºŒè¿›åˆ¶æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰è½¬åŒ–ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œä»¥ä¾¿åµŒå…¥åˆ°è¯·æ±‚ä¸­ã€‚<br>