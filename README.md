# openai_parse:

æœ¬æ–‡æ¡£åŸºäºOpenAIå®˜ç½‘ä»‹ç»ï¼Œä¸»è¦ç”¨äºä¸ªäººç†è§£ä¸APIè°ƒç”¨æµ‹è¯•ã€‚<br>

**æ–‡ä»¶ä»‹ç»:**<br>

- `fastapi_sse_example.py` : å¼‚æ­¥æ–¹å¼å®ç°fastapiè°ƒç”¨OpenAIæœåŠ¡ï¼Œç»“æœä»¥sseæ–¹å¼ä¼ è¾“ã€‚

- `sanic_sse_example.py` : å¼‚æ­¥æ–¹å¼å®ç°Sanicè°ƒç”¨OpenAIæœåŠ¡ï¼Œç»“æœä»¥sseæ–¹å¼ä¼ è¾“ã€‚

- `gradio_stream_openai.py` : å¼‚æ­¥æ–¹å¼æ¥æ”¶FastAPIæˆ–Sanicä»¥sseæ–¹å¼ä¼ è¾“çš„æ•°æ®ï¼Œç„¶åä»¥ç•Œé¢æ–¹å¼å‘ˆç°ã€‚

- `openai_test.py`: åŒæ­¥å†™æ³•ï¼Œæ”¯æŒç»ˆç«¯ç›´æ¥æµ‹è¯•openaiæ•ˆæœã€‚(å‰æ:ç»ˆç«¯éœ€èƒ½è¿æ¥openaiæœåŠ¡)

**OpenAIå®˜ç½‘å†…å®¹ä»‹ç»:**<br>

- [openai\_parse:](#openai_parse)
  - [Key concepts(å…³é”®æ¦‚å¿µ):](#key-conceptså…³é”®æ¦‚å¿µ)
    - [Text generation models(æ–‡æœ¬ç”Ÿæˆæ¨¡å‹):](#text-generation-modelsæ–‡æœ¬ç”Ÿæˆæ¨¡å‹)
    - [Assistant:](#assistant)
    - [Embeddings(è¯åµŒå…¥):](#embeddingsè¯åµŒå…¥)
    - [Tokens:](#tokens)
  - [Developer quickstart(å¼€å‘è€…å¿«é€Ÿå…¥é—¨):](#developer-quickstartå¼€å‘è€…å¿«é€Ÿå…¥é—¨)
    - [Get up and running with the OpenAI API(å¿«é€Ÿå¼€å§‹ä½¿ç”¨OpenAI API):](#get-up-and-running-with-the-openai-apiå¿«é€Ÿå¼€å§‹ä½¿ç”¨openai-api)
    - [Account setup(è´¦æˆ·è®¾ç½®):](#account-setupè´¦æˆ·è®¾ç½®)
    - [API Keys:](#api-keys)
    - [Quickstart language selection(å¿«é€Ÿå¼€å§‹è¯­è¨€é€‰æ‹©):](#quickstart-language-selectionå¿«é€Ÿå¼€å§‹è¯­è¨€é€‰æ‹©)
    - [Set your API key(è®¾ç½®ä½ çš„ API å¯†é’¥):](#set-your-api-keyè®¾ç½®ä½ çš„-api-å¯†é’¥)
      - [Seetup your API key for all projects(recommended)(ä¸ºæ‰€æœ‰é¡¹ç›®è®¾ç½®ä½ çš„ API å¯†é’¥)(æ¨è):](#seetup-your-api-key-for-all-projectsrecommendedä¸ºæ‰€æœ‰é¡¹ç›®è®¾ç½®ä½ çš„-api-å¯†é’¥æ¨è)
      - [Setup your API key for a single project(ä¸ºå•ä¸ªé¡¹ç›®è®¾ç½®ä½ çš„ API å¯†é’¥):](#setup-your-api-key-for-a-single-projectä¸ºå•ä¸ªé¡¹ç›®è®¾ç½®ä½ çš„-api-å¯†é’¥)
    - [Sending your first API request(å‘é€ä½ çš„ç¬¬ä¸€ä¸ªAPIè¯·æ±‚):](#sending-your-first-api-requestå‘é€ä½ çš„ç¬¬ä¸€ä¸ªapiè¯·æ±‚)
      - [chatcompletions(èŠå¤©è¡¥å…¨):](#chatcompletionsèŠå¤©è¡¥å…¨)
      - [Embedding:](#embedding)
      - [images:](#images)
    - [chatcompletions with dotenv:](#chatcompletions-with-dotenv)
      - [unstreaming(éæµå¼è¾“å‡º):](#unstreamingéæµå¼è¾“å‡º)
      - [streaming(æµå¼è¾“å‡º):](#streamingæµå¼è¾“å‡º)
      - [multi\_turn\_dialogue(å¤šè½®å¯¹è¯):](#multi_turn_dialogueå¤šè½®å¯¹è¯)
      - [å¼‚æ­¥æ–¹å¼è°ƒç”¨--å®˜æ–¹ç¤ºä¾‹:](#å¼‚æ­¥æ–¹å¼è°ƒç”¨--å®˜æ–¹ç¤ºä¾‹)
      - [å¼‚æ­¥æ–¹å¼è°ƒç”¨--ä½¿ç”¨dotenvçš„ç®€å•ç¤ºä¾‹:](#å¼‚æ­¥æ–¹å¼è°ƒç”¨--ä½¿ç”¨dotenvçš„ç®€å•ç¤ºä¾‹)
      - [APIä¸­çš„systemã€userã€assistantä½œç”¨è§£æ:](#apiä¸­çš„systemuserassistantä½œç”¨è§£æ)
    - [Next steps(æ¥ä¸‹æ¥çš„æ­¥éª¤):](#next-stepsæ¥ä¸‹æ¥çš„æ­¥éª¤)
  - [settings:](#settings)
    - [Billing settings(è´¦å•è®¾ç½®):](#billing-settingsè´¦å•è®¾ç½®)
  - [Embeddings](#embeddings)
    - [What are embeddings?](#what-are-embeddings)
    - [How to get embeddings(å¦‚ä½•è·å¾—è¯å‘é‡)](#how-to-get-embeddingså¦‚ä½•è·å¾—è¯å‘é‡)
    - [Embedding models(è¯å‘é‡æ¨¡å‹):](#embedding-modelsè¯å‘é‡æ¨¡å‹)
    - [Use cases(åº”ç”¨æ¡ˆä¾‹):](#use-casesåº”ç”¨æ¡ˆä¾‹)
      - [Obtaining the embeddings(è·å–è¯å‘é‡):](#obtaining-the-embeddingsè·å–è¯å‘é‡)
      - [Reducing embedding dimensions(é™ä½è¯å‘é‡ç»´åº¦):](#reducing-embedding-dimensionsé™ä½è¯å‘é‡ç»´åº¦)
      - [éªŒè¯ "æ‰‹åŠ¨é™ä½ç»´åº¦" å’Œ "é€šè¿‡ä¼ å‚é™ä½ç»´åº¦" çš„åŒºåˆ«:](#éªŒè¯-æ‰‹åŠ¨é™ä½ç»´åº¦-å’Œ-é€šè¿‡ä¼ å‚é™ä½ç»´åº¦-çš„åŒºåˆ«)
      - [å…³äº"`text-embedding-3-large` è¯å‘é‡ç¼©çŸ­åˆ°256çš„å¤§å°ï¼Œè€Œä»ç„¶æ¯”æœªç¼©çŸ­çš„text-embedding-ada-002åµŒå…¥å‘é‡ï¼ˆå¤§å°ä¸º1536ï¼‰è¡¨ç°å¾—æ›´å¥½"çš„ä¸€äº›æ€è€ƒ:](#å…³äºtext-embedding-3-large-è¯å‘é‡ç¼©çŸ­åˆ°256çš„å¤§å°è€Œä»ç„¶æ¯”æœªç¼©çŸ­çš„text-embedding-ada-002åµŒå…¥å‘é‡å¤§å°ä¸º1536è¡¨ç°å¾—æ›´å¥½çš„ä¸€äº›æ€è€ƒ)
      - [Question answering using embeddings-based search(åŸºäºè¯å‘é‡æ£€ç´¢è¿›è¡Œé—®é¢˜å›ç­”):](#question-answering-using-embeddings-based-searchåŸºäºè¯å‘é‡æ£€ç´¢è¿›è¡Œé—®é¢˜å›ç­”)
      - [å…¶ä»–ç¤ºä¾‹ä¸ä¸Šè¿°ç”¨æ³•ç›¸ä¼¼ï¼Œè¿™é‡Œå°±ä¸å¤šä»‹ç»ã€‚](#å…¶ä»–ç¤ºä¾‹ä¸ä¸Šè¿°ç”¨æ³•ç›¸ä¼¼è¿™é‡Œå°±ä¸å¤šä»‹ç»)
    - [Frequently asked questions(å¸¸è§é—®é¢˜è§£ç­”):](#frequently-asked-questionså¸¸è§é—®é¢˜è§£ç­”)
      - [Example code(ç¤ºä¾‹ä»£ç ):](#example-codeç¤ºä¾‹ä»£ç )
      - [How can I retrieve K nearest embedding vectors quickly?(å¦‚ä½•å¿«é€Ÿæ£€ç´¢Kä¸ªæœ€è¿‘çš„åµŒå…¥å‘é‡)](#how-can-i-retrieve-k-nearest-embedding-vectors-quicklyå¦‚ä½•å¿«é€Ÿæ£€ç´¢kä¸ªæœ€è¿‘çš„åµŒå…¥å‘é‡)
      - [Which distance function should I use?(åº”è¯¥ä½¿ç”¨å“ªç§è·ç¦»å‡½æ•°)](#which-distance-function-should-i-useåº”è¯¥ä½¿ç”¨å“ªç§è·ç¦»å‡½æ•°)
      - [Do V3 embedding models know about recent events?(V3åµŒå…¥æ¨¡å‹çŸ¥é“æœ€è¿‘çš„äº‹ä»¶å—)](#do-v3-embedding-models-know-about-recent-eventsv3åµŒå…¥æ¨¡å‹çŸ¥é“æœ€è¿‘çš„äº‹ä»¶å—)

"Head to chat.openai.com."ï¼šè¿™éƒ¨åˆ†æ˜¯ä¸€ä¸ªå»ºè®®æˆ–æŒ‡ä»¤ï¼Œæ„æ€æ˜¯â€œå‰å¾€ chat.openai.comã€‚â€ã€‚â€œHead toâ€æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„è‹±è¯­çŸ­è¯­ï¼Œç”¨æ¥å»ºè®®æŸäººå»æŸä¸ªåœ°æ–¹ã€‚åœ¨è¿™é‡Œï¼Œå®ƒæ„å‘³ç€å¦‚æœä½ æƒ³ä½¿ç”¨æˆ–äº†è§£æ›´å¤šå…³äºChatGPTçš„ä¿¡æ¯ï¼Œåº”è¯¥è®¿é—®ç½‘å€â€œchat.openai.comâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªç‰¹å®šçš„ç½‘ç«™é“¾æ¥ã€‚<br>

Explore the API(æ¢ç´¢è¿™ä¸ªåº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ (API))<br>

[Watch the first OpenAI Developer Day keynote(è§‚çœ‹é¦–å±Š OpenAI å¼€å‘è€…æ—¥ä¸»é¢˜æ¼”è®²)](https://youtu.be/U9mJuUkhUzk)

> OpenAI Developer Day:æŒ‡çš„æ˜¯ç”± OpenAI ç»„ç»‡çš„ä¸€ä¸ªå¼€å‘è€…æ—¥æ´»åŠ¨ã€‚
> keynote: è¿™ä¸ªè¯åœ¨è¿™é‡ŒæŒ‡çš„æ˜¯æŸä¸ªä¼šè®®æˆ–æ´»åŠ¨ä¸­çš„ä¸»è¦æ¼”è®²æˆ–ä¸»é¢˜æ¼”è®²ã€‚é€šå¸¸ï¼Œè¿™ç§æ¼”è®²ç”±é‡è¦äººç‰©è¿›è¡Œï¼Œæ—¨åœ¨é˜è¿°ä¼šè®®çš„ä¸»è¦ä¸»é¢˜æˆ–ä¼ è¾¾é‡è¦ä¿¡æ¯ã€‚

The OpenAI API can be applied to virtually any task. We offer a range of models with different capabilities and price points, as well as the ability to fine-tune custom models.

OpenAI API å®é™…ä¸Šå¯ä»¥åº”ç”¨äºä»»ä½•ä»»åŠ¡ã€‚æˆ‘ä»¬æä¾›ä¸€ç³»åˆ—å…·æœ‰ä¸åŒåŠŸèƒ½å’Œä»·æ ¼ç‚¹çš„æ¨¡å‹ï¼Œä»¥åŠå¾®è°ƒå®šåˆ¶æ¨¡å‹çš„èƒ½åŠ›ã€‚


ğŸš¨ğŸš¨ğŸš¨Note:<br>

----

ä½ å¯ä»¥åœ¨ä½¿ç”¨ openAI API çš„è¿‡ç¨‹ä¸­è§åˆ°ä»¥ä¸‹å›¾ç‰‡:

![](./materials/uasge_limit.jpg)

è¿™æ˜¯æé†’ä½ :

ä½ å·²è¾¾åˆ°ä½¿ç”¨é™é¢ã€‚æ¬²äº†è§£æ›´å¤šè¯¦æƒ…ï¼Œè¯·æŸ¥çœ‹ä½ çš„ä½¿ç”¨ä»ªè¡¨æ¿å’Œè´¦å•è®¾ç½®ã€‚å¦‚æœä½ è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·é€šè¿‡æˆ‘ä»¬çš„å¸®åŠ©ä¸­å¿ƒ help.openai.com ä¸æˆ‘ä»¬è”ç³»ã€‚

----


## Key concepts(å…³é”®æ¦‚å¿µ):

### Text generation models(æ–‡æœ¬ç”Ÿæˆæ¨¡å‹):

OpenAI's text generation models (often referred to as generative pre-trained transformers or "GPT" models for short), like GPT-4 and GPT-3.5, have been trained to understand natural and formal(æ­£å¼çš„) language. Models like GPT-4 allows text outputs in response to their inputs. **The inputs to these models are also referred to as "prompts".** Designing a prompt is essentially(æœ¬è´¨ä¸Š) how you "program" a model like GPT-4, usually by providing instructions(æŒ‡ä»¤) or some examples of how to successfully complete a task. Models like GPT-4 can be used across a great variety of tasks including content or code generation, summarization, conversation, creative writing(åˆ›æ„å†™ä½œ), and more. Read more in our introductory text generation guide and in our prompt engineering guide.<br>

OpenAIçš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ˆé€šå¸¸è¢«ç§°ä¸ºç”Ÿæˆå¼é¢„è®­ç»ƒtransformersï¼Œç®€ç§°â€œGPTâ€æ¨¡å‹ï¼‰ï¼Œæ¯”å¦‚GPT-4å’ŒGPT-3.5ï¼Œå·²è¢«è®­ç»ƒä»¥ç†è§£è‡ªç„¶è¯­è¨€å’Œæ­£å¼è¯­è¨€ã€‚åƒGPT-4è¿™æ ·çš„æ¨¡å‹å¯ä»¥æ ¹æ®è¾“å…¥ç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚**è¿™äº›æ¨¡å‹çš„è¾“å…¥ä¹Ÿè¢«ç§°ä¸ºâ€œæç¤ºâ€**ã€‚è®¾è®¡ä¸€ä¸ªæç¤ºæœ¬è´¨ä¸Šå°±æ˜¯å¦‚ä½•â€œç¼–ç¨‹â€ä¸€ä¸ªåƒGPT-4è¿™æ ·çš„æ¨¡å‹ï¼Œé€šå¸¸æ˜¯é€šè¿‡æä¾›æŒ‡ä»¤æˆ–ä¸€äº›ç¤ºä¾‹æ¥å±•ç¤ºå¦‚ä½•æˆåŠŸå®Œæˆä¸€ä¸ªä»»åŠ¡ã€‚åƒGPT-4è¿™æ ·çš„æ¨¡å‹å¯ä»¥åº”ç”¨äºå¹¿æ³›çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å†…å®¹æˆ–ä»£ç ç”Ÿæˆã€æ‘˜è¦ã€å¯¹è¯ã€åˆ›æ„å†™ä½œç­‰ç­‰ã€‚æ¬²äº†è§£æ›´å¤šï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„å…¥é—¨æ–‡æœ¬ç”ŸæˆæŒ‡å—å’Œæç¤ºå·¥ç¨‹æŒ‡å—ã€‚<br>

### Assistant:

Assistants refer to entities, which in the case of the OpenAI API are powered by large language models like GPT-4, that are capable of performing(æ‰§è¡Œ) tasks for users. These assistants operate based on the instructions(æŒ‡ä»¤) embedded within the context window of the model. They also usually have access to tools which allows the assistants to perform more complex tasks like running code or retrieving(æ£€ç´¢) information from a file. Read more about assistants in our Assistants API Overview.<br>

Assistantsæ˜¯æŒ‡ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰é©±åŠ¨çš„å®ä½“ï¼Œåœ¨ OpenAI API çš„æƒ…å†µä¸‹ï¼Œè¿™äº›Assistantsèƒ½å¤Ÿä¸ºç”¨æˆ·æ‰§è¡Œä»»åŠ¡ã€‚è¿™äº›Assistantsçš„è¿ä½œåŸºäºåµŒå…¥åœ¨æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ä¸­çš„æŒ‡ä»¤ã€‚å®ƒä»¬é€šå¸¸è¿˜å¯ä»¥è®¿é—®å·¥å…·ï¼Œä½¿Assistantsèƒ½å¤ æ‰§è¡Œæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚è¿è¡Œä»£ç æˆ–ä»æ–‡ä»¶ä¸­æ£€ç´¢ä¿¡æ¯ã€‚æ¬²äº†è§£æ›´å¤šå…³äºAssistantsçš„ä¿¡æ¯ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„Assistants API æ¦‚è§ˆã€‚<br>

### Embeddings(è¯åµŒå…¥):

An embedding is a vector representation(è¡¨ç¤º) of a piece of data (e.g. some text) that is meant to preserve(ä¿ç•™) aspects(æ–¹é¢) of its content and/or its meaning. Chunks of data(æ•°æ®å—) that are similar in some way will tend(è¶‹å‘) to have embeddings that are closer together than unrelated data. OpenAI offers text embedding models that take as input a text string and produce as output an embedding vector. Embeddings are useful for search, clustering, recommendations, anomaly(å¼‚å¸¸) detection, classification, and more. Read more about embeddings in our embeddings guide.<br>

Embeddingæ˜¯ä¸€ç§æ•°æ®ï¼ˆä¾‹å¦‚æŸäº›æ–‡æœ¬ï¼‰çš„å‘é‡è¡¨ç¤ºå½¢å¼ï¼Œæ—¨åœ¨ä¿ç•™å…¶å†…å®¹å’Œ/æˆ–å«ä¹‰çš„æŸäº›æ–¹é¢ã€‚åœ¨æŸç§ç¨‹åº¦ä¸Šç›¸ä¼¼çš„æ•°æ®å—ï¼Œå…¶embeddingé€šå¸¸ä¼šæ¯”ä¸ç›¸å…³æ•°æ®çš„embeddingæ›´ä¸ºæ¥è¿‘ã€‚OpenAIæä¾›äº†æ–‡æœ¬embeddingæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä»¥æ–‡æœ¬å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œå¹¶äº§ç”Ÿembeddingå‘é‡ä½œä¸ºè¾“å‡ºã€‚Embeddingå¯¹äºæœç´¢ã€èšç±»ã€æ¨èã€å¼‚å¸¸æ£€æµ‹ã€åˆ†ç±»ç­‰é¢†åŸŸéå¸¸æœ‰ç”¨ã€‚æƒ³äº†è§£æ›´å¤šå…³äºembeddingçš„ä¿¡æ¯ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„embeddingæŒ‡å—ã€‚<br>

### Tokens:

Text generation and embeddings models process text in chunks called tokens. Tokens represent commonly occurring sequences of characters. For example, the string " tokenization" is decomposed(åˆ†è§£) as " token" and "ization", while a short and common word like " the" is represented as a single token. **Note that in a sentence, the first token of each word typically starts with a space character.** Check out our tokenizer tool(åˆ†è¯å·¥å…·) to test specific strings and see how they are translated into tokens. As a rough rule of thumb, 1 token is approximately(å¤§æ¦‚) 4 characters(å­—ç¬¦) or 0.75 words for English text.<br>

> "thumb" åœ¨è‹±è¯­ä¸­çš„å­—é¢æ„æ€æ˜¯â€œæ‹‡æŒ‡â€ã€‚ä½†åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œ"rule of thumb" æ˜¯ä¸€ä¸ªæˆè¯­ï¼Œæ„æ€æ˜¯â€œç»éªŒæ³•åˆ™â€æˆ–â€œç²—ç•¥çš„ä¼°è®¡æ–¹æ³•â€ã€‚

æ–‡æœ¬ç”Ÿæˆå’ŒEmbeddingsæ¨¡å‹é€šè¿‡è¢«ç§°ä¸º tokens çš„å•å…ƒæ¥å¤„ç†æ–‡æœ¬ã€‚Tokens ä»£è¡¨å¸¸è§çš„å­—ç¬¦åºåˆ—ã€‚ä¾‹å¦‚ï¼Œå­—ç¬¦ä¸² " tokenization" è¢«æ‹†åˆ†ä¸º " token" å’Œ "ization"ï¼Œè€Œåƒ " the" è¿™æ ·çŸ­å°ä¸”å¸¸è§çš„å•è¯åˆ™è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªå•ç‹¬çš„ tokenã€‚**è¯·æ³¨æ„ï¼Œåœ¨ä¸€ä¸ªå¥å­ä¸­ï¼Œæ¯ä¸ªå•è¯çš„ç¬¬ä¸€ä¸ª token ğŸ¤¨**ä½ å¯ä»¥æŸ¥çœ‹æˆ‘ä»¬çš„åˆ†è¯å·¥å…·ï¼Œæµ‹è¯•ç‰¹å®šå­—ç¬¦ä¸²ï¼Œå¹¶æŸ¥çœ‹å®ƒä»¬æ˜¯å¦‚ä½•è¢«è½¬æ¢ä¸º tokens çš„ã€‚ä½œä¸ºä¸€ä¸ªç²—ç•¥çš„ç»éªŒæ³•åˆ™ï¼Œå¯¹äºè‹±æ–‡æ–‡æœ¬æ¥è¯´ï¼Œ1ä¸ª token å¤§çº¦ç›¸å½“äº4ä¸ªå­—ç¬¦æˆ–0.75ä¸ªå•è¯ã€‚<br>


## Developer quickstart(å¼€å‘è€…å¿«é€Ÿå…¥é—¨):

### Get up and running with the OpenAI API(å¿«é€Ÿå¼€å§‹ä½¿ç”¨OpenAI API):

> â€œGet up and runningâ€ æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„è‹±è¯­çŸ­è¯­ï¼Œæ„æ€æ˜¯è¿…é€Ÿå¼€å§‹æˆ–è¿…é€ŸæŠ•å…¥åˆ°æŸäº‹ä¸­ã€‚å­—é¢ä¸Šï¼Œâ€œget upâ€æ„å‘³ç€èµ·èº«æˆ–èµ·ç«‹ï¼Œä½†åœ¨è¿™é‡Œå®ƒæ›´å¤šåœ°è¡¨è¾¾çš„æ˜¯å¼€å§‹è¡ŒåŠ¨æˆ–å¯åŠ¨çš„æ„æ€ã€‚

The OpenAI API provides a simple interface for developers(å¼€å‘è€…) to create an intelligence layer(æ™ºèƒ½å±‚) in their applications, powered by OpenAI's **state of the art(æœ€å…ˆè¿›çš„)** models. The Chat Completions(å®Œæˆ) endpoint powers ChatGPT and provides a simple way to take text as input and use a model like GPT-4 to generate an output.<br>

OpenAIçš„APIä¸ºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªç®€å•çš„æ¥å£ï¼Œç”¨äºåœ¨ä»–ä»¬çš„åº”ç”¨ç¨‹åºä¸­åˆ›å»ºä¸€ä¸ªç”±OpenAIæœ€å…ˆè¿›çš„æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½å±‚ã€‚Chat Completionsç«¯ç‚¹é©±åŠ¨äº†ChatGPTï¼Œå¹¶æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹å¼ï¼Œå³æ¥å—æ–‡æœ¬è¾“å…¥ï¼Œå¹¶ä½¿ç”¨åƒGPT-4è¿™æ ·çš„æ¨¡å‹æ¥ç”Ÿæˆè¾“å‡ºã€‚<br>

This quickstart is designed to help get your local development environment setup(è®¾ç½®ï¼›æ­å»º) and send your first API request(è¯·æ±‚). If you are an experienced(ç»éªŒä¸°å¯Œçš„) developer or want to just dive into("æ·±å…¥ç ”ç©¶";"è¿…é€ŸæŠ•å…¥") using the OpenAI API, the API reference of GPT guide are a great place to start. Throughout this quickstart, you will learn:<br>

è¿™ä»½å¿«é€Ÿå…¥é—¨æ—¨åœ¨å¸®åŠ©ä½ æ­å»ºæœ¬åœ°å¼€å‘ç¯å¢ƒå¹¶å‘é€ä½ çš„é¦–ä¸ªAPIè¯·æ±‚ã€‚å¦‚æœä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„å¼€å‘è€…ï¼Œæˆ–è€…æƒ³ç›´æ¥æ·±å…¥ä½¿ç”¨OpenAI APIï¼Œé‚£ä¹ˆ**GPTæŒ‡å—ä¸­çš„APIå‚è€ƒæ–‡æ¡£**æ˜¯ä¸€ä¸ªç»ä½³çš„èµ·ç‚¹ã€‚åœ¨è¿™ä»½å¿«é€Ÿå…¥é—¨ä¸­ï¼Œä½ å°†å­¦ä¹ ï¼š<br>

- How to setup your development environment(å¦‚ä½•æ­å»ºä½ çš„å¼€å‘ç¯å¢ƒ)
- How to install the latest SDKs(å¦‚ä½•å®‰è£…æœ€æ–°çš„SDK)
- Some of the basic concepts of the OpenAI API(OpenAI APIçš„ä¸€äº›åŸºæœ¬æ¦‚å¿µ)
- How to send your first API request(å¦‚ä½•å‘é€ä½ çš„é¦–ä¸ªAPIè¯·æ±‚)

If you run into any challenges or have questions getting started, please join our developer forum(è®ºå›).<br>

å¦‚æœåœ¨å¼€å§‹è¿‡ç¨‹ä¸­é‡åˆ°ä»»ä½•æŒ‘æˆ˜æˆ–æœ‰é—®é¢˜ï¼Œè¯·åŠ å…¥æˆ‘ä»¬çš„å¼€å‘è€…è®ºå›ã€‚<br>

### Account setup(è´¦æˆ·è®¾ç½®):

First, create an OpenAI account or sign in. Next, navigate(å‰å¾€ï¼›å¯¼èˆªè‡³) to the [API key page](https://platform.openai.com/api-keys) and "Create new secret key", optionally(å¯é€‰) naming the key. Make sure to save this somewhere safe and do not share it with anyone.<br>

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªOpenAIè´¦æˆ·æˆ–ç™»å½•ã€‚æ¥ç€ï¼Œå‰å¾€â¤ï¸**APIå¯†é’¥é¡µé¢**å¹¶â€œåˆ›å»ºæ–°çš„å¯†é’¥â€ï¼Œå¯ä»¥é€‰æ‹©ä¸ºå¯†é’¥å‘½åã€‚ç¡®ä¿å°†å…¶ä¿å­˜åœ¨å®‰å…¨çš„åœ°æ–¹ï¼Œå¹¶ä¸”ä¸è¦ä¸ä»»ä½•äººåˆ†äº«ã€‚<br>


### API Keys:

Your secret API keys are listed below. Please note that **ğŸš¨we do not display your secret API keys againğŸš¨** after you generate them.<br>

ä½ çš„ `secret API keys` å¦‚ä¸‹æ‰€åˆ—ã€‚è¯·æ³¨æ„ï¼Œä¸€æ—¦ä½ ç”Ÿæˆè¿™äº›å¯†é’¥ï¼Œæˆ‘ä»¬å°†ä¸ä¼šå†æ¬¡æ˜¾ç¤ºå®ƒä»¬ã€‚<br>

Do not share your API key with others, or expose(æš´éœ²) it in the browser(æµè§ˆå™¨) or other client-side(å®¢æˆ·ç«¯) code. In order to protect the security of your account, OpenAI may also automatically(è‡ªåŠ¨åœ°) disable(ä½¿æ— æ•ˆ) any API key that we've found has leaked(æ³„æ¼) publicly.<br>

> "client-side"è¡¨ç¤ºå®¢æˆ·ç«¯ï¼ŒæœåŠ¡å™¨ç«¯ä¸º "server-side"ã€‚

è¯·ä¸è¦ä¸ä»–äººå…±äº«ä½ çš„ API å¯†é’¥ï¼Œä¹Ÿä¸è¦åœ¨æµè§ˆå™¨æˆ–å…¶ä»–å®¢æˆ·ç«¯ä»£ç ä¸­æš´éœ²å®ƒã€‚ä¸ºäº†ä¿æŠ¤ä½ è´¦æˆ·çš„å®‰å…¨ï¼Œä¸€æ—¦å‘ç°æœ‰ API å¯†é’¥è¢«å…¬å¼€æ³„éœ²ï¼ŒOpenAI å¯èƒ½ä¼šè‡ªåŠ¨ç¦ç”¨è¯¥å¯†é’¥ã€‚<br>

Enable tracking(è¿½è¸ª) to see usage per API key on the Usage page.<br>

åœ¨ **Usage** é¡µé¢ä¸Šå¯ç”¨è¿½è¸ªåŠŸèƒ½ï¼Œä»¥æŸ¥çœ‹æ¯ä¸ª API å¯†é’¥çš„ä½¿ç”¨æƒ…å†µã€‚<br>

| NAME                  | SECRET KEY | TRACKING | CREATED     | LAST USED  |
|-----------------------|------------|----------|-------------|------------|
| peilongchencc_openai  | sk-...eZeu | Enable   | 2023å¹´6æœˆ8æ—¥ | 2023å¹´6æœˆ8æ—¥ |

> â€œTRACKINGâ€åˆ—ä¸­çš„â€œEnableâ€æ„æ€æ˜¯â€œå¯ç”¨â€ã€‚åœ¨è¿™é‡Œï¼Œå®ƒè¡¨ç¤ºå¯¹è¯¥ API å¯†é’¥çš„ä½¿ç”¨æƒ…å†µè¿›è¡Œè¿½è¸ªåŠŸèƒ½æ˜¯å¼€å¯çš„ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨â€œä½¿ç”¨æƒ…å†µâ€é¡µé¢æŸ¥çœ‹åˆ°è¿™ä¸ªç‰¹å®š API å¯†é’¥çš„ä½¿ç”¨è¯¦æƒ…ï¼Œæ¯”å¦‚è°ƒç”¨æ¬¡æ•°ã€ä½¿ç”¨é¢‘ç‡ç­‰ä¿¡æ¯ã€‚è¿™ä¸ªåŠŸèƒ½å¯¹äºç›‘æ§å’Œåˆ†æ API å¯†é’¥çš„ä½¿ç”¨æƒ…å†µéå¸¸æœ‰ç”¨ï¼Œç‰¹åˆ«æ˜¯å½“ä½ æƒ³ç¡®ä¿å¯†é’¥æ²¡æœ‰è¢«æ»¥ç”¨æ—¶ã€‚

Default organization(é»˜è®¤ç»„ç»‡)<br>

If you belong to multiple(å¤šä¸ª) organizations, this setting controls which organization is used by default when making requests with the API keys above.<br>

å¦‚æœä½ å±äºå¤šä¸ªç»„ç»‡ï¼Œæ­¤è®¾ç½®å°†æ§åˆ¶åœ¨ä½¿ç”¨ä¸Šè¿° API å¯†é’¥è¿›è¡Œè¯·æ±‚æ—¶é»˜è®¤ä½¿ç”¨å“ªä¸ªç»„ç»‡ã€‚<br>

Note: You can also specify which organization to use for each API request. See Authentication to learn more.<br>

å¤‡æ³¨ï¼šä½ ä¹Ÿå¯ä»¥ä¸ºæ¯ä¸ª API è¯·æ±‚æŒ‡å®šä½¿ç”¨å“ªä¸ªç»„ç»‡ã€‚è¯·å‚é˜…â€œèº«ä»½éªŒè¯â€äº†è§£æ›´å¤šä¿¡æ¯ã€‚<br>

### Quickstart language selection(å¿«é€Ÿå¼€å§‹è¯­è¨€é€‰æ‹©):

Select the tool or language(è¿™é‡Œç”±ä¸‹æ–‡å¯çŸ¥æŒ‡çš„æ˜¯ç¼–ç¨‹è¯­è¨€) you want to get started using the OpenAI API with.<br>

è¯·é€‰æ‹©ä½ å¸Œæœ›ä½¿ç”¨ OpenAI API å¼€å§‹ä½¿ç”¨çš„å·¥å…·æˆ–ç¼–ç¨‹è¯­è¨€ã€‚<br>

Python is a popular programming language that is commonly(é€šå¸¸åœ°) used for data applications, web development(ç½‘é¡µå¼€å‘), and many other programming tasks due to its ease of use. OpenAI provides a custom(å®šåˆ¶çš„) Python library which makes working with the OpenAI API in Python simple and efficient.<br>

> "custom" è¡¨ç¤º "å®šåˆ¶çš„"ï¼Œ"è‡ªå®šä¹‰çš„"åœ¨è‹±è¯­ä¸­é€šå¸¸å¯ä»¥è¡¨è¾¾ä¸º "customized" æˆ– "personalized"ã€‚ä¾‹å¦‚ï¼Œ"customized computer" æˆ– "personalized plan" åˆ†åˆ«è¡¨ç¤ºâ€œå®šåˆ¶çš„ç”µè„‘â€å’Œâ€œä¸ªæ€§åŒ–çš„è®¡åˆ’â€ã€‚

Python æ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå› å…¶æ˜“ç”¨æ€§ï¼Œå¸¸ç”¨äºæ•°æ®åº”ç”¨ã€ç½‘é¡µå¼€å‘å’Œè®¸å¤šå…¶ä»–ç¼–ç¨‹ä»»åŠ¡ã€‚OpenAI æä¾›äº†ä¸€ä¸ªå®šåˆ¶çš„ Python åº“ï¼Œä½¿å¾—åœ¨ Python ä¸­ä½¿ç”¨ OpenAI API å˜å¾—ç®€å•é«˜æ•ˆã€‚<br>

### Set your API key(è®¾ç½®ä½ çš„ API å¯†é’¥):

#### Seetup your API key for all projects(recommended)(ä¸ºæ‰€æœ‰é¡¹ç›®è®¾ç½®ä½ çš„ API å¯†é’¥)(æ¨è):

The main advantage to making your API key accessible for all projects is that the Python library will automatically detect(æ£€æµ‹) it and use it without having to write any code.<br>

å°†ä½ çš„ API å¯†é’¥è®¾ç½®ä¸ºå¯¹æ‰€æœ‰é¡¹ç›®å¯è®¿é—®çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºï¼ŒPython åº“å°†è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨å®ƒï¼Œæ— éœ€ç¼–å†™ä»»ä½•ä»£ç ã€‚<br>

> ç”±äºç¬”è€…ä½¿ç”¨çš„æ˜¯macï¼Œæƒ¯ç”¨çš„ä¹Ÿæ˜¯Linuxç³»ç»Ÿï¼Œæ‰€ä»¥è¿™é‡Œåªä»‹ç»macçš„æ“ä½œæ–¹å¼ï¼Œwindowsä¸åšä»‹ç»ã€‚

Open Terminal: You can find it in the Applications folder or search for it using Spotlight (Command + Space).<br>

æ‰“å¼€ç»ˆç«¯ï¼šä½ å¯ä»¥åœ¨â€œåº”ç”¨ç¨‹åºâ€æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°å®ƒï¼Œæˆ–ä½¿ç”¨ Spotlightï¼ˆCommand + Spaceï¼‰è¿›è¡Œæœç´¢ã€‚<br>

Edit Bash Profile: Use the command `nano ~/.bash_profile` or `nano ~/.zshrc` (for newer MacOS versions) to open the profile file in a text editor.<br>

ç¼–è¾‘ Bash é…ç½®æ–‡ä»¶ï¼šä½¿ç”¨å‘½ä»¤ `nano ~/.bash_profile` æˆ– `nano ~/.zshrc`ï¼ˆé€‚ç”¨äºè¾ƒæ–°çš„ MacOS ç‰ˆæœ¬ï¼‰åœ¨æ–‡æœ¬ç¼–è¾‘å™¨ä¸­æ‰“å¼€é…ç½®æ–‡ä»¶ã€‚<br>

> å¦‚æœä½ ä¹ æƒ¯vimæŒ‡ä»¤ï¼ŒvimæŒ‡ä»¤æ›´æ–¹ä¾¿ã€‚
> å¦‚æœä½ ä¸æ¸…æ¥šä½ ç”¨çš„å“ªç§shellç¯å¢ƒï¼Œå¯ç»ˆç«¯è¿è¡Œ `echo $SHELL` æŒ‡ä»¤è¿›è¡ŒæŸ¥çœ‹ï¼Œç»ˆç«¯åº”è¯¥ä¼šè¾“å‡ºç±»ä¼¼ `/bin/zsh` çš„ç»“æœã€‚

Add Environment Variable: In the editor, add the line below, replacing `your-api-key-here` with your actual(å®é™…çš„) API key:<br>

æ·»åŠ ç¯å¢ƒå˜é‡ï¼šåœ¨ç¼–è¾‘å™¨ä¸­ï¼Œæ·»åŠ ä¸‹é¢çš„è¿™è¡Œä»£ç ï¼Œå°† `your-api-key-here` æ›¿æ¢ä¸ºä½ å®é™…çš„ API å¯†é’¥ï¼š<br>

```bash
export OPENAI_API_KEY='your-api-key-here'
```

Save and Exit: Press **Ctrl+O** to write the changes, followed by **Ctrl+X** to close the editor.<br>

ä¿å­˜å¹¶é€€å‡ºï¼šæŒ‰ **Ctrl+O** ä¿å­˜æ›´æ”¹ï¼Œç„¶åæŒ‰ **Ctrl+X** å…³é—­ç¼–è¾‘å™¨ã€‚<br>

Load Your Profile: Use the command `source ~/.bash_profile` or `source ~/.zshrc` to load the updated profile.<br>

åŠ è½½ä½ çš„é…ç½®æ–‡ä»¶ï¼šä½¿ç”¨å‘½ä»¤ `source ~/.bash_profile` æˆ– `source ~/.zshrc` åŠ è½½æ›´æ–°åçš„é…ç½®æ–‡ä»¶ã€‚<br>

Verification: Verify the setup by typing `echo $OPENAI_API_KEY` in the terminal. It should display your API key.<br>

éªŒè¯ï¼šåœ¨ç»ˆç«¯ä¸­è¾“å…¥ `echo $OPENAI_API_KEY` è¿›è¡ŒéªŒè¯ã€‚å®ƒåº”è¯¥ä¼šæ˜¾ç¤ºä½ çš„ API å¯†é’¥ã€‚<br>

#### Setup your API key for a single project(ä¸ºå•ä¸ªé¡¹ç›®è®¾ç½®ä½ çš„ API å¯†é’¥):

If you only want your API key to be accessible(æ˜“äºè·å¾—æˆ–ä½¿ç”¨çš„) to a single project, you can create a local `.env` file which contains the API key and then explicitly(æ˜ç¡®åœ°;ç›´æ¥åœ°) use that API key with the Python code shown in the steps to come.<br>

å¦‚æœä½ åªå¸Œæœ›ä½ çš„APIå¯†é’¥è¢«å•ä¸ªé¡¹ç›®è®¿é—®ï¼Œä½ å¯ä»¥åœ¨é¡¹ç›®æ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ªæœ¬åœ°çš„ `.env` æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«APIå¯†é’¥ï¼Œç„¶ååœ¨æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­æ˜ç¡®åœ°åœ¨Pythonä»£ç ä¸­ä½¿ç”¨è¿™ä¸ªAPIå¯†é’¥ã€‚<br>

Start by going to the project folder you want to create the `.env` file in.<br>

é¦–å…ˆï¼Œå‰å¾€ä½ æƒ³è¦åˆ›å»º `.env` æ–‡ä»¶çš„é¡¹ç›®æ–‡ä»¶å¤¹ã€‚<br>

Note: In order for your `.env` file to be ignored by version control, create a `.gitignore` file in the root of your project directory. Add a line with `.env` on it which will make sure your API key or other secrets are not accidentally(æ„å¤–åœ°) shared via version control.<br>

æ³¨æ„ï¼šä¸ºäº†è®©ä½ çš„ `.env` æ–‡ä»¶è¢«ç‰ˆæœ¬æ§åˆ¶å¿½ç•¥ï¼Œè¯·åœ¨ä½ çš„é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª `.gitignore` æ–‡ä»¶ã€‚åœ¨æ–‡ä»¶ä¸­åŠ å…¥ä¸€è¡ŒåŒ…å«.envçš„å†…å®¹ï¼Œè¿™å°†ç¡®ä¿ä½ çš„APIå¯†é’¥æˆ–å…¶ä»–æ•æ„Ÿä¿¡æ¯ä¸ä¼šé€šè¿‡ç‰ˆæœ¬æ§åˆ¶æ„å¤–åˆ†äº«ã€‚<br>

Once you create the `.gitignore` and `.env` files using the terminal or an integrated(é›†æˆçš„) development environment (IDE), copy your secret API key and set it as the `OPENAI_API_KEY` in your `.env` file. If you haven't created a secret key yet, you can do so on the API key page.<br>

ä¸€æ—¦ä½ ä½¿ç”¨ç»ˆç«¯æˆ–é›†æˆå¼€å‘ç¯å¢ƒï¼ˆIDEï¼‰åˆ›å»ºäº† `.gitignore` å’Œ `.env` æ–‡ä»¶ï¼Œå¤åˆ¶ä½ çš„ç§˜å¯†APIå¯†é’¥ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸º.envæ–‡ä»¶ä¸­çš„OPENAI_API_KEYã€‚å¦‚æœä½ è¿˜æ²¡æœ‰åˆ›å»ºç§˜å¯†å¯†é’¥ï¼Œä½ å¯ä»¥åœ¨APIå¯†é’¥é¡µé¢ä¸Šè¿›è¡Œåˆ›å»ºã€‚<br>

The `.env` file should look like the following:<br>

`.env` æ–‡ä»¶åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š<br>

```txt
# Once you add your API key below, make sure to not share it with anyone! The API key should remain private.
# æ·»åŠ ä½ çš„APIå¯†é’¥åï¼Œè¯·ç¡®ä¿ä¸ä¸ä»»ä½•äººåˆ†äº«ï¼APIå¯†é’¥åº”ä¿æŒç§å¯†ã€‚
OPENAI_API_KEY=abc123
```

The API key can be imported by running the code below:<br>

å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹ä»£ç å¯¼å…¥APIå¯†é’¥ï¼š

```python
from openai import OpenAI

client = OpenAI()

# defaults to getting the key using os.environ.get("OPENAI_API_KEY")
# if you saved the key under a different environment variable name, you can do something like:

# é»˜è®¤é€šè¿‡os.environ.get("OPENAI_API_KEY")è·å–å¯†é’¥
# å¦‚æœä½ åœ¨ä¸åŒçš„ç¯å¢ƒå˜é‡åä¸‹ä¿å­˜äº†å¯†é’¥ï¼Œä½ å¯ä»¥è¿™æ ·åšï¼š

# client = OpenAI(
#   api_key=os.environ.get("CUSTOM_ENV_NAME"),
# )
```

### Sending your first API request(å‘é€ä½ çš„ç¬¬ä¸€ä¸ªAPIè¯·æ±‚):

After you have Python configured and an API key setup, the final step is to send a request to the OpenAI API using the Python library. To do this, create a file named `openai-test.py` using th terminal or an IDE.<br>

åœ¨é…ç½®å¥½ Python å¹¶è®¾ç½®å¥½ API å¯†é’¥ä¹‹åï¼Œæœ€åä¸€æ­¥æ˜¯ä½¿ç”¨ Python åº“å‘ OpenAI API å‘é€è¯·æ±‚ã€‚ä¸ºæ­¤ï¼Œè¯·ä½¿ç”¨ç»ˆç«¯æˆ–é›†æˆå¼€å‘ç¯å¢ƒåˆ›å»ºä¸€ä¸ªåä¸º `openai-test.py` çš„æ–‡ä»¶ã€‚<br>

Inside the file, copy and paste one of the examples below:<br>

åœ¨æ–‡ä»¶ä¸­ï¼Œå¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ç¤ºä¾‹ä¹‹ä¸€ï¼š<br>

#### chatcompletions(èŠå¤©è¡¥å…¨):

```python
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a poetic assistant, skilled in explaining complex programming concepts with creative flair."},
    {"role": "user", "content": "Compose a poem that explains the concept of recursion in programming."}
  ]
)

print(completion.choices[0].message)
```

To run the code, enter `python openai-test.py` into the terminal / command line.<br>

è¦è¿è¡Œä»£ç ï¼Œè¯·åœ¨ç»ˆç«¯/å‘½ä»¤è¡Œä¸­è¾“å…¥ `python openai-test.py`ã€‚<br>

The Chat Completions example highlights(çªå‡ºæ˜¾ç¤ºï¼›å¼ºè°ƒ) just one area of strength(åŠ›é‡ï¼›å¼ºé¡¹) for our models: creative ability. Explaining recursion(é€’å½’) (the programming topic) in a well formatted poem is something both the best developers and best poets(è¯—äºº) would struggle with. In this case, gpt-3.5-turbo does it effortlessly.<br>

â€œèŠå¤©è¡¥å…¨â€ç¤ºä¾‹åªå±•ç¤ºäº†æˆ‘ä»¬æ¨¡å‹çš„ä¸€ä¸ªå¼ºé¡¹ï¼šåˆ›é€ åŠ›ã€‚ç”¨æ ¼å¼è‰¯å¥½çš„è¯—æ­Œè§£é‡Šé€’å½’ï¼ˆç¼–ç¨‹è¯é¢˜ï¼‰æ˜¯å³ä½¿æœ€ä¼˜ç§€çš„å¼€å‘è€…å’Œè¯—äººä¹Ÿä¼šæ„Ÿåˆ°å›°éš¾çš„äº‹æƒ…ã€‚è€Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ**gpt-3.5-turbo** è½»æ¾åœ°åšåˆ°äº†ã€‚<br>

> "Turbo" è¿™ä¸ªè¯æœ€åˆæ¥æºäºâ€œæ¶¡è½®å¢å‹å™¨ï¼ˆturbochargerï¼‰â€ï¼Œæ˜¯ä¸€ç§ç”¨äºæå‡å‘åŠ¨æœºæ•ˆèƒ½çš„è£…ç½®ã€‚åœ¨æ›´å¹¿æ³›çš„ç”¨æ³•ä¸­ï¼Œâ€œturboâ€é€šå¸¸ç”¨æ¥å½¢å®¹æŸäº‹ç‰©å…·æœ‰å¿«é€Ÿã€é«˜æ•ˆæˆ–å¼ºå¤§çš„æ€§è´¨ã€‚ä¾‹å¦‚ï¼Œåœ¨ç§‘æŠ€å’Œè½¯ä»¶é¢†åŸŸï¼Œ"turbo" é€šå¸¸ç”¨æ¥è¡¨ç¤ºæŸä¸ªç‰ˆæœ¬æˆ–å‹å·å…·æœ‰æ›´å¿«çš„å¤„ç†é€Ÿåº¦ã€æ›´é«˜çš„æ€§èƒ½æˆ–æ›´å…ˆè¿›çš„åŠŸèƒ½ã€‚åœ¨ä¸Šæ–‡æåˆ°çš„ "gpt-3.5-turbo" ä¸­ï¼Œ"turbo" ç”¨æ¥æŒ‡ä»£è¯¥æ¨¡å‹çš„é«˜æ•ˆç‡æˆ–é«˜æ€§èƒ½ç‰¹ç‚¹ã€‚

#### Embedding:

```python
from openai import OpenAI
client = OpenAI()

response = client.embeddings.create(
  model="text-embedding-ada-002",
  input="The food was delicious and the waiter..."
)

print(response)
```

#### images:

```python
from openai import OpenAI
client = OpenAI()

response = client.images.generate(
  prompt="A cute baby sea otter",
  n=2,
  size="1024x1024"
)

print(response)
```

### chatcompletions with dotenv:

In this section, we will introduce how to use `dotenv` to load the `OPENAI_API_KEY`, and demonstrate(å±•ç¤ºï¼›è¯æ˜) how to test it using chat completions.<br>

åœ¨è¿™é‡Œä»‹ç»ä½¿ç”¨`dotenv`åŠ è½½`OPENAI_API_KEY`ï¼Œå¹¶è°ƒç”¨chatcompletionsè¿›è¡Œæµ‹è¯•ã€‚<br>

#### unstreaming(éæµå¼è¾“å‡º):

```python
"""
@author:ChenPeilong(peilongchencc@163.com)
@description:OpenAI unstreaming output example code.
"""
import os
from loguru import logger
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®æ—¥å¿—
logger.remove()
logger.add("openai_stream.log", rotation="1 GB", backtrace=True, diagnose=True, format="{time} {level} {message}")

client = OpenAI(
  api_key=os.getenv("OPENAI_API_KEY"),
)

completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a poetic assistant, skilled in explaining complex programming concepts with creative flair."},
    {"role": "user", "content": "Compose a poem that explains the concept of recursion in programming."}
  ]
)

print(completion.choices[0].message)
print(type(completion.choices[0].message))  # <class 'openai.types.chat.chat_completion_message.ChatCompletionMessage'>
# only content
print(completion.choices[0].message.content)
```

#### streaming(æµå¼è¾“å‡º):

```python
"""
@author:ChenPeilong(peilongchencc@163.com)
@description:OpenAI streaming output example code.
"""
import os
from loguru import logger
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®æ—¥å¿—
logger.remove()
logger.add("openai_stream.log", rotation="1 GB", backtrace=True, diagnose=True, format="{time} {level} {message}")

client = OpenAI(
  api_key=os.getenv("OPENAI_API_KEY"),
)

completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a poetic assistant, skilled in explaining complex programming concepts with creative flair."},
    {"role": "user", "content": "Compose a poem that explains the concept of recursion in programming."}
  ],
  stream=True
)

for chunk in completion:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

#### multi_turn_dialogue(å¤šè½®å¯¹è¯):

```python
"""
@author:ChenPeilong(peilongchencc@163.com)
@description:OpenAI streaming output example code.
"""
import os
from loguru import logger
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®æ—¥å¿—
logger.remove()
logger.add("openai_stream.log", rotation="1 GB", backtrace=True, diagnose=True, format="{time} {level} {message}")


def get_openai_response(chat_history):
    # create openAI client
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    # connect openai API server and fetch the response of chat_history with streaming.
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=chat_history,
        stream=True
    )
    # combine the results of streaming output.
    response_content = ""
    for chunk in completion:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
            response_content += chunk.choices[0].delta.content
    print() # For Line Breaks, Optimizing Terminal Display.
    chat_history.append({"role": "assistant", "content": response_content})
    return chat_history

if __name__ == '__main__':
    # chath_istory can be [], without providing a semantic context(è¯­ä¹‰ç¯å¢ƒ).
    # chat_history = [{"role": "system", "content": "ä½ æ˜¯ä¸€åNLPç®—æ³•å·¥ç¨‹å¸ˆ"}]
    chat_history = []
    while True:
        user_input = input("\nPlease enter your question (type 'exit' to end the program):")
        print() # For Line Breaks, Optimizing Terminal Display.
        # If the user enters 'exit', then terminate the loop.
        if user_input == 'exit':
            break
        
        chat_history.append({"role": "user", "content": user_input})
        # fetch the results of the API response and display them in a streaming manner on the terminal, 
        # while simultaneously(åŒæ—¶) updating chat_history.
        chat_history = get_openai_response(chat_history)
```

#### å¼‚æ­¥æ–¹å¼è°ƒç”¨--å®˜æ–¹ç¤ºä¾‹:

OpenAIå®˜æ–¹æä¾›äº†å¼‚æ­¥æ–¹å¼è°ƒç”¨çš„ç¤ºä¾‹ä»£ç ï¼Œå‚è€ƒç½‘å€å¦‚ä¸‹:<br>

```txt
https://github.com/openai/openai-python/blob/9e6e1a284eeb2c20c05a03831e5566a4e9eaba50/README.md
```

å…·ä½“ä»£ç å¦‚ä¸‹:<br>

```python
import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test",
            }
        ],
        model="gpt-3.5-turbo",
    )


asyncio.run(main())
```

#### å¼‚æ­¥æ–¹å¼è°ƒç”¨--ä½¿ç”¨dotenvçš„ç®€å•ç¤ºä¾‹:

```python
import os
import asyncio
from loguru import logger
from dotenv import load_dotenv
import openai

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®æ—¥å¿—
logger.remove()
logger.add("openai_stream.log", rotation="1 GB", backtrace=True, diagnose=True, format="{time} {level} {message}")

async def main():
    client = openai.AsyncOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
    )

    completion = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a poetic assistant, skilled in explaining complex programming concepts with creative flair."},
            {"role": "user", "content": "Compose a poem that explains the concept of recursion in programming."}
        ],
        stream=True
    )

    async for chunk in completion:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")

# è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
if __name__ == "__main__":
    asyncio.run(main())
```

#### APIä¸­çš„systemã€userã€assistantä½œç”¨è§£æ:

- system: å®ƒè®¾å®šäº† AI çš„è¡Œä¸ºã€è§’è‰²ã€å’ŒèƒŒæ™¯ï¼Œæˆ–è€…ä½ å¯ä»¥ç†è§£ä¸ºè¯­å¢ƒã€‚å¸¸å¸¸ç”¨äºå¼€å§‹å¯¹è¯ï¼Œç»™å‡ºä¸€ä¸ªå¯¹è¯çš„å¤§è‡´æ–¹å‘ï¼Œæˆ–è€…è®¾ç½®å¯¹è¯çš„è¯­æ°”å’Œé£æ ¼ã€‚

ChatGPTç½‘é¡µç«¯ä¸æ˜¾ç¤ºsystemé€‰é¡¹ï¼Œä½ éœ€è¦ç”¨`user`è§’è‰²ç»™äºˆå®ƒå®šä¹‰ã€‚ä¾‹å¦‚ï¼Œè¾“å…¥ï¼š"ä½ æ˜¯ä¸€ä¸ªåŠ©ç†"æˆ–"ä½ æ˜¯ä¸€åå†å²æ•™å¸ˆ"ã€‚è¿™ä¸ªæ¶ˆæ¯å¯ä»¥å¸®åŠ©è®¾å®šå¯¹è¯çš„è¯­å¢ƒï¼Œä»¥ä¾¿ AI æ›´å¥½åœ°ç†è§£å…¶åœ¨å¯¹è¯ä¸­çš„è§’è‰²ã€‚<br>

- assistant:å³ç³»ç»Ÿå›å¤çš„å†…å®¹ï¼Œåœ¨ä½¿ç”¨ API çš„è¿‡ç¨‹ä¸­ï¼Œä½ ä¸éœ€è¦ç›´æ¥ç”Ÿæˆ assistant æ¶ˆæ¯ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ç”± API æ ¹æ® system å’Œ user æ¶ˆæ¯è‡ªåŠ¨ç”Ÿæˆçš„ã€‚

- user:å°±æ˜¯æˆ‘ä»¬è¾“å…¥çš„é—®é¢˜æˆ–è¯·æ±‚ã€‚æ¯”å¦‚è¯´"è¯·ç»™æˆ‘ä¸€ä»½ä½¿ç”¨pythonè¯»å–jsonæ–‡ä»¶çš„ç¤ºä¾‹ä»£ç "

### Next steps(æ¥ä¸‹æ¥çš„æ­¥éª¤):

Now that you have made you first OpenAI API request, it is time to explore what else is possible:<br>

æ—¢ç„¶ä½ å·²ç»å®Œæˆäº†é¦–æ¬¡OpenAI APIè¯·æ±‚ï¼Œç°åœ¨æ˜¯æ—¶å€™æ¢ç´¢æ›´å¤šå¯èƒ½æ€§äº†ï¼š<br>

- For more detailed information on our models and the API, see our [GPT guide](https://platform.openai.com/docs/guides/text-generation).(æƒ³è¦äº†è§£æˆ‘ä»¬çš„æ¨¡å‹å’ŒAPIçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„GPTæŒ‡å—ã€‚)

- Visit the [OpenAI Cookbook](https://cookbook.openai.com/) for in-depth example API use-cases, as well as code snippets for common tasks.(è®¿é—®OpenAIé£Ÿè°±ï¼Œäº†è§£æ·±å…¥çš„APIä½¿ç”¨æ¡ˆä¾‹ä»¥åŠå¸¸è§ä»»åŠ¡çš„ä»£ç ç‰‡æ®µã€‚)

- Wondering what OpenAI's models are capable of? Check out our library of [example prompts](https://platform.openai.com/examples).(æƒ³çŸ¥é“OpenAIçš„æ¨¡å‹èƒ½åšä»€ä¹ˆï¼ŸæŸ¥çœ‹æˆ‘ä»¬çš„ç¤ºä¾‹æç¤ºåº“ã€‚)

- Want to try the API without writing any code? Start experimenting in the [Playground](https://platform.openai.com/playground).(æƒ³ä¸ç¼–å†™ä»»ä½•ä»£ç å°±å°è¯•APIï¼Ÿå¼€å§‹åœ¨Playgroundå®éªŒã€‚)

- Keep our [usage policies](https://openai.com/policies/usage-policies) in mind as you start building.(å¼€å§‹æ„å»ºæ—¶ï¼Œè¯·ç‰¢è®°æˆ‘ä»¬çš„ä½¿ç”¨æ”¿ç­–ã€‚)

## settings:

### Billing settings(è´¦å•è®¾ç½®):

Note: This does not reflect the status of your ChatGPT account.<br>

å¤‡æ³¨ï¼šè¿™å¹¶ä¸åæ˜ ä½ çš„ChatGPTè´¦æˆ·çš„çŠ¶æ€ã€‚<br>


## Embeddings

Learn how to turn text into numbers, unlocking use cases like search.<br>

å­¦ä¹ å¦‚ä½•å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ï¼Œè§£é”æœç´¢ç­‰ç”¨ä¾‹ã€‚<br>

```txt
New embedding models

text-embedding-3-small and text-embedding-3-large, our newest and most performant(æ€§èƒ½æœ€ä¼˜) embedding models are now available, with lower costs(æˆæœ¬), higher multilingual performance(æ›´é«˜çš„å¤šè¯­è¨€æ€§èƒ½), and new parameters to control the overall(æ€»ä½“çš„) size.(æ¨æµ‹æŒ‡çš„æ˜¯æ§åˆ¶æ¨¡å‹çš„æ•´ä½“å¤§å°)
```

### What are embeddings?

OpenAIâ€™s text embeddings measure(æµ‹é‡ï¼›è¯„ä¼°ï¼›è¡¡é‡) the relatedness(ç›¸å…³æ€§) of text strings. Embeddings are commonly used for(OpenAI çš„æ–‡æœ¬è¯å‘é‡ç”¨äºè¡¡é‡æ–‡æœ¬å­—ç¬¦ä¸²çš„ç›¸å…³æ€§ã€‚åµŒå…¥å‘é‡å¸¸ç”¨äºï¼š):<br>

- **Search** (where results are ranked by relevance to a query string)æœç´¢ï¼ˆå…¶ä¸­ç»“æœæŒ‰ä¸æŸ¥è¯¢å­—ç¬¦ä¸²çš„ç›¸å…³æ€§æ’åºï¼‰

- **Clustering** (where text strings are grouped by similarity)èšç±»ï¼ˆå…¶ä¸­æ–‡æœ¬å­—ç¬¦ä¸²æŒ‰ç›¸ä¼¼æ€§åˆ†ç»„ï¼‰

- **Recommendations** (where items with related text strings are recommended)æ¨èï¼ˆå…·æœ‰ç›¸å…³æ–‡æœ¬å­—ç¬¦ä¸²çš„é¡¹ç›®ä¼šè¢«æ¨èï¼‰

- **Anomaly(å¼‚å¸¸æˆ–åç¦»å¸¸è§„çš„äº‹ç‰©) detection** (where outliers(å¼‚å¸¸å€¼) with little relatedness are identified)å¼‚å¸¸æ£€æµ‹ï¼ˆå…¶ä¸­è¯†åˆ«å‡ºä¸ä¼—ä¸åŒçš„å¼‚å¸¸å€¼ï¼‰

- **Diversity measurement** (where similarity distributions are analyzed)å¤šæ ·æ€§æµ‹é‡ï¼ˆç›¸ä¼¼æ€§åˆ†å¸ƒä¼šè¢«åˆ†æï¼‰

- **Classification** (where text strings are classified by their most similar label)åˆ†ç±»ï¼ˆæ–‡æœ¬å­—ç¬¦ä¸²æŒ‰å…¶æœ€ç›¸ä¼¼çš„æ ‡ç­¾åˆ†ç±»ï¼‰

An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.<br>

ä¸€ä¸ªåµŒå…¥å‘é‡æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ï¼ˆå‘é‡ï¼‰ã€‚ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è·ç¦»è¡¡é‡äº†å®ƒä»¬çš„ç›¸å…³æ€§ã€‚å°è·ç¦»è¡¨æ˜é«˜åº¦ç›¸å…³ï¼Œå¤§è·ç¦»è¡¨æ˜ä½åº¦ç›¸å…³ã€‚<br>

Visit our [pricing page](https://openai.com/pricing) to learn about Embeddings pricing. Requests are billed based on the number of [tokens](https://platform.openai.com/tokenizer) in the [input](https://platform.openai.com/docs/api-reference/embeddings/create).<br>

è®¿é—®æˆ‘ä»¬çš„å®šä»·é¡µé¢äº†è§£æœ‰å…³åµŒå…¥å‘é‡å®šä»·çš„æ›´å¤šä¿¡æ¯ã€‚Requests(è¯·æ±‚)æ ¹æ®è¾“å…¥ä¸­çš„tokensæ•°é‡è¿›è¡Œè®¡è´¹ã€‚<br>

### How to get embeddings(å¦‚ä½•è·å¾—è¯å‘é‡)

To get an embedding, send your text string to [the embeddings API endpoint](https://platform.openai.com/docs/api-reference/embeddings) along with the embedding model name (e.g. `text-embedding-3-small`). The response will contain an embedding (list of floating point numbers), which you can extract, save in a vector database, and use for many different use cases:<br>

è¦è·å¾—è¯å‘é‡ï¼Œå°†ä½ çš„æ–‡æœ¬å­—ç¬¦ä¸²è¿åŒåµŒå…¥æ¨¡å‹åç§°ï¼ˆä¾‹å¦‚ï¼Œtext-embedding-3-smallï¼‰ä¸€èµ·å‘é€åˆ°è¯å‘é‡APIç«¯ç‚¹ã€‚å“åº”å°†åŒ…å«ä¸€ä¸ªåµŒå…¥ï¼ˆæµ®ç‚¹æ•°åˆ—è¡¨ï¼‰ï¼Œä½ å¯ä»¥æå–è¯¥è¯å‘é‡ï¼Œä¿å­˜åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œå¹¶ç”¨äºè®¸å¤šä¸åŒçš„ç”¨ä¾‹ï¼š<br>

```python
from openai import OpenAI
client = OpenAI()

response = client.embeddings.create(
    input="Your text string goes here",
    model="text-embedding-3-small"
)

print(response.data[0].embedding)
```

The response will contain the embedding vector along with some additional metadata.<br>

å“åº”å°†åŒ…å«è¯å‘é‡ä»¥åŠä¸€äº›é¢å¤–çš„å…ƒæ•°æ®ã€‚<br>

Embeddings è¿”å›çš„ç¤ºä¾‹å¦‚ä¸‹:<br>

> "**omitted**"çš„æ„æ€æ˜¯â€œçœç•¥çš„â€ï¼Œâ€œé—æ¼çš„â€ï¼Œæˆ–â€œåˆ å»çš„â€ã€‚"**omitted for spacing**"çš„æ„æ€æ˜¯å› ä¸ºç©ºé—´é™åˆ¶è€Œçœç•¥äº†ä¸€éƒ¨åˆ†å†…å®¹ã€‚

```json
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "index": 0,
      "embedding": [
        -0.006929283495992422,
        -0.005336422007530928,
        ... (omitted for spacing)
        -4.547132266452536e-05,
        -0.024047505110502243
      ],
    }
  ],
  "model": "text-embedding-3-small",
  "usage": {
    "prompt_tokens": 5,
    "total_tokens": 5
  }
}
```

By default, the length of the embedding vector will be 1536 for `text-embedding-3-small` or 3072 for `text-embedding-3-large`. You can reduce(å‡å°‘) the dimensions(ç»´åº¦æ•°) of the embedding by passing in the dimensions parameter without the embedding losing its concept-representing properties. We go into more detail on embedding dimensions in the embedding use case section.

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`text-embedding-3-small` çš„è¯å‘é‡é•¿åº¦å°†ä¸º1536ï¼Œè€Œ `text-embedding-3-large` çš„è¯å‘é‡é•¿åº¦å°†ä¸º3072ã€‚ä½ å¯ä»¥é€šè¿‡ä¼ å…¥ç»´åº¦å‚æ•°æ¥å‡å°‘è¯å‘é‡çš„ç»´åº¦æ•°ï¼Œè€Œä¸ä¼šä¸¢å¤±è¯å‘é‡è¡¨ç¤ºæ¦‚å¿µçš„å±æ€§ã€‚æˆ‘ä»¬åœ¨åµŒå…¥ä½¿ç”¨æ¡ˆä¾‹éƒ¨åˆ†å¯¹åµŒå…¥ç»´åº¦è¿›è¡Œäº†æ›´è¯¦ç»†çš„è®¨è®ºã€‚


```txt
dimensions integer(æ•´æ•°) Optional(å¯é€‰çš„)

The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models.

ç»“æœè¾“å‡ºè¯å‘é‡åº”å…·æœ‰çš„ç»´åº¦æ•°ã€‚ä»…åœ¨ `text-embedding-3` åŠåç»­æ¨¡å‹ä¸­æ”¯æŒã€‚ğŸš¨ğŸš¨ğŸš¨
```

ç¤ºä¾‹ä»£ç å¦‚ä¸‹:<br>

```python
import os
from loguru import logger
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®æ—¥å¿—
logger.remove()
logger.add("openai_stream.log", rotation="1 GB", backtrace=True, diagnose=True, format="{time} {level} {message}")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

response = client.embeddings.create(
    input="ã€Šè€äººä¸æµ·ã€‹è¿™ç¯‡æ–‡ç« è¢«é€‰å…¥äº†å°å­¦è¯­æ–‡è¯¾æœ¬ã€‚",
    model="text-embedding-3-small",
    dimensions=768
)

print(response.data[0].embedding)
print(len(response.data[0].embedding))
```

ç»ˆç«¯è¾“å‡º:<br>

```txt
[0.06348717212677002, -0.01719544641673565, -0.006907057948410511, 0.008044195361435413, 0.05607472360134125, -0.025943584740161896, -0.08866063505411148, 0.004545541945844889, -0.05698924511671066, -0.011876770295202732, -0.012255816720426083, -0.08413615077733994, -0.0390116423368454, 0.05424567684531212, ...]
768
```

### Embedding models(è¯å‘é‡æ¨¡å‹):

OpenAI offers two powerful third-generation embedding model (denoted(â€œè¡¨ç¤ºâ€ã€â€œæŒ‡ä»£â€æˆ–â€œæ ‡è®°â€) by `-3` in the model ID). You can read the embedding v3 [announcement blog post](https://openai.com/blog/new-embedding-models-and-api-updates) for more details.<br>

OpenAIæä¾›äº†ä¸¤æ¬¾å¼ºå¤§çš„ç¬¬ä¸‰ä»£è¯å‘é‡æ¨¡å‹ï¼ˆåœ¨æ¨¡å‹IDä¸­ä»¥-3è¡¨ç¤ºï¼‰ã€‚æ‚¨å¯ä»¥é˜…è¯»è¯å‘é‡v3å…¬å‘Šåšå®¢æ–‡ç« äº†è§£æ›´å¤šè¯¦æƒ…ã€‚<br>

Usage is priced per input token, below is an example of pricing pages of text per US dollar (assuming ~800 tokens per page):<br>

ä½¿ç”¨è´¹ç”¨æŒ‰è¾“å…¥tokenè®¡ç®—ï¼Œä»¥ä¸‹æ˜¯æŒ‰ç¾å…ƒè®¡ä»·çš„æ–‡æœ¬é¡µé¢ç¤ºä¾‹ï¼ˆå‡è®¾æ¯é¡µçº¦800ä¸ªtokenï¼‰ï¼š<br>

| MODEL                 | ~ PAGES PER DOLLAR | PERFORMANCE ON MTEB EVAL | MAX INPUT |
|-----------------------|--------------------|--------------------------|-----------|
| text-embedding-3-small | 62,500            | 62.3%                    | 8191      |
| text-embedding-3-large | 9,615             | 64.6%                    | 8191      |
| text-embedding-ada-002 | 12,500            | 61.0%                    | 8191      |

> "MTEB EVAL"æŒ‡çš„æ˜¯ä¸€ä¸ªè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æ ‡å‡†æˆ–æµ‹è¯•é›†åˆã€‚MTEB é€šå¸¸ä»£è¡¨ "Multitask Text Embedding Benchmark"ï¼Œå®ƒæ˜¯ä¸€ç³»åˆ—ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™äº›ä»»åŠ¡å¯èƒ½åŒ…æ‹¬è¯­ä¹‰ç›¸ä¼¼æ€§ã€æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æ£€ç´¢ç­‰å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚"PERFORMANCE ON MTEB EVAL" å³è¡¨ç¤ºæ¨¡å‹åœ¨è¿™ä¸ªå¤šä»»åŠ¡æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æˆ–å‡†ç¡®ç‡çš„ç™¾åˆ†æ¯”ã€‚

### Use cases(åº”ç”¨æ¡ˆä¾‹):

Here we show some representative(ä»£è¡¨æ€§çš„) use cases. We will use the **[Amazon fine-food reviews dataset](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews)** for the following examples.<br>

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å±•ç¤ºä¸€äº›å…·æœ‰ä»£è¡¨æ€§çš„åº”ç”¨æ¡ˆä¾‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ **äºšé©¬é€Šç²¾é€‰é£Ÿå“è¯„è®ºæ•°æ®é›†** ä½œä¸ºä»¥ä¸‹ç¤ºä¾‹çš„åŸºç¡€ã€‚<br>

#### Obtaining the embeddings(è·å–è¯å‘é‡):

The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. <br>

> "left up to" è¡¨ç¤º "ç›´åˆ°...ä¸ºæ­¢"ã€‚

è¯¥æ•°æ®é›†åŒ…å«äº†æˆªè‡³2012å¹´10æœˆï¼Œäºšé©¬é€Šç”¨æˆ·ç•™ä¸‹çš„å…±568,454æ¡é£Ÿå“è¯„è®ºã€‚<br>

We will use a subset of 1,000 most recent reviews for illustration(å±•ç¤º) purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:<br>

> "subset"è¿™ä¸ªè¯æŒ‡çš„æ˜¯ä»ä¸€ä¸ªè¾ƒå¤§é›†åˆä¸­é€‰å‡ºçš„ä¸€éƒ¨åˆ†å…ƒç´ ç»„æˆçš„è¾ƒå°é›†åˆ

æˆ‘ä»¬å°†ä½¿ç”¨æœ€æ–°çš„1,000æ¡è¯„è®ºä½œä¸ºç¤ºä¾‹ã€‚è¿™äº›è¯„è®ºéƒ½æ˜¯ç”¨è‹±æ–‡å†™æˆçš„ï¼Œå€¾å‘äºè¡¨è¾¾ç§¯ææˆ–æ¶ˆæçš„è§‚ç‚¹ã€‚æ¯æ¡è¯„è®ºéƒ½æœ‰ä¸€ä¸ªäº§å“IDï¼ˆProductIdï¼‰ã€ç”¨æˆ·IDï¼ˆUserIdï¼‰ã€è¯„åˆ†ï¼ˆScoreï¼‰ã€è¯„è®ºæ ‡é¢˜ï¼ˆæ‘˜è¦Summaryï¼‰å’Œè¯„è®ºæ­£æ–‡ï¼ˆæ–‡æœ¬Textï¼‰ã€‚ä¾‹å¦‚ï¼š<br>

| PRODUCT ID | USER ID       | SCORE | SUMMARY               | TEXT                                              |
|------------|---------------|-------|-----------------------|---------------------------------------------------|
| B001E4KFG0 | A3SGXH7AUH8GW | 5     | Good Quality Dog Food | I have bought several of the Vitality canned...   |
| B00813GRG4 | A1D87F6ZCVE5NK| 1     | Not as Advertised     | Product arrived labeled as Jumbo Salted Peanut... |

"Good Quality Dog Food": è´¨é‡å¥½çš„ç‹—ç²®ã€‚<br>

"I have bought several of the Vitality canned...": è¯´è¯è€…å·²ç»è´­ä¹°äº†å¥½å‡ ä¸ªâ€œVitalityâ€å“ç‰Œçš„ç½è£…äº§å“ã€‚"canned"ä¸€è¯é€šå¸¸æŒ‡çš„æ˜¯ç½è£…äº§å“ã€‚<br>

"Not as Advertised": æ„æ€æ˜¯äº§å“å®é™…ä¸Šå¹¶ä¸ç¬¦åˆå¹¿å‘Šæˆ–å®£ä¼ ä¸­çš„æè¿°ã€‚é€šå¸¸è¿™è¡¨ç¤ºæ¶ˆè´¹è€…å¯¹äºè´­ä¹°çš„äº§å“æ„Ÿåˆ°å¤±æœ›ï¼Œå› ä¸ºå®ƒæ²¡æœ‰è¾¾åˆ°ä»–ä»¬æ ¹æ®å¹¿å‘Šæ‰€æœŸå¾…çš„æ ‡å‡†æˆ–å“è´¨ã€‚<br>

"Product arrived labeled as Jumbo Salted Peanut...": æ„æ€æ˜¯æ”¶åˆ°çš„äº§å“ä¸Šçš„æ ‡ç­¾æ˜¾ç¤ºå®ƒæ˜¯â€œJumbo Salted Peanutâ€ï¼Œå³å·¨å‹ç›å‘³èŠ±ç”Ÿã€‚<br>

We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding.<br>

æˆ‘ä»¬å°†æŠŠè¯„å®¡æ‘˜è¦å’Œè¯„å®¡æ–‡æœ¬åˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ–‡æœ¬ã€‚æ¨¡å‹å°†å¯¹è¿™ä¸ªåˆå¹¶åçš„æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªå•ä¸€çš„è¯å‘é‡ã€‚<br>

âš ï¸æ³¨æ„:è¿™é‡Œä¸å±•ç¤ºæ•°æ®é›†çš„åŠ è½½ï¼Œåªå±•ç¤ºå…³é”®ä»£ç çš„ä½¿ç”¨:<br>

```python
from openai import OpenAI
client = OpenAI()

def get_embedding(text, model="text-embedding-3-small"):
   text = text.replace("\n", " ")
   return client.embeddings.create(input = [text], model=model).data[0].embedding

df['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))
df.to_csv('output/embedded_1k_reviews.csv', index=False)
```

To load the data from a saved file, you can run the following:<br>

è¦ä»ä¿å­˜çš„æ–‡ä»¶ä¸­åŠ è½½æ•°æ®ï¼Œä½ å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼š<br>

```python
import pandas as pd

df = pd.read_csv('output/embedded_1k_reviews.csv')
df['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)
```

#### Reducing embedding dimensions(é™ä½è¯å‘é‡ç»´åº¦):

Using larger embeddings, for example storing them in a vector store for retrieval(æ£€ç´¢), generally(é€šå¸¸) costs more and consumes(æ¶ˆè€—) more compute(è®¡ç®—ï¼›è®¡ç®—èµ„æº), memory and storage(å­˜å‚¨ç©ºé—´) than using smaller embeddings.<br>

ä½¿ç”¨è¾ƒå¤§çš„åµŒå…¥å‘é‡ï¼Œä¾‹å¦‚å°†å®ƒä»¬å­˜å‚¨åœ¨å‘é‡å­˜å‚¨åº“ä¸­ä»¥ä¾¿æ£€ç´¢ï¼Œé€šå¸¸æ¯”ä½¿ç”¨è¾ƒå°çš„åµŒå…¥å‘é‡æˆæœ¬æ›´é«˜ï¼Œä¸”æ¶ˆè€—æ›´å¤šçš„è®¡ç®—èµ„æºã€å†…å­˜å’Œå­˜å‚¨ç©ºé—´ã€‚<br>

Both of our new embedding models were trained [with a technique](https://arxiv.org/abs/2205.13147) that allows developers to trade-off(æƒè¡¡) performance and cost of using embeddings. <br>

> è·³è½¬é“¾æ¥ä¸­æ˜¯ä¸€ç¯‡è®ºæ–‡ã€‚
> "trade-off"æ„å‘³ç€åœ¨ä¸¤ä¸ªæˆ–å¤šä¸ªç›¸äº’å…³è”çš„å› ç´ ã€å±æ€§æˆ–è¡ŒåŠ¨ä¹‹é—´è¿›è¡Œæƒè¡¡æˆ–å¦¥åã€‚é€šå¸¸ï¼Œè¿™æ¶‰åŠåˆ°åœ¨ä¸€ä¸ªæ–¹é¢è·å¾—åˆ©ç›Šçš„åŒæ—¶ï¼Œåœ¨å¦ä¸€ä¸ªæ–¹é¢æ‰¿æ‹…æŸå¤±æˆ–å‰Šå‡æŸäº›ä¼˜åŠ¿ã€‚

æˆ‘ä»¬çš„ä¸¤ä¸ªæ–°åµŒå…¥æ¨¡å‹éƒ½é‡‡ç”¨äº†ä¸€ç§æŠ€æœ¯è¿›è¡Œè®­ç»ƒï¼Œè¯¥æŠ€æœ¯å…è®¸å¼€å‘è€…åœ¨ä½¿ç”¨åµŒå…¥å‘é‡æ—¶**æƒè¡¡**æ€§èƒ½å’Œæˆæœ¬ã€‚<br>

Specifically, developers can shorten embeddings (i.e. remove some numbers from the end of the sequence) without the embedding losing its concept-representing properties by passing in the dimensions API parameter. <br>

å…·ä½“æ¥è¯´ï¼Œå¼€å‘è€…å¯ä»¥é€šè¿‡ä¼ é€’ç»´åº¦APIå‚æ•°æ¥ç¼©çŸ­åµŒå…¥å‘é‡çš„é•¿åº¦ï¼ˆå³ä»åºåˆ—æœ«å°¾åˆ é™¤ä¸€äº›æ•°å­—ï¼‰ï¼Œè€Œä¸ä¼šä¸¢å¤±åµŒå…¥å‘é‡è¡¨ç¤ºæ¦‚å¿µçš„å±æ€§ã€‚<br>

For example, on the MTEB benchmark, a `text-embedding-3-large` embedding can be shortened to a size of 256 while still outperforming(è¡¨ç°çš„æ›´å¥½) an unshortened `text-embedding-ada-002` embedding with a size of 1536. <br>

> è¯´æ˜ `text-embedding-3-large` æ¯” `text-embedding-ada-002` å¼ºå¤§çš„å¤šçš„å¤šçš„å¤šã€‚ç±»ä¼¼äº Bert åŒºåˆ«äº LSTMã€‚

ä¾‹å¦‚ï¼Œåœ¨MTEBåŸºå‡†æµ‹è¯•ä¸Šï¼Œä¸€ä¸ª `text-embedding-3-large` è¯å‘é‡ç¼©çŸ­åˆ°256çš„å¤§å°ï¼Œè€Œä»ç„¶æ¯”æœªç¼©çŸ­çš„text-embedding-ada-002åµŒå…¥å‘é‡ï¼ˆå¤§å°ä¸º1536ï¼‰è¡¨ç°å¾—æ›´å¥½ã€‚<br>

You can read more about how changing the dimensions impacts performance in our [embeddings v3 launch blog post](https://openai.com/blog/new-embedding-models-and-api-updates).<br>

ä½ å¯ä»¥åœ¨æˆ‘ä»¬çš„åµŒå…¥å‘é‡v3å‘å¸ƒåšå®¢æ–‡ç« ä¸­é˜…è¯»æ›´å¤šå…³äºæ”¹å˜ç»´åº¦å¦‚ä½•å½±å“æ€§èƒ½çš„ä¿¡æ¯ã€‚<br>

In general, using the dimensions parameter when creating the embedding is the suggested approach. <br>

ä¸€èˆ¬æ¥è¯´ï¼Œç”Ÿæˆè¯å‘é‡æ—¶ä½¿ç”¨ç»´åº¦å‚æ•°æ˜¯æ¨èçš„æ–¹æ³•ã€‚<br>

In certain cases, you may need to change the embedding dimension after you generate it. When you change the dimension manually, you need to be sure to normalize(æ ‡å‡†åŒ–) the dimensions of the embedding as is shown below.<br>

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½éœ€è¦åœ¨ç”Ÿæˆè¯å‘é‡åæ”¹å˜å…¶ç»´åº¦ã€‚å½“ä½ æ‰‹åŠ¨æ”¹å˜ç»´åº¦æ—¶ï¼Œéœ€è¦ç¡®ä¿æŒ‰ç…§ä¸‹é¢æ‰€ç¤ºæ ‡å‡†åŒ–è¯å‘é‡çš„ç»´åº¦ã€‚<br>

```python
from openai import OpenAI
import numpy as np

client = OpenAI()

def normalize_l2(x):
    x = np.array(x)
    """
    å¦‚æœè¾“å…¥xæ˜¯ä¸€ç»´æ•°ç»„ï¼ˆå•ä¸ªå‘é‡ï¼‰ï¼Œå‡½æ•°è®¡ç®—å…¶L2èŒƒæ•°ã€‚å¦‚æœèŒƒæ•°ä¸ä¸ºé›¶ï¼Œxé€šè¿‡é™¤ä»¥å…¶èŒƒæ•°è¿›è¡Œå½’ä¸€åŒ–ã€‚å¦‚æœèŒƒæ•°ä¸ºé›¶ï¼Œxä¸å˜è¿”å›ï¼Œå› ä¸ºé›¶å‘é‡æ— æ³•å½’ä¸€åŒ–ã€‚
    """
    if x.ndim == 1:
        norm = np.linalg.norm(x)
        if norm == 0:
            return x
        return x / norm
    """
    å¦‚æœxæ˜¯å¤šç»´æ•°ç»„ï¼ˆå‘é‡çŸ©é˜µï¼‰ï¼Œå‡½æ•°æ²¿æŒ‡å®šè½´ï¼ˆaxis=1å¯¹åº”è¡Œï¼‰ä¸ºæ¯ä¸ªå‘é‡è®¡ç®—L2èŒƒæ•°ã€‚ç„¶åï¼ŒçŸ©é˜µä¸­çš„æ¯ä¸ªå‘é‡éƒ½é€šè¿‡å…¶èŒƒæ•°è¿›è¡Œå½’ä¸€åŒ–ã€‚èŒƒæ•°ä¸ºé›¶çš„å‘é‡ä¸å˜è¿”å›ã€‚keepdims=Trueå‚æ•°ç¡®ä¿è¾“å‡ºæ•°ç»„ä¸è¾“å…¥æ•°ç»„å…·æœ‰ç›¸åŒçš„ç»´åº¦ï¼Œä»¥ä¾¿è¿›è¡Œé™¤æ³•æ“ä½œçš„å¹¿æ’­ã€‚
    """
    else:
        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)
        return np.where(norm == 0, x, x / norm)


response = client.embeddings.create(
    model="text-embedding-3-small", input="Testing 123", encoding_format="float"
)

cut_dim = response.data[0].embedding[:256]
norm_dim = normalize_l2(cut_dim)

print(norm_dim)
```

Dynamically(åŠ¨æ€çš„) changing the dimensions enables very flexible usage. For example, when using a vector data store that only supports embeddings up to 1024 dimensions long, developers can now still use our best embedding model `text-embedding-3-large` and specify a value of 1024 for the dimensions API parameter, which will shorten the embedding down from 3072 dimensions, trading off some accuracy in exchange for the smaller vector size.<br>

åŠ¨æ€æ”¹å˜ç»´åº¦å¯ä»¥å®ç°éå¸¸çµæ´»çš„ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œå½“ä½¿ç”¨ä¸€ä¸ªä»…æ”¯æŒæœ€é•¿1024ç»´çš„å‘é‡æ•°æ®å­˜å‚¨æ—¶ï¼Œå¼€å‘è€…ç°åœ¨ä»ç„¶å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æœ€å¥½çš„è¯å‘é‡æ¨¡å‹ `text-embedding-3-large` ï¼Œå¹¶ä¸ºç»´åº¦APIå‚æ•°æŒ‡å®šä¸€ä¸ª1024çš„å€¼ï¼Œè¿™å°†ä¼šå°†è¯å‘é‡ä»3072ç»´ç¼©çŸ­ï¼Œä»¥è¾ƒå°çš„å‘é‡å¤§å°ä¸ºä»£ä»·äº¤æ¢ä¸€äº›å‡†ç¡®æ€§ã€‚<br>

#### éªŒè¯ "æ‰‹åŠ¨é™ä½ç»´åº¦" å’Œ "é€šè¿‡ä¼ å‚é™ä½ç»´åº¦" çš„åŒºåˆ«:

è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œæˆ‘ä»¬æ¥éªŒè¯ä¸€ä¸‹ï¼Œ"æ‰‹åŠ¨é™ä½ç»´åº¦" å’Œ "é€šè¿‡ä¼ å‚é™ä½ç»´åº¦" çš„ç»“æœæ˜¯å¦ç›¸åŒ:<br>

```python
import os
import numpy as np
from loguru import logger
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®æ—¥å¿—
logger.remove()
logger.add("openai_stream.log", rotation="1 GB", backtrace=True, diagnose=True, format="{time} {level} {message}")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

user_input = "ã€Šè€äººä¸æµ·ã€‹è¿™ç¯‡æ–‡ç« è¢«é€‰å…¥äº†å°å­¦è¯­æ–‡è¯¾æœ¬ã€‚"   # ç”¨æˆ·è¾“å…¥ä¼šè¢«è½¬åŒ–ä¸º [1 x dimension_n] çš„åˆ—è¡¨

response = client.embeddings.create(
    input=user_input,
    model="text-embedding-3-small",
)

print("æ ‡å‡†embeddingè°ƒç”¨:\n")
print(response.data[0].embedding)
print(type(response.data[0].embedding))
print(len(response.data[0].embedding))
# type(response.data[0].embedding)
# <class 'list'>
# len(response.data[0].embedding)
# 1536
print("\næ ‡å‡†embeddingè°ƒç”¨åï¼Œæ‰‹åŠ¨é™ä½ç»´åº¦ï¼Œå¹¶æ‰§è¡Œæ ‡å‡†åŒ–æ“ä½œ:\n")
def normalize_l2(x):
    x = np.array(x)
    if x.ndim == 1:
        norm = np.linalg.norm(x)
        if norm == 0:
            return x
        return x / norm
    else:
        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)
        return np.where(norm == 0, x, x / norm)

cut_dim = response.data[0].embedding[:256]
norm_dim = normalize_l2(cut_dim)    # norm_dimçš„æ•°æ®ç±»å‹ä¸º<class 'numpy.ndarray'>ï¼Œå¯é€šè¿‡ `norm_dim.tolist()` è½¬ä¸ºlistå½¢å¼ã€‚
print(norm_dim)
print(type(norm_dim))
print(len(norm_dim))

print("\næ ‡å‡†embeddingè°ƒç”¨ï¼Œé‡‡ç”¨ä¼ å‚å½¢å¼é™ä½ç»´åº¦:\n")
para_response = client.embeddings.create(
    input=user_input,
    model="text-embedding-3-small",
    dimensions=256
)

para_dim = para_response.data[0].embedding  # listç±»å‹ï¼Œé•¿åº¦256ï¼Œæ•°æ®ä¸º [0.09904252737760544, -0.02682558260858059, -0.01077528577297926, 0.012549266219139099,...]
# ç”±äºarrayç±»å‹çš„æ•°æ®æ ¼å¼ä¸º `[ 9.90425200e-02 -2.68255801e-02 -1.07752855e-02  1.25492662e-02 ...]`ï¼Œæ•°æ®å«ç§‘å­¦è®¡æ•°æ³•(`e`)
# æ‰€ä»¥æƒ³æ¯”è¾ƒ2ä¸ªå˜é‡æ˜¯å¦ç›¸åŒæœ€å¥½çš„æ–¹å¼ä¸æ˜¯å°† array è½¬ä¸º listï¼Œè€Œæ˜¯å°† list è½¬ä¸º array ã€‚å› ä¸ºå°† array è½¬ä¸º list ä¼šå› ä¸º `e` çš„åŸå› é€ æˆå°æ•°ç‚¹å8ä½ä¹‹åçš„æ•°å­—ç²¾åº¦ç¼ºå¤±(9.90425200e-02 å¯¹åº”å°æ•°ç‚¹å8ä½)ã€‚
para_dim_array = np.array(para_dim)
print(para_dim_array)
print(type(para_dim_array))
print(len(para_dim_array))

# ä½¿ç”¨np.allcloseè¿›è¡Œæ¯”è¾ƒï¼Œå¯ä»¥æŒ‡å®šä¸€ä¸ªå®¹å¿åº¦(9.90425200e-02 å¯¹åº”å°æ•°ç‚¹å8ä½)
# - atolä»£è¡¨ç»å¯¹å®¹å¿åº¦ï¼Œæ˜¯ä¸€ä¸ªéè´Ÿçš„æµ®ç‚¹æ•°ã€‚
# - 1e-8æ˜¯ç§‘å­¦è®¡æ•°æ³•è¡¨ç¤ºçš„0.00000001ï¼Œå³1åé¢è·Ÿç€8ä¸ªé›¶ã€‚
are_close = np.allclose(norm_dim, para_dim_array, atol=1e-8)

print(f"\nnorm_dimå’Œpara_dimæ˜¯å¦å‡ ä¹ç›¸ç­‰: {are_close}")
```

ç»ˆç«¯ç»“æœä¸º:<br>

```txt
æ ‡å‡†embeddingè°ƒç”¨:

[0.04963647946715355, -0.013443997129797935, -0.005400178022682667, ... ... , 0.0041159894317388535, 0.017432980239391327, 0.016351062804460526, 0.04470670223236084, -0.021807687357068062]
<class 'list'>
1536

æ ‡å‡†embeddingè°ƒç”¨åï¼Œæ‰‹åŠ¨é™ä½ç»´åº¦ï¼Œå¹¶æ‰§è¡Œæ ‡å‡†åŒ–æ“ä½œ:

[ 9.90425200e-02 -2.68255801e-02 -1.07752855e-02  1.25492662e-02
  8.74788025e-02 -4.04730259e-02 -1.38314123e-01  7.09122691e-03
 -8.89054954e-02 -1.85282361e-02 -1.91195614e-02 -1.31255742e-01
  ... ...
  3.07302145e-02 -4.34988801e-04  4.52036396e-02  1.47099553e-01
  8.31236329e-02 -2.74262936e-02 -5.57536576e-02 -3.96094993e-03]
<class 'numpy.ndarray'>
256

æ ‡å‡†embeddingè°ƒç”¨ï¼Œé‡‡ç”¨ä¼ å‚å½¢å¼é™ä½ç»´åº¦:

[ 9.90425274e-02 -2.68255826e-02 -1.07752858e-02  1.25492662e-02
  8.74788016e-02 -4.04730253e-02 -1.38314128e-01  7.09122699e-03
 -8.89054984e-02 -1.85282361e-02 -1.91195626e-02 -1.31255746e-01
  ... ...
  3.07302158e-02 -4.34988819e-04  4.52036411e-02  1.47099555e-01
  8.31236392e-02 -2.74262950e-02 -5.57536595e-02 -3.96095030e-03]
<class 'numpy.ndarray'>
256

norm_dimå’Œpara_dimæ˜¯å¦å‡ ä¹ç›¸ç­‰: True
```

ğŸš€ğŸš€ğŸš€ç”±æ­¤å¯ä»¥ç¡®å®šï¼Œå®˜æ–¹ç»™å‡ºçš„ "æ‰‹åŠ¨é™ä½ç»´åº¦" ä»£ç å³ "ä¼ å‚é™ä½ç»´åº¦" ä»£ç ï¼Œ2è€…æ•ˆæœç›¸åŒã€‚<br>

#### å…³äº"`text-embedding-3-large` è¯å‘é‡ç¼©çŸ­åˆ°256çš„å¤§å°ï¼Œè€Œä»ç„¶æ¯”æœªç¼©çŸ­çš„text-embedding-ada-002åµŒå…¥å‘é‡ï¼ˆå¤§å°ä¸º1536ï¼‰è¡¨ç°å¾—æ›´å¥½"çš„ä¸€äº›æ€è€ƒ:

æ¨èOpenAIé‡‡ç”¨äº†ä¸€äº›ç‰¹æ®Šæ‰‹æ®µï¼Œå°†å…³é”®ä¿¡æ¯é›†ä¸­åœ¨äº†è¯å‘é‡ç»´åº¦çš„å‰åŠéƒ¨åˆ†ï¼Œæ‰€ä»¥åœ¨è¯å‘é‡é™ç»´çš„æ—¶å€™æ‰ä¼šå¼ºè¡Œæˆªå–å‰nç»´åº¦ã€‚<br>

è¯å‘é‡ç»´åº¦ä¸º1563ï¼Œä¹Ÿå°±æ˜¯1563ä¸ªç‰¹å¾ï¼Œä½†æ¯ä¸ªç‰¹å¾å«æœ‰çš„ä¿¡æ¯é‡å¹¶ä¸å‡è¡¡ã€‚å‡è®¾OpenAIå°†80%çš„ä¿¡æ¯é‡é›†ä¸­åœ¨äº†å‰200çš„ç»´åº¦ï¼Œ**"`text-embedding-3-large` è¯å‘é‡ç¼©çŸ­åˆ°256çš„å¤§å°ï¼Œè€Œä»ç„¶æ¯”æœªç¼©çŸ­çš„text-embedding-ada-002åµŒå…¥å‘é‡ï¼ˆå¤§å°ä¸º1536ï¼‰è¡¨ç°å¾—æ›´å¥½"** å°±å¯ä»¥å¾—åˆ°åˆç†çš„è§£é‡Šã€‚<br>

å¦å¤–ï¼Œæˆ‘ä»¬ä¸€èˆ¬é™ç»´é‡‡ç”¨çš„éƒ½æ˜¯åœ¨æ¨¡å‹ç»“æ„åé¢**åŠ ä¸€ä¸ªå…¨è¿æ¥å±‚**ï¼Œä½†ç”±äºOpenAIå¹¶æœªå¼€æ”¾æ¨¡å‹ä»£ç ï¼Œæ‰€ä»¥è¿™ç§æ–¹å¼æ˜¯æ— æ³•å®ç°çš„ã€‚<br>


#### Question answering using embeddings-based search(åŸºäºè¯å‘é‡æ£€ç´¢è¿›è¡Œé—®é¢˜å›ç­”):

There are many common cases where the model is not trained on data which contains key facts and information you want to make accessible(å¯è®¿é—®çš„ï¼›å¯è·å–çš„) when generating responses to a user query.<br> 

> â€œaccessibleâ€åœ¨è¿™é‡ŒæŒ‡çš„æ˜¯ä½¿å…³é”®äº‹å®å’Œä¿¡æ¯æ˜“äºåœ¨ç”Ÿæˆå“åº”æ—¶è¢«è®¿é—®å’Œåˆ©ç”¨ã€‚

åœ¨å¾ˆå¤šå¸¸è§æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¹¶æ²¡æœ‰é’ˆå¯¹åŒ…å«ä½ å¸Œæœ›åœ¨å›ç­”ç”¨æˆ·æŸ¥è¯¢æ—¶è®¿é—®çš„å…³é”®äº‹å®å’Œä¿¡æ¯çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚<br>

One way of solving this, as shown below, is to put additional(é¢å¤–çš„) information into the context window of the model. <br>

è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æ–¹å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œæ˜¯å°†é¢å¤–çš„ä¿¡æ¯æ”¾å…¥æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ä¸­ã€‚<br>

This is effective(æœ‰æ•ˆçš„) in many use cases but leads to higher token costs. In this notebook, we explore the tradeoff between this approach and embeddings bases search.<br>

è¿™åœ¨è®¸å¤šç”¨ä¾‹ä¸­æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ä¼šå¯¼è‡´æ›´é«˜çš„tokenæˆæœ¬ã€‚åœ¨è¿™ä¸ªnotebookä¸­ï¼Œæˆ‘ä»¬æ¢è®¨è¿™ç§æ–¹æ³•ä¸åŸºäºè¯å‘é‡çš„æ£€ç´¢ä¹‹é—´çš„æƒè¡¡ã€‚<br>

```python
query = f"""Use the below article on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found, write "I don't know."

Article:
\"\"\"
{wikipedia_article_on_curling}
\"\"\"

Question: Which athletes won the gold medal in curling at the 2022 Winter Olympics?"""

# ä¸Šè¿°queryçš„ç¿»è¯‘:

# è¯·ä½¿ç”¨ä»¥ä¸‹å…³äº2022å¹´å†¬å­£å¥¥è¿ä¼šçš„æ–‡ç« æ¥å›ç­”éšåçš„(subsequent)é—®é¢˜ã€‚å¦‚æœæ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¯·å›ç­”â€œæˆ‘ä¸çŸ¥é“ã€‚â€

# æ–‡ç« ï¼š
# ```
# {å…³äºå†°å£¶(curling)çš„ç»´åŸºç™¾ç§‘(wikipedia)æ–‡ç« }
# ```

# é—®é¢˜ï¼šå“ªäº›è¿åŠ¨å‘˜åœ¨2022å¹´å†¬å­£å¥¥è¿ä¼šä¸Šèµ¢å¾—äº†å†°å£¶é¡¹ç›®çš„é‡‘ç‰Œï¼Ÿ

response = client.chat.completions.create(
    messages=[
        {'role': 'system', 'content': 'You answer questions about the 2022 Winter Olympics.'},  # ä½ å›ç­”å…³äº2022å¹´å†¬å­£å¥¥è¿ä¼šçš„é—®é¢˜ã€‚
        {'role': 'user', 'content': query},
    ],
    model=GPT_MODEL,
    temperature=0,
)

print(response.choices[0].message.content)
```

#### å…¶ä»–ç¤ºä¾‹ä¸ä¸Šè¿°ç”¨æ³•ç›¸ä¼¼ï¼Œè¿™é‡Œå°±ä¸å¤šä»‹ç»ã€‚

è¯·å°†ä¸‹åˆ—å†…å®¹ç¿»è¯‘ä¸ºåœ°é“çš„ä¸­æ–‡ï¼š

### Frequently asked questions(å¸¸è§é—®é¢˜è§£ç­”):

How can I tell how many tokens a string has before I embed it?<br>

å¦‚ä½•åœ¨åµŒå…¥ä¹‹å‰çŸ¥é“ä¸€ä¸ªå­—ç¬¦ä¸²æœ‰å¤šå°‘ä¸ªtokensï¼Ÿ<br>

In Python, you can split a string into tokens with OpenAI's tokenizer tiktoken.<br>

åœ¨Pythonä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨OpenAIçš„åˆ†è¯å™¨ `tiktoken` å°†å­—ç¬¦ä¸²åˆ†å‰²æˆtokensã€‚<br>

#### Example code(ç¤ºä¾‹ä»£ç ):

```python
import tiktoken

def num_tokens_from_string(string: str, encoding_name: str) -> int:
    """Returns the number of tokens in a text string.
    (è¿”å›ä¸€ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ä¸­çš„ tokens æ•°é‡ã€‚)
    """
    encoding = tiktoken.get_encoding(encoding_name)
    exact_tokens = encoding.encode(string)
    num_tokens = len(exact_tokens)
    return exact_tokens, num_tokens

exact_tokens_rtn, num_tokens_rtn = num_tokens_from_string("tiktoken is great!", "cl100k_base")
print(exact_tokens_rtn) # [83, 1609, 5963, 374, 2294, 0]
print(num_tokens_rtn)   # 6

encoding = tiktoken.get_encoding("cl100k_base")
restore_str = encoding.decode(exact_tokens_rtn)
print(restore_str)  # tiktoken is great!

token_byte = [encoding.decode_single_token_bytes(token) for token in exact_tokens_rtn]
print(token_byte)   # [b't', b'ik', b'token', b' is', b' great', b'!']
```

å‡½æ•° `num_tokens_from_string` æ¥å—çš„2ä¸ªå‚æ•°å¦‚ä¸‹:<br>

- `string`: è¦ç¼–ç çš„æ–‡æœ¬å­—ç¬¦ä¸²ã€‚
- `encoding_name`: ç¼–ç åç§°ï¼Œç”¨äºæŒ‡å®šä½¿ç”¨å“ªç§æ–‡æœ¬ç¼–ç æ–¹å¼ã€‚

TikToken æ”¯æŒ OpenAI æ¨¡å‹ä½¿ç”¨çš„ä¸‰ç§ç¼–ç ï¼š<br>

```markdown
| Encoding name | OpenAI models |
|---------------|---------------|
| cl100k_base    | gpt-4, gpt-3.5-turbo, text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large |
| p50k_base     | Codex models, text-davinci-002, text-davinci-003 |
| r50k_base (or gpt2) | GPT-3 models like davinci |
```

`tiktoken` [è¯¦è§£](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)ã€‚<br>

For third-generation embedding models like `text-embedding-3-small`, use the `cl100k_base` encoding.<br>

å¯¹äºç¬¬ä¸‰ä»£åµŒå…¥æ¨¡å‹ï¼Œå¦‚ `text-embedding-3-small` ï¼Œè¯·ä½¿ç”¨ `cl100k_base` ç¼–ç ã€‚<br>

More details and example code are in the OpenAI Cookbook guide [how to count tokens with tiktoken](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken).<br>

æ›´å¤šç»†èŠ‚å’Œç¤ºä¾‹ä»£ç ï¼Œè¯·å‚è€ƒOpenAI CookbookæŒ‡å—ä¸­çš„å¦‚ä½•ä½¿ç”¨tiktokenè®¡ç®—tokensã€‚<br>

#### How can I retrieve K nearest embedding vectors quickly?(å¦‚ä½•å¿«é€Ÿæ£€ç´¢Kä¸ªæœ€è¿‘çš„åµŒå…¥å‘é‡)

For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API in our [Cookbook](https://cookbook.openai.com/examples/vector_databases/readme) on GitHub.<br>

> ç¬”è€…ä½¿ç”¨çš„ zillizï¼Œä¹Ÿå°±æ˜¯ milvus å‘é‡æ•°æ®åº“ã€‚

ä¸ºäº†å¿«é€Ÿæœç´¢è®¸å¤šå‘é‡ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨å‘é‡æ•°æ®åº“ã€‚ä½ å¯ä»¥åœ¨æˆ‘ä»¬çš„GitHubä¸Šçš„Cookbookä¸­æ‰¾åˆ°ä½¿ç”¨å‘é‡æ•°æ®åº“å’ŒOpenAI APIçš„ç¤ºä¾‹ã€‚<br>

#### Which distance function should I use?(åº”è¯¥ä½¿ç”¨å“ªç§è·ç¦»å‡½æ•°)

We recommend cosine similarity. The choice of distance function typically(é€šå¸¸) doesnâ€™t matter much.<br>

æˆ‘ä»¬æ¨èä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€‚è·ç¦»å‡½æ•°çš„é€‰æ‹©é€šå¸¸å½±å“ä¸å¤§ã€‚<br>

OpenAI embeddings are normalized(æ ‡å‡†åŒ–) to length 1, which means that:<br>

OpenAIçš„åµŒå…¥å‘é‡è¢«æ ‡å‡†åŒ–åˆ°é•¿åº¦1ï¼Œè¿™æ„å‘³ç€ï¼š<br>

- Cosine similarity can be computed slightly faster using just a dot product(ä½¿ç”¨ç‚¹ç§¯å¯ä»¥ç¨å¾®æ›´å¿«åœ°è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦)
- Cosine similarity and Euclidean distance will result in the identical(å®Œå…¨ç›¸åŒçš„) rankings(ä½™å¼¦ç›¸ä¼¼åº¦å’Œæ¬§å‡ é‡Œå¾—è·ç¦»å°†äº§ç”Ÿç›¸åŒçš„æ’å)

#### Do V3 embedding models know about recent events?(V3åµŒå…¥æ¨¡å‹çŸ¥é“æœ€è¿‘çš„äº‹ä»¶å—)

No, the `text-embedding-3-large` and `text-embedding-3-small` models lack(ç¼ºå°‘) knowledge of events that occurred after September 2021. <br>

ä¸ï¼Œtext-embedding-3-largeå’Œtext-embedding-3-smallæ¨¡å‹ä¸äº†è§£2021å¹´9æœˆä¹‹åå‘ç”Ÿçš„äº‹ä»¶ã€‚<br>

This is generally not as much of a limitation as it would be for text generation models but in certain edge cases it can reduce performance.<br>

è¿™é€šå¸¸ä¸åƒå¯¹äºæ–‡æœ¬ç”Ÿæˆæ¨¡å‹é‚£æ ·æ˜¯ä¸€ä¸ªé™åˆ¶ï¼Œä½†åœ¨æŸäº›è¾¹ç¼˜æƒ…å†µä¸‹ï¼Œå®ƒå¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚<br>